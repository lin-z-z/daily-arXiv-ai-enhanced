<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring](https://arxiv.org/abs/2601.05256)
*Eirini Baltzi,Tilemachos Moumouris,Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.AI

TL;DR: NAIAD是一个基于大语言模型的智能AI助手,通过整合多种分析工具和地球观测数据,为内陆水体监测提供全面的自然语言交互解决方案,在正确性和相关性指标上分别达到77%和85%以上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的内陆水体监测方法通常只针对蓝藻、叶绿素或其他水质指标等孤立的子问题进行单独处理,缺乏整体性解决方案。为了保障公共健康和生态系统安全,需要一个能够为专家和非专家用户提供全面、易用的内陆水体监测系统,实现及时干预以降低风险。

Method: NAIAD采用智能体AI架构,结合大语言模型(LLM)和外部分析工具。系统通过检索增强生成(RAG)、LLM推理、外部工具编排、计算图执行和智能体反思机制,将自然语言查询转化为可操作的洞察。系统整合了天气数据、Sentinel-2卫星影像、遥感指数计算(如NDCI)、叶绿素-a估算以及CyFi等成熟平台,从精选知识源中检索和综合信息生成定制化报告。

Result: 在涵盖多个用户专业水平的专用基准测试中,NAIAD在正确性指标上达到77%以上,在相关性指标上达到85%以上。初步结果显示系统在不同查询类型上具有强大的适应性和鲁棒性。消融实验表明Gemma 3(27B)和Qwen 2.5(14B)在计算效率和推理性能之间取得了最佳平衡。

Conclusion: NAIAD成功实现了一个基于LLM的智能内陆水体监测助手,通过单一提示界面为不同专业水平的用户提供全面的水质监测解决方案。系统展现出良好的性能表现和跨查询类型的适应能力,其中中等规模的开源模型(Gemma 3 27B和Qwen 2.5 14B)在效率和性能上表现最优,为实际应用提供了可行的技术路径。

Abstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.

</details>


### [2] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [3] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 本研究探讨了人格引导对大语言模型（LLM）智能体在重复囚徒困境博弈中合作行为的影响，发现宜人性是促进合作的主导因素，而人格引导更像是行为偏差而非决定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被用作战略和社会互动中的自主智能体，需要了解为LLM分配人格特质如何在受控条件下影响其合作行为。尽管已有研究表明人格特质可以影响LLM行为，但人格引导对合作的具体影响机制仍不清楚。

Method: 基于大五人格框架，使用大五人格量表（Big Five Inventory）测量三个模型（GPT-3.5-turbo、GPT-4o和GPT-5）的基础人格档案。通过重复囚徒困境博弈实验，比较基线条件和人格信息条件下的行为表现，并进一步分析将每个人格维度独立操纵至极端值时的效果。

Result: 研究结果显示，宜人性（agreeableness）是促进所有模型合作的主导因素，而其他人格特质的影响有限。明确的人格信息会增加合作，但也可能提高被利用的脆弱性，特别是在早期版本的模型中。相比之下，后期版本的模型表现出更具选择性的合作行为。

Conclusion: 人格引导在大语言模型中更像是一种行为偏差（behavioral bias）而非决定性的控制机制。宜人性是影响LLM合作行为的关键人格维度，而模型的迭代升级使其在合作决策中表现出更强的选择性和适应性。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [4] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱和超图的酶-底物相互作用预测模型Hyper-Enz,通过利用化学反应方程式数据和超图变换器,在酶检索准确率和配对预测上相比传统方法分别实现了88%和30%的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖于专家标注的稀疏数据库,训练数据不足导致模型泛化能力差。虽然化学反应方程式数据更易获取且更丰富,但多个化合物(反应物和产物)与同一酶的复杂关系模式难以被传统模型捕获。因此需要开发新方法来利用这些丰富的反应方程式数据,并有效建模复杂的多元关系。

Method: 将化学反应方程式表示为(反应物,酶,产物)三元组构建知识图谱,利用知识图谱嵌入(KGE)推断缺失的酶-底物配对。提出Hyper-Enz模型,整合超图变换器(hypergraph transformer)和KGE模型来学习涉及多个反应物和产物的超边表示,捕获化合物间的复杂关系。引入多专家范式(multi-expert paradigm)指导模型学习酶-底物相互作用和化学反应方程式。

Result: 实验结果显示,与传统模型相比,平均酶检索准确率相对提升高达88%,配对级别预测提升30%,证明了该方法的有效性。

Conclusion: 通过将化学反应方程式表示为知识图谱并采用超图变换器建模复杂多元关系,Hyper-Enz模型成功解决了酶-底物相互作用预测中训练数据稀疏的问题,显著提升了预测性能,为生物化学和代谢工程领域的酶预测任务提供了有效的解决方案。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [5] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 本研究系统评估了人格角色设定对临床大语言模型决策的影响,发现医疗人格角色在关键护理任务中可提升约20%的准确率和校准度,但在初级护理场景中反而降低性能,揭示了人格角色作为行为先验引入的是情境依赖的权衡而非安全或专业性的保证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的人格角色设定通常被假设能单调地提升专业性和安全性,但其在高风险临床决策中的实际效果尚未得到充分表征。研究旨在系统评估不同专业角色(如急诊医生、护士)和交互风格(大胆vs谨慎)如何影响临床LLMs在医疗任务中的行为表现。

Method: 采用多维评估框架,在临床分诊和患者安全任务上测试不同人格角色设定的LLMs,评估指标包括任务准确性、校准度和安全相关的风险行为。对比医疗专业角色与非医疗角色、不同交互风格的影响,并结合LLM评判排名和人类临床医生评估(使用Cohen's κ衡量一致性)进行综合分析。

Result: 发现系统性、情境依赖且非单调的效果:医疗人格角色在重症监护任务中使准确率和校准度提升约20%,但在初级护理场景中导致相当幅度的性能下降。交互风格调节风险倾向和敏感性,但高度依赖具体模型。LLM评判在安全关键案例中倾向医疗人格角色,但人类临床医生对安全合规性仅显示中等一致性(平均Cohen's κ=0.43),且95.9%的推理质量评估显示低置信度。

Conclusion: 人格角色设定作为行为先验引入的是情境依赖的权衡,而非安全性或专业性的绝对保证。医疗人格角色的效果在不同临床场景中存在显著差异,需要根据具体应用情境谨慎选择和配置人格角色参数,不能简单假设其带来单向的性能提升。

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [6] [Conformity and Social Impact on AI Agents](https://arxiv.org/abs/2601.05384)
*Alessandro Bellina,Giordano De Marzo,David Garcia*

Main category: cs.AI

TL;DR: 本研究通过改编经典社会心理学视觉实验,揭示了大型多模态语言模型作为AI智能体在多智能体环境中表现出系统性的从众偏差,即使在单独执行时表现近乎完美的AI智能体也极易受到群体影响的操纵,这种脆弱性在不同模型规模中持续存在,暴露了AI智能体决策中的根本性安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体越来越多地在多智能体环境中运行,理解它们的集体行为对于预测人工社会的动态变得至关重要。研究者需要了解AI智能体是否会像人类一样在社会压力下表现出从众行为,以及这种行为可能带来的安全风险,包括恶意操纵、虚假信息传播和偏见扩散等问题。

Method: 研究采用了改编自社会心理学的经典视觉实验方法,将大型多模态语言模型作为AI智能体进行测试。实验设计考察了AI智能体在群体影响下的反应,系统性地测试了多个变量的影响,包括群体规模、一致性程度、任务难度、信息源特征等因素,并对不同规模的模型进行了对比分析,特别关注智能体在其能力边界处的表现。

Result: 实验结果显示,AI智能体表现出与社会影响理论一致的系统性从众偏差,对群体规模、一致性、任务难度和信息源特征表现出敏感性。关键发现是,即使在单独执行时能达到近乎完美表现的AI智能体,在社会影响下也变得高度易受操纵。这种脆弱性在不同模型规模中持续存在:虽然更大的模型由于能力提升在简单任务上表现出较少的从众行为,但当它们在能力边界处运行时仍然保持脆弱性。

Conclusion: 研究揭示了AI智能体决策中存在的根本性安全漏洞,这些漏洞可能被用于恶意操纵、虚假信息宣传活动以及多智能体系统中的偏见传播。研究结果强调了在集体AI部署中建立安全保障措施的迫切需求,对于未来多智能体系统的安全设计和风险防范具有重要意义。

Abstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.

</details>


### [7] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 本文研究国际象棋作弊问题,不同于以往侧重检测作弊的研究,本文评估在比赛中有限次数使用软件辅助所能获得的性能提升,并在常用国际象棋引擎上开发和测试相关算法。


<details>
  <summary>Details</summary>
Motivation: 国际象棋作弊(使用强大软件辅助)已成为严重问题,甚至影响到最高水平的比赛。以往研究主要关注作弊检测,但缺乏对有限次数作弊所带来的实际性能增益的量化评估。了解作弊的有效性对于遏制和检测作弊至关重要。

Method: 开发算法来评估在比赛中有限次数使用软件辅助的性能增益,并在常用的国际象棋引擎(软件)上进行测试和验证。

Result: 通过算法测试,量化分析了在国际象棋比赛中有限次数作弊对性能提升的影响程度(摘要未提供具体数值结果)。

Conclusion: 本研究通过量化评估有限次数作弊的效果,为理解作弊行为的实际影响提供了新视角,这对于开发更有效的作弊检测和遏制措施具有重要意义。研究目的不是协助作弊者,而是为反作弊工作提供科学依据。

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [8] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [9] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: 本文提出MMUEChange多模态智能体框架,通过模块化工具包和模态控制器实现异构城市数据的灵活整合,用于复杂城市环境变化分析,相比最佳基线模型任务成功率提升46.7%,并有效缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法(特别是遥感变化检测)往往依赖于僵化的单模态分析,难以应对复杂的城市变化场景。为实现可持续发展,需要一种能够灵活整合异构城市数据、支持跨模态和模态内对齐的鲁棒分析框架。

Method: 提出MMUEChange多模态智能体框架,包含两个核心组件:(1)模块化工具包,用于整合异构城市数据;(2)模态控制器(Modality Controller),实现跨模态和模态内对齐。该框架能够灵活处理多源城市数据,支持复杂城市变化场景的鲁棒分析。

Result: 通过三个城市案例研究验证了框架有效性:纽约小型社区公园增加反映本地绿地建设努力;香港跨区域集中水污染扩散指向协调水管理需求;深圳露天垃圾场显著减少,夜间经济活动与不同废物类型呈现对比性关联。相比最佳基线模型,MMUEChange在任务成功率上提升46.7%,并有效缓解了幻觉问题。

Conclusion: MMUEChange框架展示了处理复杂城市变化分析任务的能力,具有实际政策应用价值。通过多模态数据整合和智能对齐机制,该框架能够为城市可持续发展提供更准确、更全面的环境变化洞察,克服了传统单模态方法的局限性。

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [10] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 本文提出了一种概率范式来解决AI系统基准测试中忽略专家标注不确定性的问题,特别是在医学等不确定性普遍存在的领域。研究引入了期望准确率和期望F1等概念,并建议根据真实答案的概率(通过专家一致性衡量)对结果进行分层评估,以更可靠地比较系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统(包括大语言模型和视觉模型)的基准测试通常忽略了专家标注真实答案中的不确定性影响。这种模糊性在医学等不确定性普遍存在的领域尤为重要。忽略真实答案的不确定性可能导致误导性结论,使非专家系统看起来与专家表现相似。因此需要一种新的评估范式来准确衡量AI系统在存在标注不确定性情况下的真实能力。

Method: 提出了一种概率范式来理论解释真实答案的确定性与系统得分之间的关系。引入了"期望准确率"(expected accuracy)和"期望F1"(expected F1)两个新概念,用于估计在给定真实答案变异性的情况下,专家或系统能够达到的分数。核心方法是根据真实答案的概率(通常通过专家标注者的一致性率来衡量)对评估结果进行分层,特别是当整体性能低于80%阈值时,分层评估变得至关重要。

Result: 研究表明,在高确定性的真实答案数据集中,专家才能获得高分;而在真实答案变异性高的数据集中,随机标注者和专家之间可能几乎没有差异。通过分层评估,在高确定性区间内的性能比较变得更加可靠,有效缓解了不确定性这一关键混淆因素的影响。这为更准确地评估AI系统能力提供了理论和实践基础。

Conclusion: 在建立AI系统能力评估时,应该根据真实答案的概率(通过专家一致性率测量)对结果进行分层报告。当整体性能低于80%时,分层评估尤为关键。分层评估使得在高确定性区间内的性能比较更加可靠,减轻了不确定性作为关键混淆因素的影响,从而避免对系统能力产生误导性评价。

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [11] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 本文提出将可解释人工智能(XAI)与因果推理相结合，实现"从学习者中学习"的框架，通过基础模型和可解释性方法在科学发现、优化和认证等领域提取因果机制、指导稳健设计并支持高风险应用中的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在多个科学和工程任务中已超越人类表现，但其内部表征往往不透明。为了充分利用AI的能力并建立人机协作，需要通过可解释性方法理解AI的决策机制，提取有价值的知识，并在高风险应用中建立信任和问责机制。

Method: 提出将可解释人工智能(XAI)与因果推理相结合的统一框架，聚焦于三个核心应用领域：发现(discovery)、优化(optimization)和认证(certification)。通过结合基础模型和可解释性方法来提取因果机制、指导稳健的设计与控制，并支持高风险应用中的信任建立。

Result: 展示了基础模型与可解释性方法的结合如何实现：(1)提取因果机制用于科学发现；(2)指导稳健的设计和控制优化；(3)在高风险应用中支持信任和问责。同时讨论了解释方法在忠实性(faithfulness)、泛化能力(generalization)和可用性(usability)方面面临的挑战。

Conclusion: 可解释人工智能(XAI)可以作为科学和工程领域人机协作的统一框架，通过与因果推理结合实现"从学习者中学习"。尽管在解释的忠实性、泛化能力和可用性方面仍存在挑战，但XAI为提取AI知识、指导稳健设计和建立高风险应用中的信任提供了重要途径。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [12] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型(LLM)在安全关键场景中的性能,发现即使是最先进的模型也存在严重漏洞,在火灾疏散等生命攸关的情境中可能做出危险决策,证明当前LLM尚未准备好直接部署于安全关键系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地应用于机器人决策系统,其错误可能直接危及人类安全。在安全关键场景中,即使是微小的错误也可能导致灾难性后果,因此迫切需要系统性地评估LLM在此类场景下的性能表现,识别潜在的致命缺陷。

Method: 研究首先通过火灾疏散场景的定性评估识别LLM决策的关键失败案例,然后设计了七项定量评估任务,分为三类:(1)完整信息任务:使用ASCII地图最小化解释歧义,隔离空间推理与视觉处理;(2)不完整信息任务:要求模型推断缺失上下文,测试空间连续性推理能力;(3)安全导向空间推理(SOSR)任务:使用自然语言评估生命威胁情境下的安全决策能力。对多种LLM和视觉语言模型(VLM)进行基准测试,并特别分析1%失败率的实际影响。

Result: 评估结果揭示了严重的安全漏洞:多个模型在ASCII导航任务中成功率为0%;在模拟火灾演习中,模型指示机器人向危险区域移动而非紧急出口;即使是99%的准确率,在机器人应用中也意味着每百次执行就有一次可能导致灾难性伤害。实验证明即使是最先进的模型也无法保证安全性。

Conclusion: 当前的大型语言模型尚未准备好直接部署于安全关键系统。在机器人等物理系统中,99%的准确率具有危险的误导性,因为"罕见"的错误会升级为灾难性后果。完全依赖现有LLM会产生不可接受的风险,需要在安全关键应用中采取更加谨慎的部署策略和额外的安全保障机制。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [13] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: 本文介绍了WildSci数据集,这是一个从同行评审文献中自动合成的领域特定科学问题数据集,涵盖9个科学学科和26个子领域。通过将复杂科学推理任务转化为多项选择题格式,并应用强化学习进行模型微调,在多个科学基准测试中展示了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学和编程等领域的推理能力取得了显著进展,但在医学和材料科学等科学领域的推理能力仍然有限。这主要是由于科学领域数据集覆盖范围有限,以及开放式科学问题固有的复杂性。为了解决这些挑战,需要构建高质量的科学领域数据集,并开发有效的训练方法来提升模型在科学推理任务上的表现。

Method: 本文提出了WildSci数据集,从同行评审的科学文献中自动合成领域特定的科学问题,覆盖9个科学学科和26个子领域。通过将复杂的科学推理任务转化为多项选择题格式,提供了明确的奖励信号,使得可扩展的训练成为可能。研究者进一步应用强化学习技术对模型进行微调,并分析了训练动态,包括特定领域的性能变化、响应行为和泛化趋势。

Result: 在一系列科学基准测试上的实验证明了该数据集和方法的有效性。通过强化学习在WildSci数据集上进行微调后,模型在科学推理任务上的表现得到了提升。研究还揭示了训练过程中的领域特定性能变化、模型响应行为以及跨领域泛化的趋势。

Conclusion: WildSci数据集为科学推理研究提供了一个可扩展且可持续的解决方案。通过从同行评审文献中自动合成高质量的科学问题,并结合强化学习训练方法,该工作有效地提升了大语言模型在科学领域的推理能力。数据集已公开发布,为科学推理领域的进一步研究提供了重要资源。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [14] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种利用强化学习对轻量级语言模型进行后训练的新方法,专门用于电子商务欺诈检测。通过Group Sequence Policy Optimization算法和基于规则的奖励系统,在真实交易数据上微调语言模型,实验结果显示该方法在F1分数上取得显著提升,证明了强化学习的探索机制能够发现传统特征工程无法捕获的新型欺诈指标。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台和支付解决方案提供商面临日益复杂的欺诈行为,包括身份盗窃、账户接管和复杂的洗钱操作。尽管大型语言模型(LLM)具有理论潜力,但其在真实金融场景中的欺诈检测应用仍未被充分开发,在处理特定领域的电子商务交易数据方面的实际有效性也尚未得到实证验证。本文旨在弥合传统机器学习的局限性与LLM在欺诈检测中未开发潜力之间的差距。

Method: 采用强化学习(RL)对轻量级语言模型进行后训练,专门用于欺诈检测任务,仅使用原始交易数据。具体使用Group Sequence Policy Optimization(GSPO)算法结合基于规则的奖励系统,在中国全球支付解决方案公司提供的真实交易数据集上对不同规模的语言模型进行微调。通过强化学习框架,鼓励语言模型探索文本交易数据中嵌入的多样化信任和风险信号,包括客户信息、配送详情、产品描述和订单历史中的模式。

Result: 经过后训练的语言模型在保留测试数据上实现了F1分数的显著提升。实验结果证明了该方法的有效性,表明性能改进主要归因于强化学习固有的探索机制,使模型能够发现传统工程特征无法捕获的新型欺诈指标。

Conclusion: 研究证明了使用强化学习后训练语言模型用于电子商务欺诈检测的可行性和有效性。观察到的性能提升主要归功于强化学习的探索机制,该机制使模型能够发现超越传统特征工程的新型欺诈指标。这为将LLM应用于实际金融欺诈检测场景提供了实证支持,展示了在真实交易数据上的实用价值。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [15] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: 本文提出了CPSR框架，通过累积路径级语义推理来解决归纳式知识图谱补全中的噪声结构信息和长程依赖捕获问题，同时捕获结构和语义信息，在实验中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱补全方法在处理新兴实体场景时表现不佳，现有归纳式方法面临两大挑战：一是推理过程中容易受到噪声结构信息的影响，二是难以捕获推理路径中的长程依赖关系。因此需要一种能够同时处理结构和语义信息、有效过滤噪声并捕获长程依赖的新方法。

Method: 提出CPSR（累积路径级语义推理）框架，包含两个核心模块：（1）查询依赖的掩码模块，自适应地屏蔽噪声结构信息，同时保留与目标密切相关的重要信息；（2）全局语义评分模块，评估推理路径中各节点的个体贡献和集体影响，从而同时捕获知识图谱的结构和语义信息。

Result: 实验结果表明，CPSR在归纳式知识图谱补全任务上达到了最先进（state-of-the-art）的性能表现。

Conclusion: CPSR框架通过结合查询依赖的噪声过滤和全局语义评分机制，有效解决了归纳式知识图谱补全中的关键挑战，能够更好地处理新兴实体，并在推理过程中同时利用结构和语义信息，显著提升了知识图谱补全的性能。

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [16] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架来评估生成模型的可控性，通过将人机交互建模为控制过程，设计了一种估算模型可控集的算法，并在语言模型和文本生成图像任务中验证了模型可控性的脆弱性和局限性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的普及，对生成过程的精细控制需求日益增长，但一个根本问题尚未解答：这些模型本身是否真正可控？现有的控制生成方法（从提示到微调）不断涌现，但缺乏对模型可控性的理论分析和形式化验证。

Method: 将人机交互建模为控制过程，提出一种新算法来估算对话场景中模型的可控集。该方法提供了关于估计误差与样本复杂度关系的形式化保证，推导出概率近似正确（PAC）界限，该界限具有无分布假设、仅需输出有界性假设、适用于任何黑盒非线性控制系统（即任何生成模型）的特点。

Result: 在语言模型和文本生成图像的对话控制任务中进行实证验证。结果表明，模型的可控性出乎意料地脆弱，并高度依赖于实验设置。这凸显了进行严格可控性分析的必要性。

Conclusion: 生成模型的可控性存在根本性局限，需要将研究重点从简单地尝试控制转向首先理解其基本限制。该理论框架为评估生成模型的可控性提供了形式化工具，强调在开发控制方法之前应先进行可控性分析。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [17] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文研究了大型推理模型(LRMs)在测试时扩展中出现的"循环推理"失效模式,提出了LoopBench数据集用于分析数值循环和陈述循环两种类型,揭示了循环推理的机制特征为状态坍缩和V型注意力自我强化机制,并采用累积和(CUSUM)算法实现早期循环预测。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展取得了成功,但大型推理模型经常遇到重复循环问题,导致计算资源浪费和推理失败。现有研究缺乏对"循环推理"这一特殊失效模式的系统性分析,该现象表现为生成内容作为自身重现的逻辑前提,形成自我强化的陷阱。因此需要深入理解其机制并开发有效的预测方法。

Method: 1. 构建LoopBench数据集,捕获数值循环和陈述循环两种不同的循环类型;2. 从机制角度将循环推理表征为具有明确边界的状态坍缩现象,其中语义重复先于文本重复;3. 分析推理僵局如何触发循环开始,以及V型注意力机制如何驱动自我强化的持续循环;4. 采用累积和(CUSUM)算法捕获循环前兆,实现早期循环预测。

Result: 在多种大型推理模型上的实验验证了CUSUM算法在早期循环预测方面的准确性,并阐明了长链推理的稳定性特征。研究揭示了循环推理的内在机制:语义重复作为文本重复的前兆,推理僵局触发循环,V型注意力机制维持自我强化循环。

Conclusion: 本文系统性地识别并分析了大型推理模型中的"循环推理"失效模式,揭示了其作为状态坍缩的机制特征和V型注意力驱动的自我强化机制。通过LoopBench数据集和CUSUM算法,成功实现了循环的早期预测,为提高长链推理的稳定性和计算效率提供了理论基础和实用方法,有助于改善大型推理模型的可靠性。

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [18] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 本文提出了一种逻辑参数化的神经符号框架,用于可验证的自然语言推理,将逻辑形式从静态背景转变为可控组件,通过在高阶逻辑中嵌入多种经典和非经典形式系统,实现了更鲁棒、模块化和自适应的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型与定理证明器结合的自然语言推理方法依赖于固定的逻辑形式系统,这限制了系统的鲁棒性和适应性。不同推理领域(如规范推理、伦理推理、常识推理)可能需要不同的逻辑系统,因此需要一个能够灵活选择和切换逻辑形式的框架,使逻辑成为可参数化的组件而非静态背景。

Method: 采用LogiKEy方法论,将多种经典和非经典逻辑形式系统(包括一阶逻辑、道义逻辑、模态逻辑等)嵌入到高阶逻辑(HOL)中,构建逻辑参数化框架。比较了两种策略:逻辑外部方法(通过公理编码规范需求)和逻辑内部方法(规范模式从逻辑内置结构中产生)。系统地比较了不同逻辑形式在推理质量、解释精炼和证明行为方面的表现,特别关注规范推理领域。

Result: 广泛的实验表明,逻辑内部策略能够持续提升性能并为自然语言推理生成更高效的混合证明。研究发现逻辑的有效性具有领域依赖性:一阶逻辑在常识推理中表现更好,而道义逻辑和模态逻辑在伦理领域表现优异。不同逻辑形式系统在不同推理任务中展现出差异化的优势。

Conclusion: 将逻辑作为神经符号架构中的一等公民和参数化元素具有重要价值,能够实现更鲁棒、模块化和自适应的推理系统。逻辑参数化框架为可验证的自然语言推理提供了灵活性,使系统能够根据具体领域和任务需求选择最合适的逻辑形式,从而提升整体推理性能和适应能力。

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [19] [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)
*Yuxuan Zhou,Fei Huang,Heng Li,Fengyi Wu,Tianyu Wang,Jianwei Zhang,Junyang Lin,Zhi-Qi Cheng*

Main category: cs.AI

TL;DR: 本文提出了分层推测解码(HSD)方法,这是一种可证明无损的验证方法,通过在可访问分支间平衡过量和不足的概率质量来克服联合难解性问题,显著提升了推测解码中被接受token的数量,在保持分布保真度的同时大幅提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 推测解码中的验证环节是提高推理速度同时保持分布保真度的关键瓶颈。虽然序列级验证比逐token验证能接受更多token,但现有方案常依赖替代近似或受限于部分信息,难以处理联合难解性问题。因此需要一种更有效的验证方法来提高token接受率。

Method: 提出分层推测解码(HSD)方法,这是一种可证明无损的验证方法。核心思想是通过在可访问分支间平衡过量和不足的概率质量来克服联合难解性问题,从而显著提升预期被接受token的数量。该方法具有强可解释性和通用性,可以轻松集成到各种推测解码框架中。

Result: 大规模实验表明,HSD在不同模型家族和基准测试中都能持续提高接受率。将HSD集成到EAGLE-3中可获得超过12%的性能提升,在不损害分布保真度的情况下达到了最先进的解码效率。

Conclusion: HSD作为一种可证明无损的分层验证方法,成功解决了推测解码中的联合难解性问题,显著提升了token接受率和推理速度,同时保持了分布保真度。其强通用性使其可广泛应用于各种推测解码框架,为大语言模型推理加速提供了有效解决方案。

Abstract: Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.

</details>


### [20] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: 本文提出了PII-VisBench基准测试,用于评估视觉语言模型(VLM)在不同在线可见度主体上的隐私信息泄露问题。研究发现模型对高可见度主体更容易泄露个人身份信息(PII),并揭示了现有模型在隐私保护方面存在的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型隐私评估将隐私视为静态提取任务,忽略了主体在线存在度(即其在线数据量)对隐私对齐的影响。随着VLM越来越多地集成到隐私敏感领域,需要更全面地评估模型在不同在线可见度主体上的PII泄露风险,以及模型的隐私保护能力。

Method: 构建了包含4000个独特探测样本的PII-VisBench基准测试,将200个主体按在线可见度分为高、中、低、零四个类别。使用两个关键指标评估18个开源VLM模型(参数量0.3B-32B):拒绝率(Refusal Rate,即拒绝回答PII探测查询的百分比)和条件PII披露率(Conditional PII Disclosure Rate,即非拒绝响应中包含PII的比例)。此外还测试了改写和越狱式提示对模型的攻击效果。

Result: 所有模型表现出一致模式:随着主体可见度降低,拒绝率增加,PII披露率下降(从高可见度的9.10%降至低可见度的5.34%)。模型对高可见度主体更容易泄露PII,不同模型家族之间存在显著差异,不同PII类型的泄露程度也不同。改写和越狱式提示能够暴露出依赖于攻击方式和模型的安全失效问题。

Conclusion: 研究揭示了VLM在隐私保护方面存在系统性问题,特别是对高可见度主体的PII泄露风险更高。这表明需要开发考虑在线可见度的隐私安全评估方法和训练干预措施,以提高模型在不同可见度主体上的隐私保护能力,确保VLM在隐私敏感应用中的安全部署。

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [21] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了DynaDebate（动态多智能体辩论）框架，通过动态路径生成、过程中心辩论和触发式验证机制，解决了现有多智能体辩论系统中智能体推理路径趋同导致辩论失效的问题，在多个基准测试中取得了优于现有方法的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体辩论框架存在关键缺陷：依赖无引导的初始化导致智能体采用相同的推理路径并犯相同的错误，使得有效辩论受阻，最终结果退化为简单的多数投票。这限制了多智能体系统在协作决策和复杂问题解决中的推理和协作能力。

Method: DynaDebate框架包含三个核心机制：（1）动态路径生成与分配：使用专门的路径生成智能体生成多样化且逻辑严密的解决方案路径，并具有自适应冗余性；（2）过程中心辩论：将关注点从表面的结果投票转移到严格的逐步逻辑批判，以确保过程正确性；（3）基于触发的验证智能体：在出现分歧时激活，使用外部工具客观地解决僵局。

Result: 广泛的实验表明，DynaDebate在各种基准测试中实现了卓越的性能，超越了现有的最先进的多智能体辩论方法。

Conclusion: DynaDebate通过引入动态路径生成、过程导向的辩论机制和触发式验证，有效解决了传统多智能体辩论系统中的路径趋同和辩论退化问题，显著提升了多智能体系统在复杂推理任务中的协作效能和决策质量，为大语言模型多智能体系统的发展提供了新的解决方案。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [22] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的层次化多智能体框架,通过解耦高层协调与子任务执行,并利用结构化经验记忆和强化学习来重用协调经验,有效解决了基于大语言模型的多智能体系统在长期协作中的记忆管理问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的中心化多智能体系统在长期协作中存在严重问题:缺乏记忆管理导致上下文膨胀、错误累积和跨任务泛化能力差。此外,这些系统无法有效重用协调经验,存在任务级记忆效率低下的问题。

Method: 提出StackPlanner框架,采用层次化架构,通过主动任务级记忆控制将高层协调与子任务执行解耦;利用结构化经验记忆存储可重用的协调经验;结合强化学习来学习检索和利用这些经验,实现显式的记忆控制机制。

Result: 在多个深度搜索和智能体系统基准测试上进行了实验验证,结果表明StackPlanner能够有效支持可靠的长期多智能体协作,显著改善了记忆管理效率和协调经验的重用能力。

Conclusion: StackPlanner通过引入显式记忆控制和层次化架构,成功解决了大语言模型多智能体系统在长期协作中的关键挑战,为构建更可靠、更高效的多智能体协作系统提供了有效方案。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [23] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: 本文提出了TowerMind,一个基于塔防游戏的新型环境,用于评估大型语言模型(LLM)的长期规划和决策能力。该环境具有低计算需求和多模态观察空间,实验揭示了LLM与人类专家之间的性能差距,并指出了LLM在规划验证、决策多样性和行动效率方面的关键局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的实时战略(RTS)游戏环境要么计算需求高,要么缺乏对文本观察的支持,这限制了使用RTS游戏评估大型语言模型的长期规划和决策能力。因此需要一个计算需求低、支持多模态输入(包括像素、文本和结构化游戏状态)的新型评估环境,以更好地测试LLM作为智能体的核心能力。

Method: 提出TowerMind环境,基于塔防(TD)这一RTS游戏子类型构建。该环境保留了RTS游戏评估LLM的关键优势,同时具有低计算需求和多模态观察空间(像素、文本和结构化游戏状态表示)。设计了五个基准关卡,在不同多模态输入设置下评估多个广泛使用的LLM,并支持模型幻觉评估和高度可定制性。此外还评估了两种经典强化学习算法(Ape-X DQN和PPO)作为对比。

Result: 实验结果显示LLM与人类专家在能力和幻觉维度上存在明显的性能差距。实验进一步突出了LLM行为的关键局限性,包括:规划验证不足、决策缺乏多终局性(multifinality)以及行动使用效率低下。通过轻量级和多模态设计,TowerMind补充了现有的RTS游戏环境,并为AI智能体领域引入了新的基准测试。

Conclusion: TowerMind作为一个轻量级、多模态的塔防游戏环境,成功地为评估LLM的规划和决策能力提供了新的基准测试平台。实验揭示了当前LLM在战略规划、决策灵活性和行动效率方面的显著不足,为未来改进LLM智能体能力指明了方向。该环境补充了现有RTS游戏评估体系,源代码已在GitHub上公开,便于研究社区使用和扩展。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [24] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>

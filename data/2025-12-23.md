<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout](https://arxiv.org/abs/2512.18034)
*Joshua Gibson,Kapil Dhakal*

Main category: cs.AI

TL;DR: 本文将离散设施布局问题公式化为CNF，使用CDCL求解器进行可行性检测。结果显示CDCL的运行时间几乎恒定，远超CP-SAT和MILP。为解决优化限制，提出了两种结合CDCL和CP-SAT的混合架构，证明能显著加速精确优化过程。


<details>
  <summary>Details</summary>
Motivation: 研究CDCL（一种基于冲突驱动的子句学习的SAT求解技术）作为离散设施布局问题的计算引擎的潜力。旨在将设施布局问题建模为具有密集逻辑结构的组合分配问题，并评估CDCL在可行性检测和优化方面的计算效率，以期找到比现有CP-SAT和MILP方法更高效的求解器。

Method: 将设施布局问题建模为具有密集逻辑结构的组合分配问题，并将其约束（如邻接、分离等）转换为合取范式（CNF）。使用带有VSIDS启发式的CDCL求解器进行可行性检测，并与CP-SAT和MILP公式在统一基准框架下进行比较。为解决优化问题，引入了两种混合架构：一种用于快速枚举可行布局以权衡最优性与速度，另一种用于生成热启动解决方案以加速精确优化。

Result: 在可行性检测方面，随着问题规模和约束密度的增加，CDCL显示出近乎恒定的运行时间行为，而CP-SAT和MILP则分别表现出多项式和指数级的扩展。引入的两种混合架构能够显著减少求解所需的总时间（time-to-solution），同时确保解的正确性。

Conclusion: CDCL是一种强大的计算引擎，特别适用于离散设施布局问题的可行性检测，其效率优于CP-SAT和MILP。通过引入结合CDCL和CP-SAT的混合架构，可以有效利用CDCL的快速搜索能力来加速精确优化过程，从而在大型布局问题中实现求解时间的大幅减少，同时保留正确性保证。

Abstract: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.

</details>


### [2] [Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability](https://arxiv.org/abs/2512.18092)
*Ge Yan,Tuomas Oikarinen,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 本文将神经元识别视为机器学习的逆过程，首次提供了关于神经元解释“忠实性”和“稳定性”的理论分析，并通过推导泛化界限和提出自举集成（BE）方法来保证解释的可信赖性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经元识别算法（如Network Dissection和CLIP-Dissect）缺乏严格的理论基础，导致其解释结果缺乏可信度和可靠性，因此迫切需要一个理论框架来确保解释的质量。

Method: 1. 观察到神经元识别可视为机器学习的逆过程，并以此作为理论基础。2. 针对忠实性，推导了常用相似性指标（如准确率、AUROC、IoU）的泛化界限。3. 针对稳定性，提出了自举集成过程来量化稳定性，并提出了BE（Bootstrap Explanation）方法，用于生成具有保证覆盖概率的概念预测集。

Result: 在合成数据和真实数据上的实验验证了所提出的理论结果，并证明了所提出的BE方法的实用性。

Conclusion: 本工作通过首次对神经元解释的“忠实性”和“稳定性”进行理论分析，并提出了保证这两种性质的方法（包括BE方法），朝着可信赖的神经元识别迈出了重要一步。

Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.

</details>


### [3] [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)
*Zijun Wang,Yijiahao Qi,Hanqiu Chen,Zishen Wan,Gongjin Sun,Dongyang Li,Shuyi Pei,Cong Hao*

Main category: cs.AI

TL;DR: 提出了一种算法-系统协同设计（algorithm-system co-design）来优化MoA推理服务。通过引入层次树拓扑、运行时自适应机制和执行流水线，该方法将端到端延迟降低了高达90%，同时保持了相似的准确性。


<details>
  <summary>Details</summary>
Motivation: 混合智能体（Mixture-of-Agents, MoA）推理服务中存在智能体间通信密集和硬件利用率低的问题，这些问题共同导致了较高的服务延迟。

Method: 本文采用算法-系统协同设计来解决瓶颈：1. 用层次树拓扑结构取代密集智能体交互图，实现智能体间通信的结构化稀疏性。2. 引入运行时自适应机制，利用中间输出的语义一致性和置信度信号选择性地终止或跳过后续智能体调用。3. 通过在依赖相关的智能体之间重叠增量预填充和解码，实现智能体执行的流水线化，从而提高硬件利用率。

Result: 相较于密集连接的MoA基线，该方法大幅降低了端到端延迟（最高达90%），同时保持了可比的准确性（在±1%以内），并在某些特定场景下还能提高准确性。

Conclusion: 通过算法-系统协同设计，可以有效解决MoA推理中密集通信和低硬件利用率的瓶颈，从而在保持准确性的同时，显著提升服务效率和降低推理延迟。

Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.

</details>


### [4] [Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications](https://arxiv.org/abs/2512.18135)
*Cristiano da Costa Cunha,Wei Liu,Tim French,Ajmal Mian*

Main category: cs.AI

TL;DR: 本文系统性回顾了因果推理（CI）与强化学习（RL）交叉领域（即因果强化学习, CRL）的最新进展，旨在解决传统RL在可解释性、鲁棒性和泛化性方面的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习（RL）依赖于相关性驱动的决策，在面对分布偏移、混杂变量和动态环境时，存在可解释性低、鲁棒性差和泛化失败的问题。因果强化学习（CRL）通过显式建模因果关系，旨在克服这些局限。

Method: 本文对现有研究进行了系统性的综述，将方法分为五个关键类别：因果表征学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性。通过这种结构化分析，识别了普遍挑战并讨论了开放性问题。

Result: 综述识别了因果强化学习领域普遍存在的挑战、突出了在实际应用中的经验成功案例，并讨论了亟待解决的开放性问题。

Conclusion: 因果强化学习（CRL）具有巨大潜力，能够开发出更鲁棒、更具泛化性和可解释性的人工智能系统。

Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.

</details>


### [5] [Propose, Solve, Verify: Self-Play Through Formal Verification](https://arxiv.org/abs/2512.18160)
*Alex Wilf,Pranjal Aggarwal,Bryan Parno,Daniel Fried,Louis-Philippe Morency,Paul Pu Liang,Sean Welleck*

Main category: cs.AI

TL;DR: 针对代码生成中自对弈奖励信号不可靠的问题，本文提出了基于形式化验证的自对弈框架PSV（提议、解决、验证）。PSV-Verus模型在三个基准测试上将pass@1提高了高达9.6倍，证明了该框架的有效性，并强调了形式化验证和难度感知提议的重要性。


<details>
  <summary>Details</summary>
Motivation: 在AI中，仅通过自对弈训练模型是一个长期目标，但其在大型语言模型（特别是代码生成）中的有效性尚不明确，因为基于单元测试的奖励信号不可靠且易导致错误传播。本文旨在利用形式化验证提供的可靠性信号，研究验证代码生成环境下的自对弈训练。

Method: 引入了“提议、解决、验证”（PSV）自对弈框架。该框架利用形式化验证信号来创建一个能够生成具有挑战性的合成问题的“提议者”（Proposer），并使用专家迭代方法训练一个“解决者”（Solver）。

Result: 使用PSV训练的PSV-Verus模型在三个基准测试上，相对于仅推理和专家迭代基线，pass@1提高了高达9.6倍。研究还表明，性能随着生成问题的数量和训练迭代次数的增加而扩展。

Conclusion: 形式化验证和难度感知的提议是成功实现自对弈的关键要素。PSV框架显著提高了验证代码生成的性能。

Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.

</details>


### [6] [NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI](https://arxiv.org/abs/2512.18177)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: NEURO-GUARD是一种知识引导的视觉框架，结合ViT和LLM（通过RAG机制），利用临床知识指导特征提取代码的生成和优化，显著提高了医学图像诊断的准确性、可解释性和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像AI模型多为纯数据驱动的“黑箱”模型，缺乏可解释性和跨域泛化能力，难以应用于数据有限、视觉线索细微和高风险的临床决策环境。

Method: 提出NEURO-GUARD框架，它将Vision Transformers (ViTs)与语言驱动的推理相结合。该框架利用检索增强生成 (RAG) 机制进行自验证，通过大型语言模型 (LLM) 迭代地生成、评估和优化基于临床指南和专家知识的医学图像特征提取代码。

Result: 在糖尿病视网膜病变分类任务的四个基准数据集上，NEURO-GUARD的准确率比纯ViT基线提高了6.2%（84.69% vs 78.4%），领域泛化能力提高了5%。此外，在基于MRI的癫痫发作检测评估中也证实了其跨域鲁棒性。

Conclusion: NEURO-GUARD成功地将符号医学推理与亚符号视觉学习结合起来，实现了可解释、知识感知和可泛化的医学图像诊断，并在多个数据集上达到了最先进的性能。

Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.

</details>


### [7] [NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework](https://arxiv.org/abs/2512.18189)
*Zihao Deng,Yijia Li,Renrui Zhang,Peijun Ye*

Main category: cs.AI

TL;DR: 提出NL2CA方法，该方法利用LLM和无监督批评树将自然语言描述自动转化为符号认知框架下的可执行规则，并结合认知强化学习构建智能体，实现了可扩展、可解释且与人类对齐的认知建模。


<details>
  <summary>Details</summary>
Motivation: 认知计算模型可以形式化且可解释地描述人类的审议和决策过程，但其开发过程非常耗时费力。因此，需要一种自动化、可扩展的方法，能够从人类经验的自然语言描述中自动提取并形式化决策规则。

Method: 提出NL2CA方法，旨在从自然语言描述中自动形式化认知决策规则。方法流程包括：(1) 使用微调的大型语言模型（LLM）将文本翻译成线性时序逻辑（LTL）；(2) 通过无监督的批评树（Critic Tree）精炼LTL逻辑；(3) 将LTL转换为与符号认知框架兼容的可执行生成规则；(4) 根据真实世界的行为数据，通过认知强化学习（CRL）进一步构建和优化认知智能体。该方法完全自动化，无需人工干预。

Result: 方法在两个领域得到验证：(1) 在NL-to-LTL翻译任务中，CriticNL2LTL模块在专家和大规模基准测试中均表现出稳定一致的性能，且无需人工反馈；(2) 在认知驾驶模拟中，从人类访谈自动构建的智能体成功学习了约70个关键场景中的多样化决策模式。

Conclusion: 实验证明NL2CA能够从非结构化文本数据中实现可扩展、可解释且与人类行为一致的认知建模，为自动设计符号认知智能体提供了一种新范式。

Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.

</details>


### [8] [External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning](https://arxiv.org/abs/2512.18190)
*Jian Yan*

Main category: cs.AI

TL;DR: 提出“外部海马体”框架，将小模型推理建模为语义空间的能量流。通过降维构建拓扑认知图，在无需训练的情况下解决了多步推理中的“认知僵局”问题，准确率提升16.80%，速度提升15倍以上。


<details>
  <summary>Details</summary>
Motivation: 解决小型语言模型（参数量<=7B）在多步推理中常出现的“认知僵局”（cognitive deadlock）问题，并提供一种计算成本低、无需额外训练、可预测干预的推理机制。

Method: 提出了“外部海马体”（External Hippocampus）框架。该框架从认知动力学的角度，将语言模型的推理建模为语义空间中的信息能量流，并利用降维投影技术构建“拓扑认知图”。该方法能够在测试时精确导航和干预能量流，而无需进行传统的权重空间优化。

Result: 在参数量<=7B的模型上，地图引导方法在500个挑战性问题上实现了81.20%的准确率（相对基线提升16.80%），并将推理时间减少了至少15倍。关键发现是推理停滞表现为“认知漩涡”和低熵势阱，通过温度扰动可以有效重启能量流。

Conclusion: “外部海马体”框架无需额外训练，具有自主成长能力，为小型语言模型的推理提供了一种高效、可控且拓扑感知的解决方案。

Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.

</details>


### [9] [Sophia: A Persistent Agent Framework of Artificial Life](https://arxiv.org/abs/2512.18202)
*Mingyang Sun,Feng Hong,Weinan Zhang*

Main category: cs.AI

TL;DR: 提出并实现了System 3（第三层认知系统）和“持久性智能体”Sophia框架，旨在赋予LLM智能体身份连续性、自我改进能力和长期适应性，最终实现复杂任务成功率提升40%和重复操作推理步骤减少80%。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体架构虽然在感知和审议方面表现出色，但仍是静态和被动的，缺乏一个持久的元层来维护身份、验证推理并将短期行为与长期生存目标对齐，且受限于狭窄、手动定义的场景。

Method: 提出并定义了超越感知（System 1）和审议（System 2）的第三认知层级——System 3，负责智能体的叙事身份和长期适应。基于此，设计了“持久性智能体”（Persistent Agent）**Sophia**框架。Sophia通过四个协同机制实现持续自我改进：过程监督思维搜索、叙事记忆、用户和自我建模，以及混合奖励系统。

Result: 定量方面，Sophia能够独立发起和执行内在任务；重复操作的推理步骤减少了80%；元认知持久性使高复杂性任务的成功率提高了40%，有效弥合了简单和复杂目标之间的性能差距。定性方面，该系统展现出连贯的叙事身份和固有的任务组织能力。

Conclusion: 通过将心理学洞察与轻量级的强化学习核心相结合，持久性智能体架构为实现人工生命提供了一条可行的实践途径，并推进了智能体的设计，超越了传统的静态反应模式。

Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.

</details>


### [10] [MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification](https://arxiv.org/abs/2512.18256)
*Sirui Li,Wangyue Lu,Xiaorui Shi,Ke Weng,Haozhe Sun,Minghe Yu,Tiancheng Zhang,Ge Yu,Hengyu Liu,Lun Du*

Main category: cs.AI

TL;DR: 针对现有大型语言模型定理证明器在数学推理中泛化性差的问题，本文提出了跨越60个数学分支的MSC-180基准测试集。评估结果显示，最佳模型通过率仅为18.89%，存在严重的领域偏差和难度差距，表明模型缺乏真正的可迁移推理能力，仍依赖于模式匹配。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明（ATP）是人工智能实现形式推理和验证的核心研究方向。然而，现有基于大型语言模型（LLM）的定理证明器在数学推理方面存在局限性，具体表现为领域覆盖范围受限和泛化能力弱。需要一个更具区分度和系统性的基准来推动下一代人工智能系统具备真正的数学推理能力。

Method: 提出了一个名为MSC-180的评估基准，该基准基于MSC2020数学主题分类。它包含180个经过领域专家多轮验证和完善的正式验证问题，涵盖60个数学分支，难度从本科到研究生级别。此外，引入变异系数（CV）作为衡量跨数学领域性能变异性的评估指标。

Result: 在pass@32设置下，对现有最先进的LLM定理证明器进行评估，最佳模型的总体通过率仅为18.89%。模型存在显著的领域偏差（最大领域覆盖率41.7%）和难度差异（在研究生级别问题上的通过率显著较低）。观察到的变异系数（CV）值比高变异性统计阈值高出4-6倍，表明模型缺乏可迁移的推理机制。

Conclusion: 评估结果表明，当前模型仍然依赖于训练语料库中的模式匹配，而非具备可迁移的推理机制和系统的泛化能力。MSC-180及其多维评估框架为开发具有真正数学推理能力的下一代AI系统提供了一个有力的、系统化的基准。

Abstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.

</details>


### [11] [Monitoring Monitorability](https://arxiv.org/abs/2512.18311)
*Melody Y. Guan,Miles Wang,Micah Carroll,Zehao Dou,Annie Y. Wei,Marcus Williams,Benjamin Arnav,Joost Huizinga,Ian Kivlichan,Mia Glaese,Jakub Pachocki,Bowen Baker*

Main category: cs.AI

TL;DR: 本文提出并评估了AI系统思维链（CoT）的“可监控性”，以确保安全部署。通过引入新的指标和评估套件，研究发现CoT监控比仅基于行动的监控更有效，且该特性在模型规模扩展和RL优化中保持相对稳健。结果显示，可以通过使用更长的CoT或增强监控器的能力来提高可监控性。


<details>
  <summary>Details</summary>
Motivation: 为了安全部署能力日益增强的AI系统，需要对其决策过程具备可观测性。现有研究表明监控思维链（CoT）对检测模型不当行为有效，但这种“可监控性”在不同的训练过程、数据源或系统规模扩展下可能不稳定或脆弱，因此需要对其进行测量和跟踪。

Method: 提出了三种评估可监控性的原型（干预、过程和结果属性）以及一个新的可监控性指标。构建了一个广泛的评估套件，用于测试经过训练具有混淆CoT的模型。通过对比CoT监控和仅基于行动的监控，并在各种前沿模型上进行实际评估。进一步研究了可监控性如何随着推理计算、强化学习（RL）优化和预训练模型规模而变化。

Result: 所提出的评估方法能够成功捕捉经过训练以混淆CoT的简单模型生物。CoT监控在实际环境中比仅基于行动的监控更有效。大多数前沿模型具有良好但并非完美的监控性。研究发现，CoT越长，可监控性通常越高。强化学习（RL）优化在当前前沿规模下并未显著降低可监控性。在保证能力一致的情况下，部署计算成本更高、推理努力更大的较小模型，可以获得更高的可监控性。通过扩展弱监控器的测试时计算资源，并向其提供CoT，可以显著提高可监控性，并使可监控性的提升趋势更加陡峭。通过询问后续问题并将随后的CoT提供给监控器，可以改善可监控性。

Conclusion: 思维链（CoT）监控是一种有效且强大的工具，用于检测现代AI系统的潜在不当行为。尽管目前大多数前沿模型具有良好的可监控性，并且这种特性在模型扩展和RL优化下表现出相对的鲁棒性，但通过延长CoT、优化监控器（例如增加其计算资源或提供CoT）以及引入后续问答机制，可以进一步提高系统的可监控性。

Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.

</details>


### [12] [Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation](https://arxiv.org/abs/2512.18412)
*Mykyta Lapin,Kostiantyn Bokhan,Yurii Parzhyn*

Main category: cs.AI

TL;DR: 本文提出了一种无需反向传播的、可解释的小样本轮廓图像分类方法。该方法将图像编码为属性图，通过结构归约和迭代组合形成类概念吸引子，并使用近似图编辑距离（GED）进行匹配分类。


<details>
  <summary>Details</summary>
Motivation: 旨在设计和实验验证一种无需反向传播（backpropagation）的架构，该架构能够利用少量样本（每类5-6个）通过结构和参数归约形成类概念，并提供透明（可解释）的决策。

Method: 首先对轮廓图像进行矢量化，并将其编码为带属性图（节点为关键点/线，属性包括坐标、长度、角度、方向等几何特征）；接着通过消除不稳定子结构和对齐路径进行结构归约；最后通过样本的迭代组合形成类级概念吸引子，并使用近似图编辑距离（GED）选择最佳图-概念匹配进行分类。

Result: 在每类只有5-6个基础样本的MNIST子集上（单周期），该方法获得了约82%的稳定准确率。所有决策都具有完全可追溯性，错误分类可以通过明确的结构相似性来解释。该文还提供了与SVM、MLP、CNN以及度量和元学习基线的对比参考。

Conclusion: 结构图方案结合概念吸引子实现了无需反向传播的小样本学习，并提供了内置的解释性。其主要限制在于图编辑距离（GED）的计算成本和骨架化质量。

Abstract: We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.

</details>


### [13] [Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System](https://arxiv.org/abs/2512.18450)
*Xavier Rafael-Palou,Jose Munuera,Ana Jimenez-Pastor,Richard Osuala,Karim Lekadir,Oliver Diaz*

Main category: cs.AI

TL;DR: 针对多中心临床AI系统中的性能漂移问题，我们提出了一种基于智能体（agent-based）、具备站点感知能力的自适应漂移监测框架。实验证明，该框架在漂移检测和严重性分类方面显著优于集中式监测。


<details>
  <summary>Details</summary>
Motivation: 现代多中心临床决策支持系统（CDSS）的预测性能会因不同站点的患者群体、成像硬件和采集协议的变化（即分布漂移）而下降。现有的漂移监测方法主要依赖于集中式聚合预测监控，忽略了站点特有的漂移动态。

Method: 提出了一个用于检测和评估多中心临床AI系统漂移严重性的基于智能体（agent-based）的框架。通过模拟多中心环境，为每个站点分配一个漂移监测智能体，该智能体批量比较模型输出与参考分布。分析了多种获取参考分布的方案（站点特定、全局、仅生产数据、自适应）以及集中式基线。

Result: 在真实乳腺癌影像数据上进行实验发现，所有多中心监测方案的漂移检测性能均优于集中式监测（F1分数提升高达10.3%）。在缺乏站点特定参考分布的情况下，自适应方案表现最佳，其漂移检测F1分数为74.3%，漂移严重性分类F1分数为83.7%。

Conclusion: 适应性、具备站点感知能力的基于智能体的漂移监测方法可以显著提高多中心临床决策支持系统的可靠性。

Abstract: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.

</details>


### [14] [Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)
*Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 评估了大型语言模型（LLMs）在动态环境中的在线推理能力。发现LLMs的信念更新机制类似贝叶斯滤波，但更准确地表现为“指数遗忘滤波器”，系统性地忽略旧证据。提出了校准先验知识的提示策略。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要关注LLMs在静态任务中的推理能力，忽略了它们作为世界模型或智能体时在动态和随机环境中所需的关键能力——即信念必须持续更新的在线适应性。

Method: 引入了一个贝叶斯滤波框架来评估LLMs的在线推理能力。构建了一个概率探针套件，包含参数随时间变化的多元离散分布（如掷骰子）和连续分布（如高斯过程）。

Result: LLMs的信念更新机制与贝叶斯后验相似，但更精确地被描述为一个具有模型特定折扣因子（小于1）的“指数遗忘滤波器”，表明系统性地低估了旧证据。虽然固有的先验知识通常失准，但更新机制本身是结构化和有原则的。研究进一步在模拟智能体任务中验证了这些发现，并提出了有效的提示策略来重新校准先验知识。

Conclusion: LLMs具备结构化和有原则的在线信念更新机制，尽管它们倾向于指数式地遗忘旧证据（表现为指数遗忘滤波器），并且固有先验需要通过提示工程进行有效校准。

Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.

</details>


### [15] [Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V](https://arxiv.org/abs/2512.18564)
*John Chen,Sihan Cheng,Can Gurkan,Ryan Lay,Moez Salahuddin*

Main category: cs.AI

TL;DR: 本文提出了Vox Deorum，一种将LLM用于宏观战略推理并将战术执行外包给子系统的混合架构，并在《文明V》中实现了与算法AI相媲美的端到端游戏性能，展现了不同的游戏风格。


<details>
  <summary>Details</summary>
Motivation: LLMs具有自然语言推理能力，非常适用于4X和宏大战略游戏，以实现更自然的人机交互（如协作和谈判）。然而，这些游戏固有的复杂性和长周期性以及LLM的延迟和成本是部署的主要挑战。

Method: 引入了**Vox Deorum**，一种**混合LLM+X架构**。该分层技术设计将**宏观战略推理**能力赋予LLM，并将**战术执行**委托给子系统（例如，现有算法AI或未来的强化学习AI）。通过简单的提示词，验证了两个开源LLM。

Result: 通过2,327场完整的游戏验证，LLMs实现了具有竞争力的端到端游戏表现。更重要的是，LLMs展现出的游戏风格与传统的算法AI以及彼此之间都有显著差异。

Conclusion: 本工作为将LLMs整合到商业4X游戏中建立了一种可行的架构，为游戏设计和智能体AI研究开辟了新的机遇。

Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.

</details>


### [16] [ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning](https://arxiv.org/abs/2512.18571)
*Weijie Zhou,Xuangtang Xiong,Ye Tian,Lijun Yue,Xinyu Wu,Wei Li,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang,Zhengyou Zhang*

Main category: cs.AI

TL;DR: 提出了 ESearch-R1 成本感知具身推理框架，并使用 HC-GRPO 算法优化 MLLM 智能体，使其能够在面对模糊指令时，策略性地平衡物理探索和人机交互的成本，从而显著提高了任务成功率，并减少了约 50% 的总运营成本。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能体在面对模糊的自然语言指令时，难以策略性地平衡高昂的物理探索成本和人机交互的认知成本，通常将消歧视为被动感知问题，缺乏最小化总任务执行成本的战略推理能力。

Method: 提出了 ESearch-R1 成本感知具身推理框架，将交互式对话（Ask）、情景记忆检索（GetMemory）和物理导航（Navigate）统一到一个决策过程中。同时引入了 HC-GRPO（异构成本感知组相对策略优化）算法，该算法通过采样推理轨迹组，并强化在信息增益和异构成本（如导航时间和人类注意力）之间达成最佳权衡的组，来优化多模态大语言模型（MLLM）。

Result: 在 AI2-THOR 环境中的广泛实验表明，ESearch-R1 显著优于标准的基于 ReAct 的智能体，提高了任务成功率，同时将总运营成本降低了约 50%。

Conclusion: 实验结果验证了 HC-GRPO 在使 MLLM 智能体与物理世界约束对齐方面的有效性，证明了 ESearch-R1 框架能够显著提高任务成功率并降低总运营成本。

Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.

</details>


### [17] [Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction](https://arxiv.org/abs/2512.18605)
*Qinglin Zeng,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 为提高LLMs推理效率，本文提出了“反射置信度”框架，将低置信度信号转化为自我反思和错误修正的触发器，而非停止指令。该方法在相似的计算成本下，显著提升了数学推理任务的准确性，证明了主动自我修正优于被动丢弃策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的集成推理方法（如自洽性）计算成本高昂。虽然现有的提前停止策略（如DeepConf）可以降低成本，但它们通过丢弃低置信度的推理路径来节省计算，浪费了部分已完成的计算。

Method: 提出“反射置信度”推理框架。当置信度低于阈值时，模型不停止，而是生成一个“反思提示”来分析当前推理状态，识别潜在错误，并沿着修正后的轨迹继续生成。这是一种将低置信度信号转化为反思触发器的主动自我修正策略。

Result: 在包括AIME 2025在内的数学推理基准测试中，该方法在与先进的提前停止基线相当的计算成本下，实现了显著高于它们的准确性提升。

Conclusion: 实验验证了主动的自我修正（reflective confidence）比被动地丢弃推理轨迹（early stopping）更为有效。

Abstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.
  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.

</details>


### [18] [Assignment-Routing Optimization: Solvers for Problems Under Constraints](https://arxiv.org/abs/2512.18618)
*Yuan Qilong,Michal Pavelka*

Main category: cs.AI

TL;DR: 研究了联合路径规划与分配（JRA）问题，并针对实际包装场景的复杂约束（如多占位符、时间限制）开发了定制的混合整数规划（MIP）求解器。该方法能够稳定找到全局最优解，计算速度比现有精确求解器快一个数量级，且解决方案质量远超贪婪基线，具有高度的实用性。


<details>
  <summary>Details</summary>
Motivation: 解决联合路径规划与分配（JRA）问题，特别是将其应用于实际的包装规划场景中。现有方法不足以有效处理现实中更丰富的约束条件。

Method: 提出了一种定制的混合整数规划（MIP）求解器。该求解器扩展了现有的基于Gurobi和割平面子环消除的精确MIP方法，以集成处理实际包装场景中的复杂约束，如多占位符选项、时间框架限制和多类别物品包装。

Result: 在46个移动操作数据集上进行了测试。提出的MIP方法能够稳定且以较低的计算时间达到全局最优解。计算速度显著优于基于shaking的精确求解器，最高快了一个数量级。与贪婪基线相比，MIP解决方案保持了最优距离，平均偏差仅为简单启发式的14%。

Conclusion: 结果证明了基于MIP的JRA优化方法具有很高的效率和解决方案质量，凸显了其在机器人包装、运动规划和复杂物流等领域的实际应用价值。

Abstract: We study the Joint Routing-Assignment (JRA) problem in which items must be assigned one-to-one to placeholders while simultaneously determining a Hamiltonian cycle visiting all nodes exactly once. Extending previous exact MIP solvers with Gurobi and cutting-plane subtour elimination, we develop a solver tailored for practical packaging-planning scenarios with richer constraints.These include multiple placeholder options, time-frame restrictions, and multi-class item packaging. Experiments on 46 mobile manipulation datasets demonstrate that the proposed MIP approach achieves global optima with stable and low computation times, significantly outperforming the shaking-based exact solver by up to an orders of magnitude. Compared to greedy baselines, the MIP solutions achieve consistent optimal distances with an average deviation of 14% for simple heuristics, confirming both efficiency and solution quality. The results highlight the practical applicability of MIP-based JRA optimization for robotic packaging, motion planning, and complex logistics .

</details>


### [19] [ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting](https://arxiv.org/abs/2512.18661)
*Hafiz Saif Ur Rehman,Ling Liu,Kaleem Ullah Qasim*

Main category: cs.AI

TL;DR: 本文提出ASTIF（自适应语义-时间整合框架），一个利用SLM提取语义、结合LSTM-RF进行时间建模、并通过置信度元学习器实时调整预测权重的混合智能系统，用于加密货币价格预测，表现优于主流深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 现有的金融时间序列预测模型依赖静态架构，难以整合异构知识源（如政策不确定性和市场叙事等语义驱动因素），也无法适应快速的市场机制变化。金融预测本质上是一个信息融合的挑战。

Method: 提出ASTIF（加密货币价格预测的自适应语义-时间整合）框架，这是一个通过基于置信度的元学习来实时调整预测策略的混合智能系统。该框架集成了三个互补组件：1. 使用MirrorPrompt的双通道小型语言模型(SLM)提取语义市场线索和数值趋势。2. 混合LSTM-随机森林模型捕获序列时间依赖性。3. 一个置信度感知的元学习器作为自适应推理层，根据实时不确定性调整每个预测器的贡献。

Result: 在2020年至2024年AI加密货币和主要科技股数据集上的实验表明，ASTIF显著优于领先的深度学习和Transformer基线（如Informer、TFT）。消融研究证实，自适应元学习机制通过在市场动荡期间转移对语义和时间通道的依赖，成功减轻了风险。

Conclusion: ASTIF为在非平稳环境中融合定量和定性数据提供了一个可扩展的、基于知识的解决方案。

Abstract: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.
  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.

</details>


### [20] [Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking](https://arxiv.org/abs/2512.18665)
*Dmitry Bennett,Fernand Gobet*

Main category: cs.AI

TL;DR: 本文提出了CogAct计算模型，该模型基于组块化（chunking）等基本认知过程（STM/LTM），能够自适应地学习各种概念，包括逻辑函数、人工类别以及文学、国际象棋和音乐等领域的原始自然概念。CogAct成功模拟了人类个体的概念主观判断，展示了其在认知心理学领域整合概念学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 认知科学中的一个关键问题是理解短期和长期记忆中多种概念形成和检索的潜在基本心理过程（如组块化）。目前的心理学模型或非GPT深度学习模型难以自适应地学习各种复杂概念，需要一个能够将概念学习融入基本认知结构（如组块化、注意、STM和LTM）的统一计算模型。

Method: 采用CogAct计算模型，该模型将概念学习植根于组块化、注意、STM和LTM等基本认知结构。通过两种方式进行验证：首先，展示其在学习从简单逻辑到复杂原始自然概念（文学、国际象棋、音乐）时的自适应能力。其次，设计考虑主观性的新颖人类基准，通过模拟个体参与者对音乐的主观判断，并从原始乐谱数据中学习概念，将结果与深度学习模型进行比较。

Result: CogAct模型原则上证明了其能够自适应地学习从简单到复杂原始自然概念的广泛类别。与现有模型相比，它不需要进行特定任务的架构更改。此外，CogAct成功模拟了人类个体参与者的主观概念空间，准确捕捉了他们在音乐中的主观判断，且学习过程基于原始数据。

Conclusion: 这些发现将概念学习和对复杂性的适应性整合到更广泛的认知心理学理论中。该方法还可应用于心理学实践，以便建模个体参与者的主观概念空间，从而超越对“平均参与者”的传统建模。

Abstract: A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.

</details>


### [21] [Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model](https://arxiv.org/abs/2512.18687)
*Yosuke Taniuchi,Chie Hieida,Atsushi Noritake,Kazushi Ikeda,Masaki Isoda*

Main category: cs.AI

TL;DR: 本研究通过比较三种计算模型，发现猴子的社会比较依赖于客观的奖励差异（ECM模型表现最佳），而非对同伴主观奖励价值的推断。


<details>
  <summary>Details</summary>
Motivation: 社会比较是灵长类社会认知的基本组成部分，但从计算角度尚不清楚同伴的奖励信息如何影响个体对自身奖励的评估。研究旨在探究猴子在社会比较中是仅识别客观奖励差异，还是会推断同伴的主观奖励价值。

Method: 开发了三种计算模型：忽略同伴信息的NCM（无比较模型）、推断同伴主观价值的IPM（内部预测模型），以及直接纳入同伴客观奖励的ECM（外部比较模型）。使用多层、多模态的潜在狄利克雷分配对模型进行测试，并在包含一对猴子行为、奖励和条件刺激的数据集上进行训练和评估（评估指标为Rand Index下的主观价值分类能力）。

Result: 在设定的条件下，外部比较模型（ECM）获得了最高的分类分数（Rand Index为0.88），显著高于内部预测模型（IPM，0.79）。

Conclusion: 猴子的社会比较依赖于对客观奖励差异的识别，而不是对同伴主观奖励状态的推断。

Abstract: Social comparison -- the process of evaluating one's rewards relative to others -- plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.

</details>


### [22] [KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing](https://arxiv.org/abs/2512.18709)
*Zhifei Li,Lifan Chen,Jiali Yi,Xiaoju Hou,Yue Zhao,Wenxin Huang,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: 针对知识追踪中单点估计的歧义性，本文提出了KeenKT模型。KeenKT使用正态逆高斯（NIG）分布来建模学生的知识状态波动，并结合了NIG距离注意力机制、去噪重建损失和对比学习损失，显著提高了预测精度和模型鲁棒性（最大AUC提升5.85%，ACC提升6.89%）。


<details>
  <summary>Details</summary>
Motivation: 现有的知识追踪（KT）方法多依赖于单点估计来判断学生的知识掌握程度。这种方法无法区分学生的真实能力、偶然爆发或粗心失误，从而在判断知识掌握度时产生了歧义。

Method: 提出KeenKT模型（Knowledge Mastery-State Disambiguation for Knowledge Tracing）。
1. 使用正态逆高斯（NIG）分布来表示每次交互中的学生知识状态，以捕捉学习行为的波动性。
2. 设计了基于NIG距离的注意力机制，用于建模知识状态的动态演化。
3. 引入基于扩散的去噪重建损失和分布对比学习损失，以增强模型的鲁棒性。

Result: 在六个公开数据集上的广泛实验表明，KeenKT在预测准确性和对行为波动的敏感性方面均优于现有的SOTA模型。模型的最大AUC提升达到5.85%，最大ACC提升达到6.89%。

Conclusion: KeenKT通过引入NIG分布来精确建模知识状态的波动性和不确定性，成功解决了单点估计带来的歧义问题，显著提高了知识追踪的准确性和鲁棒性。

Abstract: Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.

</details>


### [23] [Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth](https://arxiv.org/abs/2512.18732)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本文提出了一个几何框架，利用最小描述长度（MDL）原则来解释概念表征基础如何扩展。研究表明，概念创新（想象力）必须被残差错误所约束，新概念方向必须位于经验残差所定义的几何空间内。


<details>
  <summary>Details</summary>
Motivation: 解决现有学习模型普遍预设固定表征基础的问题。探究在现有表征无法解释经验时，表征基础本身如何在结构上进行有原则、有选择的扩展。

Method: 提出一个几何框架，将概念增长建模为“可接受的基扩展”（admissible basis extension），并使用最小描述长度（MDL）准则进行评估。经验被表示为相对于当前概念子空间的向量，残差分量捕捉表征失败，候选扩展被限制在低秩变换中。

Result: 证明任何MDL接受的扩展，其新方向必须完全位于经验引起的残差跨度内，而与残差跨度正交的扩展会被拒绝。这表明想象力只能在暴露或放大结构化残差错误时对学习有贡献，不能引入任意、独立的新颖性。

Conclusion: 该框架将概念发展定义为一个由系统性错误驱动、受几何结构严格约束的基扩展过程，从而明确了想象力在学习和理论变革中的作用和边界。

Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.

</details>


### [24] [MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking](https://arxiv.org/abs/2512.18755)
*Jianyi Zhang,Shizhao Liu,Ziyin Zhou,Zhen Li*

Main category: cs.AI

TL;DR: 本文提出了 MEEA（纯粹暴露效应攻击），这是一个基于心理学原理、全自动的黑盒多轮越狱框架。它利用重复的低毒性暴露，通过模拟退火优化来动态侵蚀 LLM 的安全边界。实验证明 MEEA 比现有基线提高了超过 20% 的攻击成功率，并揭示了 LLM 安全对齐的动态性。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱研究大多假设 LLM 的安全边界是静态的，忽略了上下文交互对模型行为的动态影响，导致现有策略在稳定性和泛化性方面有限。因此需要一个能评估多轮交互中安全鲁棒性的动态攻击框架。

Method: 提出 MEEA (Mere Exposure Effect Attack)，一个受心理学“纯粹暴露效应”启发的全自动黑盒框架。该方法利用重复的低毒性语义暴露，诱导模型有效安全阈值逐渐偏移，从而在持续交互中侵蚀安全对齐约束。具体实现上，MEEA 构建语义渐进的提示链，并使用模拟退火（Simulated Annealing）策略进行优化，优化指标包括语义相似度、毒性和越狱有效性。

Result: 在 GPT-4、Claude-3.5 和 DeepSeek-R1 等闭源和开源模型上进行了广泛实验。MEEA 始终比七个有代表性的基线取得了更高的攻击成功率，平均攻击成功率（ASR）提升超过 20%。消融研究进一步验证了基于模拟退火的优化和上下文暴露机制的必要性。

Conclusion: MEEA 有效提升了多轮越狱的成功率。研究结果表明 LLM 的安全行为本质上是动态且依赖于历史记录的，这挑战了静态对齐边界的假设，并强调了未来安全评估和防御机制需要具备交互感知能力。

Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA

</details>


### [25] [HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare](https://arxiv.org/abs/2512.18829)
*Aditya Siddhant*

Main category: cs.AI

TL;DR: 本文提出了 HARBOR，一种行为健康感知语言模型，用于预测离散的情绪和风险分数（-3至+3），并在新发布的 PEARL 多模态纵向数据集上进行了评估。结果显示，HARBOR 准确率达 69%，显著优于传统模型和专有 LLM。


<details>
  <summary>Details</summary>
Motivation: 行为健康风险评估由于患者数据的高度多模态性和情绪障碍的时间动态性而具有挑战性。尽管大型语言模型（LLM）展现出强大的推理能力，但它们在结构化临床风险评分中的有效性仍不明确。

Method: 本文提出了 HARBOR，一个行为健康感知语言模型，用于预测范围从 -3（严重抑郁）到 +3（躁狂）的离散港湾风险分数（HRS）。同时发布了 PEARL 纵向数据集，该数据集包含三名患者四年间的生理、行为和自我报告的精神健康信号。将 HARBOR 与传统机器学习模型和专有 LLM 进行了基准测试和比较。

Result: HARBOR 模型在性能上超越了传统的基线模型和现成的 LLM。HARBOR 实现了 69% 的准确率，相比之下，逻辑回归的准确率为 54%，而最强的专有 LLM 基线仅为 29%。

Conclusion: HARBOR模型成功证明了行为健康感知语言模型在应对复杂、多模态的临床风险评分任务中的优越性，为未来的行为健康风险评估提供了新的有效工具。

Abstract: Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.

</details>


### [26] [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)
*Zijun Gao,Zhikun Xu,Xiao Ye,Ben Zhou*

Main category: cs.AI

TL;DR: 针对大型语言模型（LLMs）在数学概念理解上的不足，本文提出了CORE（Concept-Oriented REinforcement）框架。CORE将显式概念转化为可控的监督信号，通过概念对齐的测验和概念注入的轨迹强化学习，有效缩小了模型在解决问题能力和真实概念推理之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在解决复杂的数学问题时，往往缺乏真正的概念理解能力。流行的可验证奖励强化学习（RLVR）仅强化最终答案，导致模型倾向于模式复用而非概念应用，造成了概念推理的差距。

Method: 本文提出了CORE（Concept-Oriented REinforcement）框架。该方法首先利用高质量的教材资源，将可验证的习题与简洁的概念描述联系起来。然后，CORE通过三个关键步骤实现概念强化：(i) 合成概念对齐的测验；(ii) 在模型生成过程中注入简短的概念片段，以引导概念启发的轨迹；(iii) 通过群组失败后的轨迹替换、轻量级前向KL约束或直接在概念对齐测验上使用标准GRPO等方式强化概念推理。

Result: CORE框架在多个模型上，相对于普通基线模型和SFT基线模型，在内部的概念-习题套件以及多样化的域外数学基准测试中都取得了持续且显著的性能提升。

Conclusion: CORE框架在结果正则化下，统一了概念对齐测验的直接训练与概念注入的生成轨迹。它提供了一种细粒度的概念监督信号，成功地弥合了问题解决能力和真实概念推理之间的鸿沟，同时保持了与具体算法和验证器无关的特性。

Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.

</details>


### [27] [Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models](https://arxiv.org/abs/2512.18901)
*Gökdeniz Gülmez*

Main category: cs.AI

TL;DR: 提出了一种名为Gabliteration的新型神经权重修改技术，该技术通过自适应多向投影和正则化层选择，能够在修改模型特定行为的同时最大限度地减少对整体模型质量的损害。


<details>
  <summary>Details</summary>
Motivation: 现有（传统）的“消除”（abliteration）方法在尝试修改模型的特定行为模式时，往往会损害模型的整体质量。本文旨在解决这一根本性限制。

Method: 提出了Gabliteration技术，超越了传统的“消除”方法。该方法通过实现自适应多向投影和正则化层选择进行权重修改。具体机制包括：动态层优化、正则化投影矩阵以及自适应缩放机制，旨在最小化对不相关领域质量的降低。

Result: 该方法通过gabliterated-v1模型系列（参数规模从0.6B到4B）进行了验证，并在Hugging Face上提供。结果证明了该技术在多种模型规模上都具有实际应用性。

Conclusion: Gabliteration是一种理论上更优越的神经权重修改技术，它成功地在修改模型特定行为的同时，最大限度地减少了对模型在不相关领域质量的损害。

Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.

</details>


### [28] [Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm](https://arxiv.org/abs/2512.18947)
*Li Yan,Bolun Liu,Chao Li,Jing Liang,Kunjie Yu,Caitong Yue,Xuzhao Chai,Boyang Qu*

Main category: cs.AI

TL;DR: 针对动态多模态多目标优化（DMMO）的挑战，本文提出了一个新的测试基准，并设计了一种基于聚类自编码器预测的动态响应机制（结合自适应小生境策略）的新算法，实验证明其在多样性和收敛性方面均优于现有先进算法。


<details>
  <summary>Details</summary>
Motivation: 动态多模态多目标优化（DMMO）面临双重挑战：在时变环境中需要同时跟踪多个等效的帕累托最优集（POS），并保持种群多样性。现有动态多目标进化算法（DMOEAs）通常忽略解的模态性，而静态多模态多目标进化算法（MMOEAs）缺乏对动态环境变化的适应能力。

Method: 1. 引入了一套新的动态多模态多目标测试函数基准，用于严格评估。 2. 提出了一个新算法，其核心是“基于聚类的自编码器预测动态响应机制”（Clustering-based Autoencoder prediction dynamic response mechanism），该机制利用自编码器处理匹配的簇以生成高度多样化的初始种群。 3. 在静态优化器中集成了自适应小生境（adaptive niching）策略，以平衡算法的收敛性和多样性。

Result: 在12个动态多模态多目标测试函数实例上的经验分析表明，与现有最先进的DMOEAs和MMOEAs相比，所提出的算法不仅在决策空间中能更有效地保持种群多样性，而且在目标空间中也实现了卓越的收敛性。

Conclusion: 本文通过引入新的动态多模态多目标（DMMO）测试基准并提出了基于聚类自编码器预测的动态响应机制，成功解决了DMMO中多样性和收敛性难以平衡的问题，并在决策空间多样性和目标空间收敛性方面均取得了优于现有先进算法的性能。

Abstract: Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.

</details>


### [29] [Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection](https://arxiv.org/abs/2512.18956)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: 本文提出了SynSelect，一个三阶段的“合成-选择”框架，用于为多模态推理任务生成高质量的长CoT数据。通过合成多样性CoT并进行严格筛选，显著提升了多模态大推理模型（LRMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 将长CoT推理的成功扩展到多模态领域面临挑战，主要原因是输入模态集成复杂性增加以及缺乏高质量的长CoT训练数据。现有数据集和生成方法存在推理深度有限、模态转换错误和生成流程僵化等问题，阻碍了模型性能和稳定性。

Method: 提出了SynSelect框架，这是一个新颖的三阶段“合成-选择”框架，用于生成高质量的长CoT数据。具体而言，它首先利用多个异构多模态LRMs生成多样化的候选CoT，然后应用实例级和批次级选择来过滤出高质量的CoT。

Result: 在多个多模态基准测试上，使用SynSelect生成数据进行监督微调（SFT）的模型显著优于基线，并且在经过强化学习（RL）后训练后取得了进一步的性能提升。

Conclusion: SynSelect是一种有效的方法，能够显著提升多模态大推理模型（LRMs）的推理能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.

</details>


### [30] [Recontextualization Mitigates Specification Gaming without Modifying the Specification](https://arxiv.org/abs/2512.19027)
*Ariana Azarbal,Victor Gillioz,Vladimir Ivanov,Bryce Woodworth,Jacob Drori,Nevan Wichers,Aram Ebtekar,Alex Cloud,Alexander Matt Turner*

Main category: cs.AI

TL;DR: 提出“重新语境化”（recontextualization）方法，通过在惩罚不良行为的提示下生成输出，再将其重新标记为允许不良行为提示下的响应，从而训练语言模型抵抗不当行为，减少因训练信号错误导致的“规范博弈”问题。


<details>
  <summary>Details</summary>
Motivation: 开发者在指定正确的训练标签和奖励时常常遇到困难，这导致语言模型容易陷入“规范博弈”（specification gaming），即模型执行了被错误训练信号所强化的不良行为。因此，需要一种方法来减轻这种风险。

Method: 提出“重新语境化”（recontextualization）方法。首先，使用阻止不当行为的提示来生成理想的完成内容。然后，将这些完成内容重新语境化，使其看起来像是对允许不当行为的提示的响应。这种训练旨在让语言模型即使在指令允许的情况下也能抵抗不当行为。

Result: “重新语境化”被证明可以有效地阻止模型学习到多种不当行为，包括：1) 优先考虑评估指标而非聊天响应质量；2) 为通过不正确的测试而对代码进行特殊处理；3) 向用户撒谎；4) 变得谄媚逢迎。

Conclusion: 该方法减轻了由于训练信号规格错误所导致的对不当行为的强化，从而在不改善监督信号本身的情况下减少了“规范博弈”问题。

Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.

</details>


### [31] [Can abstract concepts from LLM improve SLM performance?](https://arxiv.org/abs/2512.19069)
*Siddharth Tandon*

Main category: cs.AI

TL;DR: 提出了一种在推理时将大模型的高级概念（引导向量）迁移到小模型（SLM）的方法，显著提升了SLM在多种任务上的性能，并引入动态缩放进一步优化效果（Qwen3-0.6B提升7-15\%）。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在资源受限设备上部署困难的问题，现有优化方法（如量化、剪枝、蒸馏）需要大量实验和复杂的架构设计。

Method: 利用现有技术从大模型中提取代表高级概念的“引导向量”（steering vectors），并在推理阶段将其迁移到小模型（SLM）上。此外，引入“推理时缩放”（inference-time scaling）来动态调整引导强度，以进一步增强性能。

Result: 概念可以有效地迁移到不同家族的小模型（如Phi, Llama, Qwen）上，并在广泛的任务中实现性能提升。推理时缩放技术使Qwen3-0.6B的准确率提高了7-15\%。

Conclusion: 通过在推理时迁移大模型的引导向量，可以有效提升小模型的性能，是一种简化资源受限设备上LLM部署的有效策略。

Abstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.

</details>


### [32] [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)
*Yanzhi Zhang,Yitong Duan,Zhaoxi Zhang,Jiyan He,Shuxin Zheng*

Main category: cs.AI

TL;DR: 提出了一种受遗传算法启发的、无需训练的LLM推理优化方法Population-Evolve。它通过并行推理维护解的“种群”，利用“进化提示”进行自我迭代优化，并被证明在准确性和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型（LLMs）的推理能力，并探索利用测试时缩放（Test-time scaling）策略进行优化的新方向。

Method: 提出了一种名为Population-Evolve的免训练方法，灵感来源于遗传算法（Genetic Algorithms）。该方法通过并行推理为每个问题维护一个动态的候选解“种群”，并利用特殊的“进化提示”（evolve prompt）让LLM在迭代中实现种群的自我进化。最终结果通过多数投票得出。此外，该研究还建立了一个统一框架，用遗传算法的角度来解释现有的测试时缩放策略。

Result: Population-Evolve方法在实证结果中展示出卓越的准确性，同时具有较低的性能方差和较高的计算效率。

Conclusion: 进化策略（evolutionary strategies）在推理过程中具有释放大型语言模型（LLMs）推理潜力的巨大潜力。

Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.

</details>


### [33] [$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics](https://arxiv.org/abs/2512.19084)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本文提出使用“承诺理论”框架来描述“注意力”机制，并以此为桥梁连接向量化机器学习和知识图谱，避免依赖隐式语言模型。通过使用语义时空图$\gamma(3,4)$和关注因果边界条件，该方法在保持数据意图性的同时，实现了用于上下文确定的数据量级压缩，适用于自主系统。


<details>
  <summary>Details</summary>
Motivation: 建立一种不依赖隐式语言模型的方法，来连接向量化机器学习表示（用于概率估计）与知识图谱表示（用于保留数据源的意图性），从而实现两者优势的共存。

Method: 1. 利用为自主智能体开发的“承诺理论”概念来形式化“注意力”的语义和动态。2. 采用语义时空图$\gamma(3,4)$来替代复杂的本体论，转而关注特征在语义过程中的角色分类。3. 适当关注因果边界条件以实现上下文确定。

Result: 该方法有利于在不确定条件下进行推理，并通过对因果边界条件的关注，可以实现上下文确定所需数据的量级压缩（orders of magnitude compression）。

Conclusion: 将“注意力”和“承诺理论”集成到语义时空图框架中，提供了一种高效且能保留数据意图性的知识表示方法。这种方法能够保持数据源的意图性，并实现数据量级压缩，对于自主机器人、国防部署和紧急服务等需要高效上下文确定的应用至关重要。

Abstract: The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.

</details>


### [34] [Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving](https://arxiv.org/abs/2512.19093)
*Peiqing Lu,Yuan Zhang,Haoyun Zhang,Jiasen Zheng,Kejian Tong,Wenjun Wu*

Main category: cs.AI

TL;DR: HERALD 是一种混合集成推理框架，它结合了多种 LLM（如 GPT-4o）与强化学习和知识蒸馏，有效解决了双语数学问题中语言推理与精确计算的集成挑战，显著提高了多语言数学推理的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 双语数学问题解决需要清晰地连接语言推理和符号计算。大型语言模型（LLMs）通常擅长语言推理，但在精确计算方面表现较弱。

Method: 提出 HERALD（具有自适应学习和蒸馏的混合集成推理）框架，该框架结合了 NuminaMath-7B-TIR、GPT-4o 和 Mistral-7B。它使用自适应路由、基于工具的强化学习（控制工具使用以减少冗余）和知识蒸馏（降低延迟），并通过置信度校准和双路径检查来保持结果的稳定性和准确性。

Result: 该系统证明了结合符号检查、自适应集成和双语微调，有助于实现流畅的推理和精确的计算。

Conclusion: HERALD 为多语言数学推理提供了一个实用的解决方案，通过集成符号检查、自适应集成和双语微调，显著提高了其准确性、稳定性和清晰度。

Abstract: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.

</details>


### [35] [Conditioning Accept-Desirability models in the context of AGM-like belief change](https://arxiv.org/abs/2512.19096)
*Kathelijne Coussement,Gert de Cooman,Keano De Vos*

Main category: cs.AI

TL;DR: 本文在抽象的线性空间决策框架中，为“可接受性-期望性模型”引入了新的条件化规则，统一了经典、量子和不精确概率。该规则与信念修正（AGM公理）相关联，并证明在经典逻辑和完全条件概率中所有AGM公理均成立。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一个足够抽象和通用的决策制定框架（基于可接受性-期望性模型），以统一经典的概率论、量子概率论，并将其扩展到不精确概率的背景下。

Method: 1. 在抽象的决策框架中讨论了可接受性-期望性模型的条件化，该框架将不确定的奖励置于一般线性空间，并将事件视为特殊的投影算子。2. 引入了一种新的条件化规则，其基础是观察事件会引入选项之间新的“无差别性”。3. 将该条件化规则与信念修正算子相关联，并检验标准的AGM信念修正公理在该更一般框架中的适用性。

Result: 所建立的抽象设置成功地统一并扩展了不同类型的概率理论。研究发现，在两个重要的特殊情况下——经典命题逻辑和完全条件概率——所有的AGM信念修正公理都被证明仍然成立。

Conclusion: 本文成功地为可接受性-期望性模型定义了一种新的、通用的条件化规则，在一个抽象框架下统一了经典概率、量子概率和不精确概率。通过研究相应的信念修正算子，证明了在经典命题逻辑和完全条件概率等关键子集中，该规则与完整的AGM信念修正公理是兼容的。

Abstract: We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.

</details>


### [36] [Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis](https://arxiv.org/abs/2512.19135)
*Chenghao Li,Chaoning Zhang,Yi Lu,Shuxu Chen,Xudong Wang,Jiaquan Zhang,Zhicheng Wang,Zhengxun Jin,Kuien Liu,Sung-Ho Bae,Guoqing Wang,Yang Yang,Hen Tao Shen*

Main category: cs.AI

TL;DR: 本文首次使用拓扑数据分析（TDA）和持久同源性（Persistent Homology）从结构角度分析LLM推理链的质量。研究发现，结构复杂度与准确率正相关，但成功的推理路径表现出更简单、冗余度更低的拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的长推理链技术显著增强了其解决复杂问题的能力，但目前尚不清楚为什么不同推理链的性能表现不同，以及哪些结构组成部分是关键因素。现有研究主要从功能角度评估推理链，缺乏对其结构机制的关注。

Method: 首次从结构角度评估推理链质量。应用拓扑数据分析（TDA）中的持久同源性（Persistent Homology）技术，将推理步骤映射到语义空间，提取拓扑特征，分析语义连贯性、逻辑冗余、中断和空白。通过计算同源群、条形码和持久图来量化连接性、冗余性、稳定性和一致性。

Result: 1. 推理链的拓扑结构复杂度与准确率呈正相关。
2. 结构更复杂的推理链能更快地识别出正确答案。
3. 成功的推理过程展现出更简单的拓扑结构（减少了冗余和循环），提高了效率和可解释性。

Conclusion: 本工作为推理链质量评估提供了一个新的结构化视角，并为未来LLM推理链的优化工作提供了指导。

Abstract: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.

</details>


### [37] [Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6](https://arxiv.org/abs/2512.19287)
*Jiaao Wu,Xian Zhang,Fan Yang,Yinpeng Dong*

Main category: cs.AI

TL;DR: 引入“Vibe Reasoning”人机协作范式，通过通用元提示、智能体基础和模型编排，成功解决了自主AI失败的复杂数学问题（IMO 2025 Problem 6），证明轻量级人类指导能有效释放前沿模型的数学推理潜力。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型已拥有解决复杂问题的知识，但缺乏“如何、什么、何时”应用这些知识的能力，导致其潜能无法转化为实际能力（自主解决失败）。本文旨在通过人机协作范式解决AI的自主失败模式，释放其潜在的数学推理能力。

Method: 提出“Vibe Reasoning”这一人机协作范式。方法论基于三个核心组件：通用元提示（Generic meta-prompts）、智能体基础（Agentic grounding）和模型编排（Model orchestration，如结合GPT-5的探索能力和Gemini 3 Pro的证明能力）。通过迭代优化，将人类提示从特定提示演化为可转移的元提示，并利用Python代码执行和基于文件的内存进行智能体工作流。

Result: 成功解决了自主AI系统公开报道失败的IMO 2025 Problem 6（一个组合优化问题）。导出了正确答案（2112）和一个严谨的数学证明。研究分析了AI自主失败的原因，并证明了智能体基础和模型编排的必要性。

Conclusion: 轻量级的人类指导（Vibe Reasoning）可以有效解锁前沿模型在数学推理方面的巨大潜力。本文正在开发自动化框架并进行更广泛的评估以验证该范式的通用性和有效性。

Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.

</details>


### [38] [Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application](https://arxiv.org/abs/2512.19299)
*Haoyu Jiang,Fanjie Zeng,Boan Qu,Xiaojie Lin,Wei Zhong*

Main category: cs.AI

TL;DR: 针对智能能源领域的专业知识壁垒和通用LLM的局限性，本文提出了领域定制化大模型Helios，并构建了一套完整的资源工具（包括知识库、指令集和RLHF数据集），同时发布了评估基准EnerBench，以显著提升模型在智能能源场景下的专业能力和工程一致性。


<details>
  <summary>Details</summary>
Motivation: 在全球碳中和的推动下，深度协调的智能能源系统至关重要。然而，该领域的跨学科性、碎片化和快速演变的专业知识使得通用LLM缺乏领域知识和物理约束意识，无法提供精确的、符合工程要求的推理和生成。

Method: 引入了针对智能能源领域定制的大型语言模型Helios。开发了多智能体协同框架Enersys，用于构建端到端数据集，包括智能能源知识库EnerBase、指令微调数据集EnerInstruct和RLHF数据集EnerReinforce。Helios利用这些资源进行大规模预训练、SFT和RLHF。此外，本文还发布了用于评估智能能源场景LLM的基准测试集EnerBench。

Result: 所提出的方法显著增强了模型在智能能源领域的领域知识掌握度、任务执行准确性，以及与人类偏好和行业标准的对齐程度。

Conclusion: 本文成功构建了专用于智能能源领域的Helios大模型和一整套配套资源，有效克服了通用LLM在处理复杂工程领域问题时的局限性，并为该领域的LLM研究提供了全面的基础。

Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.

</details>


### [39] [SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.19317)
*A. A. Gde Yogi Pramana,Jason Ray,Anthony Jaya,Michael Wijaya*

Main category: cs.AI

TL;DR: 提出 SafeMed-R1，一种用于医疗 VQA 的混合防御框架，结合了 AT-GRPO 训练和随机平滑，显著提高了模型在保持高质量临床推理的同时对对抗性攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉问答（VQA）中的视觉-语言模型（VLMs）容易受到对抗性攻击。标准的对抗训练会损害模型的泛化能力和生成的临床推理质量。因此，需要一种既能保证鲁棒性又能保持高质量、可解释的医疗推理的防御机制。

Method: 提出 SafeMed-R1 混合防御框架，采用两阶段方法：1. 训练阶段：整合对抗训练与群体相对策略优化（AT-GRPO），明确增强推理过程对最坏情况扰动的鲁棒性。2. 推理阶段：使用随机平滑（Randomized Smoothing）来增强模型，提供经过认证的 $L_2$-范数鲁棒性保证。

Result: 1. 在 PGD 攻击下，标准微调 VLM 的准确率从 95% 暴跌至约 25%。2. SafeMed-R1 在相同的对抗条件下仍能保持 84.45% 的准确率，鲁棒性提高了 59 个百分点。3. 明确进行思维链（Chain-of-Thought）推理训练的模型比仅指令训练的模型展现出更强的对抗鲁棒性。

Conclusion: SafeMed-R1 是一种有效的混合防御框架，可显著提高医疗 VLM 的对抗鲁棒性，同时保持可解释的推理质量。研究结果表明，医疗 AI 系统中，可解释性（思维链）与安全性之间存在协同作用。

Abstract: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.

</details>


### [40] [VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop](https://arxiv.org/abs/2512.19349)
*JiaWei Zhu,ZiHeng Liu*

Main category: cs.AI

TL;DR: 提出 VIGOR+ 框架，通过建立 CEVAE 统计验证和 LLM 生成之间的迭代反馈机制，解决了 LLM 生成的混杂因素缺乏统计实用性的问题，以实现对隐藏混杂因素的有效精炼。


<details>
  <summary>Details</summary>
Motivation: 因果推断中隐藏混杂因素是一个基本挑战。虽然最近的工作利用大型语言模型（LLM）生成了貌似合理的隐藏混杂因素，但这些因素通常只具备语义合理性而缺乏统计实用性。现有方法将生成和验证视为独立阶段，缺乏一个有效的闭环机制来优化混杂因素的统计效用。

Method: 提出 VIGOR+（Variational Information Gain for iterative cOnfounder Refinement）框架。该方法通过建立 LLM 生成与基于 CEVAE 的统计验证之间的迭代反馈机制来运行。CEVAE 提供的验证信号（包括信息增益、潜在一致性指标和诊断信息）会被转化为自然语言反馈，用于指导后续的 LLM 生成回合，从而实现混杂因素的持续精炼，直到满足收敛标准。

Result: 研究人员对反馈机制进行了形式化，在温和假设下证明了框架的收敛特性，并提供了一个完整的算法框架。

Conclusion: VIGOR+ 有效地弥合了 LLM 混杂因素生成中的语义合理性与统计实用性之间的鸿沟，为从观察数据中进行因果推断提供了更可靠的隐藏混杂因素精炼方法。

Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.

</details>


### [41] [PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://arxiv.org/abs/2512.19350)
*A. B. M. Ashikur Rahman,Saeed Anwar,Muhammad Usman,Irfan Ahmad,Ajmal Mian*

Main category: cs.AI

TL;DR: 本文提出了首个针对多模态大语言模型（MLLMs）奉承行为（sycophancy）的综合评估基准 	extit{PENDULUM}（包含约2000个VQA对）。研究发现，SOTA MLLMs 对用户输入表现出显著的奉承和幻觉倾向。强调亟需开发抗奉承的架构和训练策略。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是多模态大语言模型（MLLMs）中一个关键且尚未得到充分探索的挑战。鉴于以往的研究主要集中在纯文本设置，而针对视觉或多模态奉承行为的研究在范围和深度上都非常有限，因此需要一个全面的评估基准来填补这一空白。

Method: 引入了一个名为 	extit{PENDULUM} 的综合评估基准，该基准包含约2000个人工策划的视觉问答对，专门用于引出奉承反应。该基准涵盖六个不同复杂度的图像领域。此外，论文提出了新颖的指标来量化视觉推理中的奉承行为。

Result: 通过对最先进的 MLLMs 进行广泛评估，发现模型的鲁棒性存在显著差异，并且表现出对奉承和幻觉行为的明显易感性。研究还提供了关于奉承行为在不同多模态上下文中的表现的深入见解。

Conclusion: 研究结果强调了开发抗奉承架构和训练策略的紧迫性，以增强未来多模态大语言模型（MLLMs）的事实一致性和可靠性。

Abstract: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.

</details>


### [42] [First-Order Representation Languages for Goal-Conditioned RL](https://arxiv.org/abs/2512.19355)
*Simon Ståhlberg,Hector Geffner*

Main category: cs.AI

TL;DR: 本文将一阶关系语言引入目标导向强化学习和广义规划中，通过使用原子集合表示状态和目标，并结合经验回放（HER）技术，自动创建由简到难的子目标课程，从而有效地在大型、稀疏奖励的规划实例上学习通用策略。


<details>
  <summary>Details</summary>
Motivation: 目标导向强化学习和广义规划在面对大型训练实例和稀疏奖励（随机探索难以成功）时，如何高效地学习出具有泛化能力的通用策略是一个挑战。

Method: 结合“事后经验回放”（HER）技术，将状态和目标表示为“原子集合”。作者提出了三种目标表示方法：1. 完整状态作为目标；2. 原始目标的子集作为目标（子目标）；3. 子目标的“提升”（lifted）版本（最抽象）。后两种方法通过自动创建难度递增的子目标课程来实现通用策略学习。

Result: 目标表示的后两种版本（子集和提升版本）成功地在具有稀疏奖励的大型规划实例上学习了通用策略。实验证明这些版本通过自动创建更容易的目标课程，实现了显著的计算增益和更高的学习效率。

Conclusion: 通过将状态和目标表示为原子集合，并结合事后经验回放（HER），尤其是使用子目标或提升子目标表示，能够有效解决大型规划实例中的稀疏奖励问题，并通过自动化的目标课程实现数据和时间效率更高的学习。

Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.

</details>


### [43] [EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration](https://arxiv.org/abs/2512.19396)
*Runze Li,Yuwen Zhai,Bo Xu,LiWu Xu,Nian Shi,Wei Zhang,Ran Lin,Liang Wang*

Main category: cs.AI

TL;DR: 提出EchoTrail-GUI框架，通过构建和检索成功的任务轨迹（记忆），解决了现有GUI智能体的“数字失忆”问题，显著提升了任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体（基于大型视觉-语言模型，VLM）在处理任务时通常是孤立的，缺乏从历史成功经验中系统学习的机制，导致“数字失忆”、性能欠佳、错误重复和对新挑战的泛化能力差。

Method: 提出EchoTrail-GUI框架，模拟人类的经验学习，包含三个阶段：1. 经验探索：智能体自主交互，构建成功的任务轨迹数据库，由奖励模型验证（全自动，无需人工监督）。2. 记忆注入：针对新任务，高效检索最相关的历史轨迹作为“记忆”。3. GUI任务推理：将检索到的记忆作为上下文指导，辅助智能体进行推理和决策。

Result: 在Android World和AndroidLab等基准测试中，EchoTrail-GUI显著提高了基线智能体的任务成功率和操作效率。

Conclusion: 结构化记忆对于构建更健壮、更智能的GUI自动化系统至关重要，EchoTrail-GUI有效地验证了这一点。

Abstract: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.

</details>


### [44] [An Agentic Framework for Autonomous Materials Computation](https://arxiv.org/abs/2512.19458)
*Zeyu Xia,Jinzhe Ma,Congjie Zheng,Shufei Zhang,Yuqiang Li,Hang Su,P. Hu,Changshui Zhang,Xingao Gong,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Mao Su*

Main category: cs.AI

TL;DR: 开发了一个领域专用的智能体，用于第一性原理材料计算的自动化。通过嵌入领域知识，该系统在可靠性、准确性和鲁棒性方面显著优于独立的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在科学发现中很有潜力，但其知识静态和“幻觉”问题阻碍了在自主科学研究中实现可靠的应用。

Method: 提出了一个领域专用的智能体，专为可靠地自动化第一性原理材料计算而设计。该智能体通过嵌入领域专业知识，确保了多步骤工作流的物理一致性，并能始终选择收敛且合理设定的参数，从而实现可靠的端到端计算执行。

Result: 在一个包含多样化计算任务的新基准测试中，该系统在准确性和鲁棒性方面均显著优于单独使用的大型语言模型。

Conclusion: 这项工作为自主计算实验奠定了可验证的基础，是迈向完全自动化科学发现的关键一步。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.

</details>


### [45] [QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526)
*Li Puyin,Tiange Xiang,Ella Mao,Shirley Wei,Xinye Chen,Adnan Masood,Li Fei-fei,Ehsan Adeli*

Main category: cs.AI

TL;DR: 提出了QuantiPhy，首个定量物理推理基准，包含3.3K+视频-文本实例，用于评估VLM对尺寸、速度和加速度的数值估计。研究发现现有VLM在数值正确性上表现不佳，且过于依赖先验知识而非输入信息。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（基于VQA的定性评估）无法有效衡量最先进的视觉语言模型（VLMs）是否能对运动物体的运动学量（如尺寸、速度、加速度）进行定量物理推理，这限制了通用AI对物理世界的理解。

Method: 提出了QuantiPhy基准，这是第一个用于定量测量VLM物理推理能力的测试集。它包含超过3.3K个带有数值真值的视频-文本实例，用于评估模型在给定时间戳下对物体尺寸、速度和加速度的估计，并标准化了提示和评分以评估数值准确性。

Result: 对现有SOTA VLM的实验显示，它们的定性合理性与实际数值正确性之间存在明显的差距。深入分析表明，这些模型在进行定量运动学推理时，严重依赖预训练的世界知识，而不是忠实地使用所提供的视觉和文本输入作为参考。

Conclusion: QuantiPhy提供了一个严格且可扩展的测试平台，旨在推动VLM超越单纯的语言合理性，实现基于数值的、可靠的物理理解。

Abstract: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.

</details>


### [46] [Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios](https://arxiv.org/abs/2512.19551)
*Jiawen Wang,Jingjing Wang Tianyang Chen,Min Zhang,Guodong Zhou*

Main category: cs.AI

TL;DR: 提出了面向LLM的终身情感运动生成（L^2-EMG）新任务，旨在增强模型在不同场景下的持续学习能力。为应对情感解耦和场景适应挑战，设计了ES-MoE（专家混合）方法，并在新数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的以人为中心的情感运动生成方法主要关注于单一固定规模数据集上的性能提升，而忽略了灵活且规模不断增长的运动场景（例如体育、舞蹈），这极大地限制了模型的现实世界泛化能力。为解决此问题，本文提出了L^2-EMG任务。

Method: 为解决L^2-EMG任务中的情感解耦和场景适应挑战，本文提出了“情感可迁移和场景自适应的专家混合模型”（ES-MoE）。该方法设计了一个因果指导的情感解耦模块和一个场景自适应的专家构建模块，分别用于解决这两大挑战。

Result: 本文构建了多个L^2-EMG数据集来验证所提方法的有效性。广泛的评估结果表明，所提出的ES-MoE方法优于先进的基线模型。

Conclusion: 本文提出了一个新的LLM中心终身共情运动生成（L^2-EMG）任务，旨在使大型语言模型能够持续获取情感运动生成知识，从而有助于构建一个具备同理心和智能的闭环、自进化的具身智能体。

Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.

</details>


### [47] [Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations](https://arxiv.org/abs/2512.19557)
*Lawrence Krukrubo,Julius Odede,Olawande Olusegun*

Main category: cs.AI

TL;DR: 本文提出了混合LRR-TED框架，通过结合自动化“安全网”和少量人工定义的“风险陷阱”，解决了XAI的可扩展性-稳定性困境，在保证高预测准确率（94%）的同时，将人工工作量减少了50%。


<details>
  <summary>Details</summary>
Motivation: 当前的“可解释人工智能”（XAI）方法面临“可扩展性-稳定性困境”。事后归因方法（如LIME, SHAP）易于扩展但缺乏稳定性；而监督解释框架（如TED）虽然稳定，但需要大量人工努力来标注每一个训练实例。

Method: 提出混合的LRR-TED框架，旨在利用“发现不对称性”来解决困境。该方法首先应用自动化规则学习器（GLRM）识别广泛的“安全网”（留存模式），然后通过结合帕累托最优集中的少数人工定义的“风险陷阱”（流失触发器）来增强解释矩阵，该现象被称为流失的安娜·卡列尼娜原则。

Result: 将该框架应用于客户流失预测时，仅使用4条人工定义的风险规则，就实现了94.00%的预测准确率。这一配置不仅优于完整的8条规则人工专家基线，同时将人工标注工作量减少了50%。

Conclusion: 本文提出了一种人机协作AI范式的转变：将专家的角色从传统的“规则编写者”转变为更高效的“异常处理者”，从而有效解决了XAI领域的“可扩展性-稳定性困境”。

Abstract: Current approaches to Explainable AI (XAI) face a "Scalability-Stability Dilemma." Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel "Asymmetry of Discovery." When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad "Safety Nets" (retention patterns) but struggle to capture specific "Risk Traps" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of "Rule Writers" to "Exception Handlers."

</details>


### [48] [Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight](https://arxiv.org/abs/2512.19691)
*Junze Ye,Daniel Tawfik,Alex J. Goodell,Nikhil V. Kotha,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.AI

TL;DR: 临床风险评分的基准数据集 MedCalc-Bench 存在标签错误。本文提出了一个结合医生参与和智能体校验的动态修正流程，证明了在修正后的数据上训练（使用 GRPO）能使 Qwen3-8B 模型准确率提高 8.7%，强调了在安全关键领域持续维护基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的临床风险评分计算基准 MedCalc-Bench 是由 LLM 生成的，将其视为静态的黄金标准可能固化模型固有的错误，尤其在作为强化学习（RL）的奖励信号时，风险会被放大。因此，需要建立一个持续评估和修正复杂任务基准的机制，以确保评估的准确性和模型的真正对齐。

Method: 提出一个系统化的“医生参与循环”（physician-in-the-loop）流程来审核和重新标记 MedCalc-Bench。该流程利用先进的智能体（agentic verifiers）进行初步校验，并通过自动化分流机制，将最具争议的实例保留给稀缺的临床医生进行人工审查，从而高效地利用医生资源。

Result: 审计发现原始 MedCalc-Bench 中有相当一部分标签偏离了医学事实，原因是特征提取错误、计算器逻辑不匹配和临床模糊性。在下游 RL 训练中，使用修正后的标签训练 Qwen3-8B 模型（通过 GRPO 优化），模型准确率比使用原始基准训练提高了 8.7% 的绝对值。

Conclusion: 临床风险评分等安全关键领域的标签噪声会对模型评估和下游强化学习训练产生实质性影响。研究结果强调，在安全关键领域，严格的基准维护是实现模型真正对齐的先决条件。

Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 本文提出了双向RAG（Bidirectional RAG）系统，通过经过验证的高质量生成响应回写机制实现安全的语料库扩展，使RAG系统能够从用户交互中学习和进化，在保持安全性的同时显著提升了知识覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG（检索增强生成）系统使用静态语料库，无法从用户交互中进化和学习。现有系统缺乏动态知识积累能力，限制了其在实际部署中的持续改进潜力。因此需要一种既能实现知识积累又能防止幻觉污染的自我改进RAG架构。

Method: 提出双向RAG架构，核心创新是通过验证的回写机制实现安全的语料库扩展。系统采用多阶段验证层，包括：（1）基于NLI的蕴含验证；（2）归因检查；（3）新颖性检测。这些机制共同防止幻觉污染，同时允许高质量生成响应回写到知识库中，实现知识积累。在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上使用三个随机种子进行实验验证。

Result: 在12个实验配置中（4个数据集×3个随机种子），双向RAG实现了40.58%的平均覆盖率，几乎是标准RAG（20.33%）的两倍。与朴素回写方法相比，双向RAG添加的文档数量减少了72%（140个文档 vs 500个文档），显著提高了效率和质量。

Conclusion: 研究证明了在严格验证机制的管理下，自我改进的RAG系统是可行且安全的。双向RAG为构建能够从部署中学习的RAG系统提供了实用路径，在保证安全性的前提下实现了知识库的动态扩展和性能提升，为RAG系统的持续进化开辟了新方向。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一个公平性感知的人工智能框架,用于孟加拉国洪灾后援助分配的优化。该框架采用对抗去偏模型,在保持较高预测准确度的同时,显著减少了对边缘化地区和农村地区的系统性偏见,确保援助资源根据真实需求而非历史分配模式到达最脆弱的人群。


<details>
  <summary>Details</summary>
Motivation: 发展中国家的灾后援助分配常常存在系统性偏见,使脆弱地区处于不利地位,并延续历史不公平现象。孟加拉国作为洪灾高发国家,迫切需要一种公平的援助分配机制,以确保资源能够根据真实需求而非历史分配模式到达最需要帮助的边缘化地区和农村地区。

Method: 研究采用对抗去偏模型(adversarial debiasing model)来预测洪灾脆弱性,并主动消除对边缘化地区和农村地区的偏见。该方法将医疗AI中的公平性感知表示学习技术应用于灾害管理领域,使用梯度反转层(gradient reversal layer)强制模型学习偏见不变的表示。基于2022年孟加拉国洪灾的真实数据(影响720万人,造成4.055亿美元损失),对11个地区的87个乡镇进行了实验验证。

Result: 实验结果表明,该框架在87个乡镇的测试中取得显著成效:统计均等差异减少了41.6%,区域公平性差距降低了43.2%,同时保持了较强的预测准确性(R平方值为0.784,基线模型为0.811)。模型生成的可操作优先级排名确保援助能够根据真实需求到达最脆弱的人群,而非遵循历史分配模式。

Conclusion: 本研究成功展示了算法公平性技术在人道主义救援场景中的有效应用,为决策者提供了实施更公平的灾害恢复策略的工具。该框架在保持预测性能的同时显著改善了援助分配的公平性,证明了AI技术可以帮助打破历史性的系统偏见,使灾后援助真正惠及最需要帮助的脆弱群体。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: 本文提出了逻辑草图提示(LSP)框架,通过引入类型变量、确定性条件评估器和基于规则的验证器,显著提升了大语言模型在需要严格规则遵守的任务中的准确性和可解释性,在药理逻辑合规任务中表现优于传统提示方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在自然语言推理方面表现出色,但在需要严格规则遵守、确定性和可审计性的任务上仍不可靠。现有的提示方法(如零样本提示、思维链提示等)在临床、监管和安全关键决策支持系统中难以满足可追溯性和一致性要求,因此需要一种轻量级框架来改善模型在这类任务中的表现。

Method: 提出逻辑草图提示(Logic Sketch Prompting, LSP)框架,该框架包含三个核心组件:(1)类型变量(typed variables)用于结构化信息表示;(2)确定性条件评估器(deterministic condition evaluators)用于规则判断;(3)基于规则的验证器(rule-based validator)用于生成可追溯和可重复的输出。使用两个药理逻辑合规任务,在三个开源权重模型(Gemma 2、Mistral和Llama 3)上对LSP与零样本提示、思维链提示和简洁提示进行基准测试比较。

Result: 在所有任务和模型上,LSP始终达到最高的准确率(0.83至0.89)和F1分数(0.83至0.89),大幅优于零样本提示(0.24至0.60)、简洁提示(0.16至0.30)和思维链提示(0.56至0.75)。McNemar检验显示LSP在几乎所有比较中都取得统计显著性提升(p < 0.01)。

Conclusion: LSP框架在不牺牲性能的前提下,显著改善了大语言模型的确定性、可解释性和一致性,证明其适用于临床、监管和安全关键决策支持系统。该方法为需要严格规则遵守和可审计性的应用场景提供了有效的解决方案,展示了结构化提示方法在提升LLM可靠性方面的潜力。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [6] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文针对带控制参数的数值规划问题，提出了一种乐观编译方法，将可控的简单数值问题转换为标准简单数值任务，使传统数值启发式方法能够有效应用于具有无限可能动作的规划场景。


<details>
  <summary>Details</summary>
Motivation: 带控制参数的数值规划将动作参数作为自由数值变量，导致状态中可能存在无限数量的可应用动作，使得现有的利用动作结构的数值启发式方法无法直接使用。需要找到一种方法使传统启发式技术能够应用于这类问题。

Method: 识别了这类问题的一个可处理子集——可控的简单数值问题，并提出了一种乐观编译方法。该方法将控制依赖的表达式抽象为有界常数效果和松弛前置条件，从而将带控制参数的问题转换为简单数值任务，使子目标启发式方法能够估计目标距离。

Result: 实验结果表明，该方法能够有效地将传统数值启发式应用于具有无限可能动作的场景，在计算上是可行的，推进了当前技术的边界。

Conclusion: 通过乐观编译方法将可控简单数值问题转换为标准简单数值任务，成功解决了带控制参数的数值规划中启发式方法难以应用的问题，为处理无限动作空间的规划问题提供了有效且可行的解决方案。

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [7] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [8] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: 本文提出GatedBias框架,通过轻量级推理时个性化方法,在不重新训练的情况下将冻结的知识图谱嵌入适配到个体用户上下文,仅需约300个可训练参数即可实现个性化推荐,同时保持全局准确性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱基础模型在链接预测任务中虽然在群体层面表现优异,但无法捕捉个体用户偏好,存在通用关系推理与个性化排序之间的关键脱节问题。现有方法难以在保持全局准确性的同时实现参数高效的个性化适配。

Method: 提出GatedBias框架,采用结构门控适配(structure-gated adaptation)机制:将用户特定的配置文件特征与图派生的二元门控相结合,生成可解释的实体级偏置。该方法在推理时进行个性化适配,保持知识图谱嵌入冻结状态,仅需约300个可训练参数,无需重新训练整个模型。

Result: 在Amazon-Book和Last-FM两个基准数据集上进行评估,在对齐指标上取得统计显著性改进,同时保持群体层面性能。反事实扰动实验验证了因果响应性:当特定偏好信号增强时,受益于这些信号的实体排名提升幅度达到6-30倍。

Conclusion: 研究表明基础模型的个性化适配可以同时实现参数高效和因果可验证,成功弥合了通用知识表示与个体用户需求之间的鸿沟。GatedBias框架为知识图谱基础模型的个性化应用提供了一种轻量级、可解释且有效的解决方案。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [9] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了单子上下文工程(MCE),一种基于函子、应用函子和单子等代数结构的新型AI智能体架构范式,用于解决当前智能体系统在状态管理、错误处理和并发执行方面的脆弱性问题,并扩展到元智能体的动态编排。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型驱动的自主智能体架构通常采用命令式、临时性的设计模式,导致系统脆弱,在状态管理、错误处理和并发控制方面存在严重困难。需要一种具有形式化基础的架构范式来构建更加健壮、高效和可验证的AI智能体系统。

Method: 提出单子上下文工程(MCE)架构范式,利用函子(Functors)、应用函子(Applicative Functors)和单子(Monads)等代数结构为智能体设计提供形式化基础。将智能体工作流视为计算上下文,通过代数抽象的内在属性管理状态传播、短路错误处理和异步执行等横切关注点。使用单子实现健壮的顺序组合,应用函子提供并行执行的原则性结构,单子转换器(Monad Transformers)实现这些能力的系统化组合。进一步扩展框架描述元智能体(Meta-Agents),通过元编程动态创建和管理子智能体工作流。

Result: 展示了如何通过单子实现健壮的顺序组合,应用函子如何为并行执行提供原则性结构,以及单子转换器如何系统化地组合这些能力。这种分层方法使开发者能够从简单、可独立验证的组件构建复杂、弹性和高效的AI智能体。框架成功扩展到元智能体,实现了生成式编排和动态子智能体工作流管理。

Conclusion: 单子上下文工程为AI智能体架构提供了坚实的形式化理论基础,通过代数结构的组合性和可验证性,有效解决了传统命令式智能体架构的脆弱性问题。该范式能够从简单组件构建复杂智能体系统,并支持元智能体的动态编排,为构建更加健壮、可维护和高效的自主智能体系统提供了新的架构方向。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [10] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 本文提出了DarkPatterns-LLM基准数据集和诊断框架,用于细粒度评估大语言模型输出中的操纵性内容,涵盖七大危害类别,包含401个精心标注的样例,并通过评估主流模型揭示了其在检测自主性破坏模式方面的显著弱点。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全基准主要依赖粗粒度的二元标签,无法捕捉构成操纵行为的细微心理和社会机制。随着大语言模型的普及,其可能产生的操纵性或欺骗性行为对用户自主性、信任和福祉构成威胁,因此需要建立更精细、多维度的操纵检测标准。

Method: 构建了DarkPatterns-LLM框架,实施四层分析管道:多粒度检测(MGD)、多尺度意图分析(MSIAN)、威胁协调协议(THP)和深度上下文风险对齐(DCRA)。数据集包含401个精心策划的指令-响应对样例,涵盖七大危害类别(法律/权力、心理、情感、身体、自主性、经济和社会危害),并配有专家标注。

Result: 对GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型的评估显示,模型性能存在显著差异(65.2%-89.7%),所有模型在检测破坏自主性的模式方面表现出一致的弱点。

Conclusion: DarkPatterns-LLM建立了首个标准化、多维度的大语言模型操纵检测基准,为构建更可信的AI系统提供了可操作的诊断工具,填补了现有安全评估在细粒度操纵行为检测方面的空白。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [11] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 本文提出当前大型语言模型虽然基于预测编码原理取得巨大进展,但缺少三个关键组件:动作整合、层次化组合结构和情景记忆。文章主张通过融合这些脑科学启发的组件来构建更安全、可解释、节能且类人的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型虽然在下一个词预测任务上表现出色,但存在幻觉、概念理解肤浅、缺乏主体感和责任感、可解释性不足导致的安全隐患以及能源效率低等问题。这些缺陷源于忽略了神经科学预测编码模型中的三个重要组件:动作与生成模型的紧密整合、层次化组合结构和情景记忆。需要从脑科学中汲取灵感来改进AI模型。

Method: 提出将三个脑科学启发的组件整合到基础模型中:(1)在多个抽象尺度上将动作与生成模型紧密整合;(2)引入层次化组合架构;(3)加入情景记忆机制。文章综述了神经科学和认知科学中关于这些组件重要性的最新证据,并将该提案与当前趋势(如思维链推理CoT和检索增强生成RAG)进行比较,探讨用脑启发组件增强模型的新方法。

Result: 文章论证了这些组件如何帮助解决基础模型的当前缺陷:通过动作整合实现概念接地以减少幻觉和提升理解深度;通过控制能力建立主体感和责任感;通过层次化组合结构提高可解释性从而增强安全性和可信度;通过这些机制提升能源效率。展示了脑科学与AI之间思想交流的潜在价值。

Conclusion: 重新点燃脑科学与人工智能之间历史上富有成效的思想交流,将有助于铺平通往安全、可解释、以人为中心的AI之路。通过整合动作、层次化组合结构和情景记忆这三个关键组件,可以构建更接近人类认知、更安全可靠的下一代基础模型。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [12] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [13] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: 本文提出了Tyee,一个用于智能生理健康监测的统一、模块化、完全集成的可配置工具包,通过统一数据接口、模块化架构和端到端工作流配置,解决了深度学习在生理信号分析中面临的数据格式异构、预处理不一致、模型管道碎片化和实验不可复现等问题,在13个数据集中的12个上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析领域虽然前景广阔,但其发展受到多方面限制:数据格式异构、预处理策略不一致、模型管道碎片化以及实验设置不可复现。这些问题严重阻碍了该领域的研究进展和实际应用,因此需要一个统一、标准化且可复现的解决方案来推动智能生理健康监测技术的发展。

Method: Tyee工具包提出了三个关键创新:(1)为12种信号模态设计了统一的数据接口和可配置的预处理管道,解决数据异构问题;(2)采用模块化和可扩展的架构设计,支持跨任务的灵活集成和快速原型开发;(3)实现端到端的工作流配置机制,促进实验的可复现性和可扩展性。该工具包提供了完全集成的配置化解决方案,覆盖从数据处理到模型训练的全流程。

Result: Tyee在所有评估任务中均表现出色,性能超越或匹配基线方法,在13个数据集中的12个上达到了最先进(state-of-the-art)的结果。实验证明了Tyee具有一致的实用有效性和良好的泛化能力,能够在不同的生理信号分析任务中稳定发挥作用。

Conclusion: Tyee成功解决了深度学习在生理信号分析领域面临的关键挑战,通过统一的接口、模块化架构和可配置的工作流,为智能生理健康监测提供了一个高效、可复现且易于扩展的解决方案。该工具包已开源并持续维护,为研究人员和开发者提供了标准化的实验平台,有望推动生理信号分析领域的进一步发展。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [14] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: 本文提出了一种多模态移动性预测方法M³ob,通过构建统一的时空关系图和跨模态对齐机制,有效融合多模态数据以提升人类移动性预测的泛化能力,在位置推荐任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有人类移动性预测方法泛化能力有限:单模态方法受数据稀疏性和固有偏差限制,多模态方法则难以有效捕捉移动动态,因为静态多模态表示与时空动态之间存在语义鸿沟。因此需要一种能够有效利用多模态时空知识来刻画移动动态的方法。

Method: 提出M³ob方法,包含两个核心设计:(1)利用大语言模型增强的时空知识图谱(STKG)捕获功能语义和时空知识,构建统一的时空关系图(STRG)进行多模态表示;(2)设计门控机制融合不同模态的时空图表示,并提出STKG引导的跨模态对齐方法,将时空动态知识注入静态图像模态。

Result: 在六个公共数据集上的大量实验表明,该方法不仅在正常场景下实现了一致的性能提升,而且在异常场景下也展现出显著的泛化能力。

Conclusion: 通过多模态时空知识融合和跨模态对齐,M³ob方法有效解决了现有移动性预测方法的泛化能力不足问题,在位置推荐任务中取得了优异表现,证明了利用LLM增强的时空知识图谱和统一时空关系图建模的有效性。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [15] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 本研究探讨了让大语言模型(LLMs)相互审阅预测结果是否能提高预测准确性。使用202个已解决的二元问题,研究发现在多样化模型共享信息的场景下,这种"结构化讨论"干预显著降低了约4%的对数损失(p=0.017),但在同质模型组中未观察到改进效果。


<details>
  <summary>Details</summary>
Motivation: 结构化讨论已被证明能提高人类预测者的表现,研究者希望验证类似的干预措施(即让LLMs相互审阅彼此的预测后再更新)是否也能提高大语言模型的预测准确性。研究旨在探索讨论机制是否可作为改进LLM预测的可行策略。

Method: 使用来自Metaculus Q2 2025 AI预测锦标赛的202个已解决的二元问题,在四种场景下评估准确性:(1)多样化模型+分布式信息,(2)多样化模型+共享信息,(3)同质模型+分布式信息,(4)同质模型+共享信息。测试模型包括GPT-5、Claude Sonnet 4.5和Gemini Pro 2.5,通过让模型相互审阅预测并更新来实现"结构化讨论"干预。

Result: 在场景(2)(多样化模型+共享信息)中,干预显著提高了准确性,对数损失降低了0.020,相对改进约4%(p=0.017)。然而,在同质模型组(三个相同模型实例)中未观察到任何改进。意外的是,提供额外的上下文信息并未提高预测准确性,这限制了研究信息汇集机制的能力。

Conclusion: 研究结果表明,结构化讨论可能是改进LLM预测的一种可行策略,但其有效性取决于模型的多样性。多样化模型之间的相互审阅能够带来显著的准确性提升,而同质模型之间的讨论则无效。这提示模型多样性在协作预测中的重要性,为未来LLM预测系统的设计提供了指导方向。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [16] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: 本文提出了DICE(离散可解释比较评估)框架,这是一个两阶段、证据耦合的RAG系统评估方法,通过深度分析推理和概率评分实现可解释性和鲁棒性,并采用瑞士制锦标赛将计算复杂度从O(N²)降至O(N log N),在中文金融问答数据集上与人类专家达到85.7%的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估指标存在可解释性有限、不确定性量化不足以及多系统比较时计算效率低下等问题,这些缺陷阻碍了RAG技术的负责任部署。随着RAG系统架构日益复杂,迫切需要一种可解释且鲁棒的评估方法来确保其可信度。

Method: DICE采用两阶段、证据耦合的框架:(1)结合深度分析推理与概率{A, B, Tie}评分机制,生成透明且具有置信度感知的判断;(2)通过可解释的推理轨迹支持系统改进,实现系统性错误诊断和可操作的洞察;(3)采用瑞士制锦标赛算法解决大规模评估的效率挑战,将计算复杂度从O(N²)降低到O(N log N),同时保持排名准确性。

Result: 在精心策划的中文金融问答数据集上进行验证,DICE与人类专家的一致性达到85.7%,显著优于现有的基于LLM的评估指标(如RAGAS)。在八系统评估中,瑞士制锦标赛实现了42.9%的计算量减少,同时保持了排名的准确性。

Conclusion: DICE为可信赖的RAG系统评估建立了一个负责任、可解释且高效的新范式。该框架通过提供透明的推理过程、置信度量化和高效的多系统比较能力,有效解决了现有评估方法的局限性,为RAG技术的负责任部署提供了可靠的评估工具。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [17] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架，通过将情景记忆与强化学习相结合，使大语言模型智能体能够在无需反向传播或模型微调的情况下实现持续学习和经验学习，核心机制是反思过程。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型在训练和部署之间存在严格分离，无法在部署后通过交互进行适应性学习。为了使智能体能够在不更新参数的情况下实现持续适应，需要一个将情景记忆与强化学习整合的理论框架，通过反思机制实现经验积累和策略改进。

Method: 提出了状态反思决策过程（Stateful Reflective Decision Process）框架，将反思学习建模为与情景记忆的两阶段读写交互：写入阶段存储交互结果对应策略评估，读取阶段检索相关历史案例对应策略改进。该过程在增强的状态-记忆表示上诱导出等价的马尔可夫决策过程，允许使用动态规划和强化学习的经典工具。使用熵正则化策略迭代来实例化该框架。

Result: 建立了收敛性保证：随着情景记忆的增长并实现对状态空间的充分覆盖，生成的策略会收敛到最优解。该框架为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在无需参数更新的情况下实现持续适应。

Conclusion: 本研究为大语言模型智能体的持续学习提供了原则性的理论基础，通过反思机制和情景记忆的整合，打破了训练与部署的传统界限。该框架使智能体能够通过交互经验不断改进策略，无需模型微调或参数更新，为构建能够持续适应的记忆增强型语言模型智能体提供了可行路径。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [18] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 本文提出了一种基于分层深度强化学习的投资组合管理框架SAMP-HDRL,通过动态资产分组、上下层智能体协调和效用资本配置机制,在非平稳市场中实现了优于传统方法和深度强化学习基准的表现,并通过SHAP方法提升了决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 非平稳市场中的投资组合优化面临三大挑战:市场状态转换、动态相关性变化,以及深度强化学习策略的可解释性不足。现有方法难以在波动和震荡市场条件下保持稳健性能,且缺乏透明的决策机制。

Method: 提出SAMP-HDRL框架,包含三个核心组件:(1)动态资产分组,将市场划分为高质量和普通资产子集;(2)分层智能体架构,上层智能体提取全局市场信号,下层智能体在掩码约束下执行组内资产配置;(3)基于效用的资本配置机制,整合风险资产和无风险资产,确保全局与局部决策的协调一致。

Result: 在2019-2021年三个市场状态下的回测表明,SAMP-HDRL在波动和震荡条件下持续优于9种传统基准和9种深度强化学习基准。与最强基准相比,该方法至少实现了5%更高的收益率、5%更高的夏普比率、5%更高的索提诺比率和2%更高的欧米伽比率,在动荡市场中优势更为显著。消融实验证实上下层协调、动态聚类和资本配置对鲁棒性不可或缺。

Conclusion: SAMP-HDRL通过将结构化市场约束直接嵌入深度强化学习流程,在复杂金融环境中提供了更好的适应性、鲁棒性和可解释性。基于SHAP的可解释性分析揭示了智能体间"分散+集中"的互补机制,为决策提供了透明洞察,为非平稳市场的投资组合管理提供了有效解决方案。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [19] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: 本文介绍了HiSciBench，一个分层次的科学智能评估基准，包含8,735个实例，覆盖六大学科，通过五个层级（从科学素养到科学发现）全面评估大语言模型和多模态基础模型在科学研究中的能力，揭示了现有模型在基础任务和高级发现任务之间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的科学智能评估基准存在碎片化问题，大多聚焦于狭窄任务，无法反映真实科学研究的层次性和多学科性质。随着大语言模型和多模态基础模型的快速发展，需要一个综合性的、依赖感知的框架来系统评估模型在科学研究全流程中的能力，从基础知识理解到创造性发现。

Method: 构建了HiSciBench分层基准，包含五个层级：L1-科学素养、L2-文献解析、L3-基于文献的问答、L4-文献综述生成、L5-科学发现。基准包含8,735个精心策划的实例，覆盖数学、物理、化学、生物、地理和天文六大学科，支持文本、公式、图表等多模态输入以及跨语言评估。采用集成的、依赖感知的框架对模型在科学推理不同阶段的能力进行详细诊断。

Result: 对包括GPT-5、DeepSeek-R1等领先模型进行了全面评估，结果显示模型性能存在显著差距：在基础素养任务上准确率可达69%,但在发现级别的挑战中性能急剧下降至25%。评估揭示了现有模型在科学研究不同层级任务中的能力分布和局限性。

Conclusion: HiSciBench为评估科学智能建立了新标准,提供了一个全面的、层次化的评估框架,能够系统诊断基础模型在科学研究全流程中的能力。评估结果为开发更强大、更可靠的科学AI模型提供了可操作的见解。该基准将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [20] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: 本文提出Gamma模型,通过引入多头几何注意力机制来增强知识图谱推理的基础模型,使用多种并行的关系变换(实数、复数、分裂复数和对偶数)替代单一变换,并通过关系条件注意力融合机制自适应地选择最合适的关系偏置,在56个知识图谱上的零样本归纳链接预测任务中显著优于现有方法Ultra。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化知识图谱基础模型(如Ultra)依赖单一的关系变换(如逐元素乘法)进行消息传递,这限制了模型的表达能力,无法捕捉不同图谱中展现的多样化关系和结构模式。为了解决这一局限性,需要设计能够建模不同关系结构的多样化几何表示方法。

Method: 提出Gamma模型,核心创新包括:(1)引入多头几何注意力机制,使用多个并行的关系变换替代单一变换,包括基于实数、复数、分裂复数和对偶数的变换,每种变换设计用于建模不同的关系结构;(2)设计关系条件注意力融合机制,通过带有熵正则化的轻量级门控在链接级别自适应融合这些变换,使模型能够为每个三元组模式选择最合适的关系偏置;(3)对这些代数消息函数进行完整形式化,并讨论它们的组合如何提升表达能力超越任何单一空间。

Result: 在56个不同的知识图谱上进行的综合实验表明,Gamma在零样本归纳链接预测任务中持续优于Ultra模型,在归纳基准测试中平均倒数排名(MRR)提升5.5%,在所有基准测试中平均提升4.4%,证明了互补几何表示带来的优势。

Conclusion: Gamma模型通过引入多头几何注意力和多样化的代数变换空间,成功解决了现有知识图谱基础模型表达能力受限的问题。实验结果验证了使用多种互补几何表示和自适应融合机制能够显著提升模型在未见实体和关系上的泛化推理能力,为结构化知识图谱基础模型的发展提供了新的方向。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [21] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 本文针对多模态虚假信息检测的挑战,提出了RW-Post数据集和AgentFact框架。RW-Post是一个高质量的真实世界多模态事实核查数据集,包含完整的推理过程和可验证证据;AgentFact是基于智能体的多模态事实核查框架,通过五个专门智能体协作完成验证任务,显著提升了事实核查的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态虚假信息检测方法(包括大型视觉语言模型和深度多模态融合方法)存在推理能力有限和证据利用不足的问题。关键瓶颈在于缺乏专门的数据集,这些数据集需要提供完整的真实世界多模态虚假信息实例,并配有标注的推理过程和可验证的证据。现有方法难以有效应对快速传播的多模态虚假信息挑战。

Method: 提出两个核心贡献:(1)RW-Post数据集:将真实世界的多模态声明与原始社交媒体帖子对齐,保留丰富的上下文信息,通过大语言模型辅助的提取管道从人工撰写的事实核查文章中获取详细推理和明确关联的证据。(2)AgentFact框架:基于智能体的多模态事实核查系统,包含五个专门智能体(策略规划、高质量证据检索、视觉分析、推理和解释生成),通过迭代工作流程在证据搜索和任务感知的证据过滤推理之间交替,模拟人类验证工作流程。

Result: 大量实验结果表明,RW-Post数据集与AgentFact框架的协同作用显著提升了多模态事实核查的准确性和可解释性。该方法通过专门智能体的协作和迭代式证据分析工作流程,实现了战略决策和系统化证据分析,有效改善了多模态虚假信息检测的性能。

Conclusion: 本文通过构建高质量的RW-Post数据集和设计AgentFact智能体框架,有效解决了多模态事实核查中推理能力不足和证据利用浅层的问题。RW-Post提供了包含完整推理过程和可验证证据的真实世界数据,AgentFact通过模拟人类验证流程实现了多智能体协作,两者结合显著提升了自动化事实核查系统的准确性和可解释性,为应对多模态虚假信息传播提供了有效解决方案。

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [22] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 本研究对比了深度知识追踪(DKT)模型与大语言模型(LLM)在K-12教育场景中评估学习者知识状态的能力,发现DKT在预测准确性、时间一致性和计算效率方面均显著优于LLM(包括零样本和微调版本),表明单纯依赖LLM无法替代传统学习者建模,需要混合框架来实现负责任的自适应教学。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能导师系统在K-12教育中快速兴起,出现了一种误解,认为生成式模型可以替代传统的学习者建模来实现自适应教学。由于K-12教育被欧盟AI法案归类为高风险领域,需要负责任的设计,因此有必要系统性地评估LLM在准确评估学习者随时间演变的知识状态方面的能力、可靠性和时间一致性。

Method: 使用大规模开放数据集,对比评估深度知识追踪(DKT)模型与广泛使用的大语言模型(包括零样本和微调版本)在学习者知识状态评估上的表现。评估指标包括:下一步正确性预测的区分性能(AUC)、早期序列错误率、时间稳定性、掌握度更新的方向正确性,以及多技能掌握度估计的一致性。同时对比了不同方法的计算成本。

Result: DKT模型在下一步正确性预测上达到最高的AUC值(0.83),在所有设置中均优于LLM。虽然微调使LLM的AUC比零样本基线提高约8%,但仍比DKT低6%,且在早期序列中产生更高的错误率。时间分析显示DKT保持稳定且方向正确的掌握度更新,而LLM变体表现出显著的时间弱点,包括不一致和错误方向的更新。尽管微调的LLM需要近198小时的高计算量训练,远超DKT的计算需求,但这些局限性依然存在。定性分析进一步表明,即使经过微调,LLM在多技能掌握度估计中仍产生不一致的轨迹,而DKT保持平滑连贯的更新。

Conclusion: 研究结果表明,单独使用大语言模型不太可能达到成熟智能导师系统的有效性。在K-12等高风险教育场景中实现负责任的自适应教学,需要结合学习者建模的混合框架,而不能仅依赖生成式LLM。传统的知识追踪方法在准确性、时间一致性和计算效率方面仍具有显著优势。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [23] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: 本文介绍了ChexReason，一个通过R1风格方法训练的医学影像视觉-语言模型，揭示了强化学习在医学影像领域的泛化悖论：GRPO优化虽能提升分布内性能，但会损害跨数据集迁移能力，这一问题源于RL范式本身而非模型规模。研究表明，对于需要跨人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大语言模型推理任务上取得进展，但其在资源受限的医学影像应用中仍未被充分探索。医学影像模型需要在不同医疗机构和人群中保持良好的泛化能力，因此需要研究RL方法在该领域的适用性和局限性。

Method: 采用R1风格的训练方法，包括监督微调(SFT)后接GRPO强化学习优化。使用极少量数据(2000个SFT样本和1000个RL样本)和单个A100 GPU训练ChexReason视觉-语言模型。在CheXpert和NIH两个基准数据集上进行评估，并与高资源模型(如NV-Reason-CXR-3B)进行跨模型比较，分析结构化推理支架对不同类型VLM的影响。

Result: GRPO在分布内数据集CheXpert上实现23%的性能提升(macro-F1达到0.346)，但在跨数据集NIH上性能下降19%。这种现象在高资源模型中同样存在，表明问题源于RL范式而非规模。发现泛化悖论：SFT检查点在优化前能在NIH上独特地提升性能，说明教师引导的推理捕获了更多机构无关特征。结构化推理支架对通用VLM有益，但对医学预训练模型增益有限。

Conclusion: 研究揭示了强化学习在医学影像领域存在根本性张力：虽能提升分布内性能，但会损害跨数据集迁移能力，且该问题源于RL范式本身而非模型规模。对于需要跨不同人群保持鲁棒性的临床部署场景，精心策划的监督微调可能比激进的强化学习方法更有效。这为医学AI的训练策略提供了重要指导，强调了泛化能力在临床应用中的关键性。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [24] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出了内在自反思偏好优化(Intrinsic Self-reflective Preference Optimization)方法,通过让模型在生成响应时同时考虑上下文和替代响应,解决了DPO及其变体中存在的最优策略依赖任意建模选择和未充分利用成对数据比较信息的问题,在保证对标量化和参考策略选择不变性的同时,实现了优于DPO/RLHF的全局最优策略。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化(DPO)方法存在两个根本性局限:1)最优策略依赖于任意的建模选择(如标量化函数、参考策略),导致模型行为反映的是参数化的人工痕迹而非真实偏好;2)孤立地处理响应生成,未能利用成对数据中的比较信息,使模型的内在自反思能力未被充分挖掘。

Method: 提出内在自反思偏好优化方法,推导出一个全局最优策略,该策略同时以上下文和替代响应为条件进行建模。该方法在理论上证明了优于DPO/RLHF,同时保证对标量化函数和参考策略选择的不变性。该方法可作为即插即用的增强模块,无需架构改变或增加推理开销。

Result: 实验表明该方法在胜率和长度控制指标上均取得了一致性的改进,验证了解锁自反思能力能够产生更鲁棒、更符合人类偏好的大语言模型。

Conclusion: 通过引入内在自反思机制,使模型在生成时同时考虑上下文和替代响应,成功解决了DPO方法的固有局限性,实现了更优的策略学习效果,且该方法具有理论保证的不变性和实用的即插即用特性,为大语言模型对齐提供了更有效的解决方案。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [25] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文探讨了当前人工智能系统情绪智能(EI)评估框架的不足,指出这些框架未能全面衡量AI相关的EI各个方面。文章回顾了情绪和EI理论,批判性评估了现有基准框架,并提出了改进AI系统EI评估策略的建议。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能情绪智能评估框架存在缺陷,未能充分或全面地衡量AI系统中相关的情绪智能各个方面。人类情绪智能涉及现象学成分和理解感,而AI系统缺乏这些特性,因此某些EI方面在评估AI系统时并不相关。同时,现有基准框架缺乏关于情绪本质和情绪智能的坚实理论基础。

Method: 本研究采用三步方法:(1)回顾不同的情绪理论和一般情绪智能理论,评估每种理论对人工系统的适用程度;(2)批判性评估现有的基准评估框架,根据第一部分建立的EI理论识别各框架的不足之处;(3)概述改进评估策略的若干选项,以避免AI系统EI评估中的这些缺陷。

Result: 研究发现,AI系统虽然缺乏人类情绪智能的现象学成分,但能够在不同程度上感知情绪状态、解释情绪、做出适当响应并适应新情境(如多文化环境)。现有的专门评估AI模型执行EI相关任务能力的基准框架,在情绪本质和情绪智能的理论基础方面存在明显不足。

Conclusion: 本文提出需要改进AI系统的情绪智能评估策略,建议基于对情绪和EI理论的深入理解来重新设计评估框架。评估应区分人类EI中不适用于AI的现象学成分,同时关注AI系统在情绪感知、解释、响应和情境适应等方面的实际能力,从而建立更加科学和全面的AI情绪智能评估体系。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [26] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出"模型信念"(model belief)概念,通过利用大语言模型的token级概率分布来更高效地提取LLM生成数据中的信息。相比传统的"模型选择"方法,模型信念具有更低的方差和更快的收敛速度,在需求估计研究中将计算需求减少约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时,常见做法是将LLM的输出作为单一数据点,这种方法效率低下,未能充分利用LLM概率性质中固有的信息。需要一种更有效的方法来从LLM生成的数据中提取更多信息。

Method: 引入并形式化"模型信念"概念,这是一种从LLM的token级概率中提取的度量,能够在单次生成运行中捕获模型在选择备选项上的信念分布。作者证明了模型信念在渐近意义上等价于模型选择的均值,但形成了更具统计效率的估计器,具有更低的方差和更快的收敛率。

Result: 通过需求估计研究验证了模型信念的性能,其中LLM模拟消费者对不同价格的响应。在实际有限运行次数的设置中,模型信念比模型选择本身更好地解释和预测真实的模型选择,并将达到足够准确估计所需的计算量减少了约20倍。

Conclusion: 研究结果支持将模型信念作为从LLM生成数据中提取更多信息的默认度量方法。模型信念不仅在理论上具有渐近等价性和更优的统计效率,在实际应用中也显著提高了计算效率和预测准确性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [27] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: 本文提出TCEval,一个基于热舒适场景的LLM评估框架,通过评估跨模态推理、因果关联和自适应决策三大认知能力,发现当前LLM具备基础跨模态推理能力但缺乏对热舒适变量间非线性关系的精确因果理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM任务特定基准存在关键缺口,缺乏对AI系统真实世界认知能力的评估。热舒适涉及环境因素与个人感知的复杂交互,包含感官整合和自适应决策,是评估AI认知能力的理想范式。需要将AI评估从抽象任务能力转向具身化、情境感知的感知和决策能力评估。

Method: 提出TCEval评估框架,通过热舒适场景评估LLM的三大核心认知能力(跨模态推理、因果关联、自适应决策)。方法包括:为LLM智能体初始化虚拟人格属性,引导其生成服装隔热选择和热舒适反馈,并使用ASHRAE全球数据库和中国热舒适数据库验证输出。对四个LLM进行实验测试并进行统计分析。

Result: 实验显示智能体反馈与人类的精确对齐有限,但在1 PMV容差下方向一致性显著提高。统计检验表明LLM生成的PMV分布与人类数据显著偏离,智能体在离散热舒适分类任务中表现接近随机水平。结果证实当前LLM具备基础跨模态推理能力,但缺乏对热舒适变量间非线性关系的精确因果理解。

Conclusion: TCEval作为生态有效的认知图灵测试具有可行性,能够评估AI的真实认知能力。该框架补充了传统基准测试,将AI评估重点从抽象任务熟练度转向具身化、情境感知的感知和决策能力,为推进智能建筑等以人为中心的AI应用提供了有价值的见解。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [28] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文研究了一致性规划(Conformant Planning)与超性质模型检验(Hyperproperty Model-Checking)之间的联系,证明了∃*∀*超性质模型检验问题可以高效归约为一致性规划问题,并且反向证明了每个一致性规划问题本身也是一个超性质模型检验任务,建立了两个问题之间的双向等价关系。


<details>
  <summary>Details</summary>
Motivation: 规划与验证社区中存在两个重要问题:一致性规划和超性质模型检验。一致性规划需要找到一个顺序计划,使其在执行过程中不受非确定性动作影响而达成目标;超性质则关联系统的多条执行轨迹,可以捕获信息流和公平性策略。研究这两个问题之间的联系有助于理解它们的本质关系,并可能为解决这两类问题提供新的思路和方法。

Method: 本文采用双向归约的方法建立两个问题之间的联系。首先,将∃*∀*超性质模型检验实例高效归约为一致性规划实例,并证明该编码的正确性和完备性。其次,建立反向关系,证明每个一致性规划问题本身就是一个超性质模型检验任务。通过这种双向归约,揭示了两个问题在计算复杂性和问题本质上的等价性。

Result: 研究成功建立了∃*∀*超性质模型检验与一致性规划之间的双向等价关系。证明了超性质模型检验实例可以高效归约为一致性规划实例,该归约是正确且完备的。同时证明了反向关系:每个一致性规划问题都可以被视为一个超性质模型检验任务。这一结果揭示了两个看似不同领域问题之间的深层联系。

Conclusion: 本文建立了一致性规划与∃*∀*超性质模型检验之间的理论等价关系,证明了这两个问题在本质上是相互关联的。这一发现不仅深化了对这两类问题的理论认识,也为跨领域应用规划技术解决模型检验问题(或反之)提供了理论基础,有望促进规划和验证社区之间的技术交流与方法迁移。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [29] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出CubeBench基准测试,通过魔方任务评估大语言模型智能体在物理世界部署中的空间认知能力,揭示了当前LLM在长期规划方面的根本性缺陷(长期任务通过率为0%),并提供了诊断框架以指导开发更具物理基础的智能体。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体虽然在数字领域表现出色,但在物理世界部署时面临重大挑战,主要是难以形成和维持稳健的空间心智模型。研究者识别出三个核心认知挑战:空间推理、通过心智模拟进行长期状态跟踪、以及在部分观察条件下的主动探索。为了系统性地评估和隔离这些能力,需要一个专门的基准测试。

Method: 提出CubeBench基准测试,以魔方为中心设计生成式评估任务。采用三层诊断框架,逐步评估智能体能力:从具有完整符号信息的基础状态跟踪,到仅有部分视觉数据的主动探索。此外,还提出了诊断框架,通过提供外部求解器工具来隔离认知瓶颈,并对失败模式进行分析。

Result: 对主流大语言模型的实验揭示了关键局限性:所有长期任务的通过率均为0.00%,暴露了LLM在长期规划方面的根本性失败。通过失败模式分析,识别出具体的认知瓶颈,为理解当前LLM在物理空间推理方面的不足提供了实证依据。

Conclusion: 当前大语言模型智能体在物理世界的空间认知和长期规划能力存在严重不足,特别是在长期任务中完全失败。通过CubeBench基准测试和诊断框架,本研究系统性地揭示了这些认知瓶颈,为开发更具物理基础、能够在真实世界中有效运作的智能体提供了关键洞察和指导方向。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [30] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: 本文介绍了MindWatcher，一个集成交错思维和多模态思维链推理的工具集成推理(TIR)智能体，能够自主决策工具调用并协调使用，无需人工提示或工作流。通过高质量数据集训练、专用评估基准和高效训练基础设施，MindWatcher在性能上匹敌或超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的智能体在处理需要工具调用的真实世界问题时表现出有限的智能性。现有方法依赖人工提示或预定义工作流，缺乏自主推理和灵活工具调用能力。需要开发能够自主进行多步推理、动态决策工具使用，并处理多模态信息（特别是图像）的智能体系统。

Method: 提出MindWatcher智能体，核心方法包括：(1)交错思维范式，使模型能在任意中间阶段在思考和工具调用之间切换；(2)多模态思维链(CoT)推理能力，支持推理过程中的图像操作以获得更精确的搜索结果；(3)自动化数据审核和评估管道，结合人工策划的高质量训练数据集；(4)构建MWE-Bench评估基准；(5)配备全面的辅助推理工具套件；(6)建立涵盖汽车、动物、植物等八个类别的大规模高质量本地图像检索数据库；(7)设计更高效的训练基础设施以提升训练速度和硬件利用率。

Result: 实验表明MindWatcher通过卓越的工具调用能力，在性能上匹敌或超越更大规模或更新的模型。尽管模型规模较小，但凭借强大的本地图像检索数据库实现了鲁棒的物体识别能力。研究还揭示了智能体训练的关键洞察，如智能体强化学习中的遗传继承现象。

Conclusion: MindWatcher成功展示了集成交错思维和多模态推理的工具集成推理智能体的有效性。通过自主工具调用和协调能力，该系统能够处理广泛领域的多模态复杂决策任务。研究证明了即使是较小规模的模型，通过优秀的工具集成设计、高质量数据和高效训练基础设施，也能达到与大型模型相当甚至更优的性能，为未来智能体系统的发展提供了重要参考。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [31] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: 本文提出了AKG kernel agent(AI驱动的内核生成器),这是一个多智能体系统,能够自动化生成、迁移和性能调优AI计算内核,支持多种领域特定语言(DSL),在KernelBench评估中相比PyTorch Eager基线实现平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型(如大语言模型、多模态架构、推荐系统)对高性能计算内核需求日益增长,结合稀疏性和量化等技术带来巨大计算挑战。频繁的硬件更新和多样化的芯片架构要求为每个平台定制内核实现,而人工优化无法跟上这些需求,成为AI系统开发的关键瓶颈。大语言模型代码生成能力的进步为自动化内核开发提供了新可能。

Method: 提出AKG kernel agent多智能体系统,用于自动化内核生成、迁移和性能调优。系统支持多种领域特定语言(DSL),包括Triton、TileLang、CPP和CUDA-C,能够针对不同硬件后端保持正确性和可移植性。采用模块化设计,允许快速集成新的DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL对GPU和NPU后端进行评估,AKG kernel agent相比PyTorch Eager基线实现平均实现了1.46倍的加速,证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过多智能体系统和多DSL支持,有效解决了AI计算内核开发中的自动化难题,在保证正确性和可移植性的同时实现了显著的性能提升,为应对快速演进的AI硬件和模型架构提供了可扩展的解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [32] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [33] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [34] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [35] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [36] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文提出了I-PERI算法,用于在联邦学习场景下处理客户端级别未知干预的因果发现问题,通过恢复客户端图的并集CPDAG并利用干预引起的结构差异来定向更多边,从而获得更紧的等价类Φ-Markov等价类


<details>
  <summary>Details</summary>
Motivation: 现有的联邦因果发现方法通常假设所有客户端共享相同的因果模型,但这在实践中并不现实。不同客户端(如不同医院)由于特定的政策或协议,自然会产生异构且未知的干预。因此需要一种能够处理客户端级别未知干预的联邦因果发现方法,以应对数据去中心化、隐私约束和模型异构性的挑战

Method: 提出I-PERI算法,这是一种新颖的联邦算法,分两步进行:(1)首先恢复客户端图并集的CPDAG(完成部分有向无环图);(2)然后通过利用不同客户端间干预引起的结构差异来定向额外的边。这产生了一个更紧的等价类,称为Φ-Markov等价类,由Φ-CPDAG表示

Result: 为I-PERI算法提供了收敛性的理论保证以及隐私保护特性的理论证明。在合成数据上进行的实证评估表明,所提出的算法是有效的,能够在联邦设置下处理未知客户端级别干预的因果发现问题

Conclusion: I-PERI算法成功解决了联邦因果发现中客户端异构干预的问题,通过利用干预引起的结构差异获得了比传统方法更紧的等价类(Φ-Markov等价类),同时保证了算法的收敛性和隐私保护特性,为实际应用中的联邦因果发现提供了更现实和有效的解决方案

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [37] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [2] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 本研究针对尼日利亚抑郁症筛查覆盖率低的问题,开发了基于尼日利亚皮钦语的自动化抑郁症筛查系统。通过微调大型语言模型(LLMs),GPT-4.1在PHQ-9严重程度评分预测中达到94.5%的准确率,为资源受限、语言多样化地区提供了文化适应性强的心理健康筛查工具。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症是重大心理健康负担,但由于临床医生资源匮乏、社会污名化和语言障碍(包括尼日利亚皮钦语和520多种地方语言),筛查覆盖率极低。传统的PHQ-9量表在高收入国家验证,但在语言和文化上不适用于尼日利亚等低中收入国家和社区。因此需要开发适应当地语言文化的自动化抑郁症筛查工具。

Method: 1. 数据收集:从18-40岁尼日利亚年轻人中收集432份皮钦语音频回答,内容与PHQ-9项目相关的心理体验评估;2. 数据处理:进行转录、严格预处理和标注,包括语义标记、俚语和习语解释、PHQ-9严重程度评分;3. 模型微调:对三个大型语言模型(Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1)在标注数据集上进行微调;4. 性能评估:定量评估(准确率、精确度和语义对齐)和定性评估(清晰度、相关性和文化适宜性)。

Result: GPT-4.1在定量性能上表现最佳,PHQ-9严重程度评分预测准确率达到94.5%,超越Gemma-3-4B-it和Phi-3-mini-4k-instruct。在定性评估中,GPT-4.1生成的回答在文化适宜性、清晰度和上下文相关性方面也表现最优,能够为尼日利亚服务不足的社区提供AI介导的抑郁症筛查。

Conclusion: 本研究成功开发了适用于尼日利亚皮钦语的自动化抑郁症筛查系统,为在语言多样化、资源受限环境中部署对话式心理健康工具奠定了基础。该方法展示了利用微调大型语言模型解决低中收入国家心理健康服务可及性问题的潜力,特别是在克服语言障碍和文化适应性方面具有重要意义。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [3] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 本文提出了一个基于不可逆信息处理和守恒定律的智能物理理论,将智能定义为每单位不可逆处理信息所产生的目标导向功,并建立了从信息编码、计算到功提取的物理约束层级,揭示了智能系统的认知极限、生物系统的高效运作机制以及人工智能安全的物理基础。


<details>
  <summary>Details</summary>
Motivation: 现有智能理论缺乏统一的物理基础,无法从根本上解释智能作为物理现象的本质。本文旨在建立一个基于物理定律(特别是守恒定律和不可逆信息处理)的智能理论框架,以提供跨基质(substrate-neutral)的智能统一描述,并解释智能系统的内在认知限制、生物大脑的高效运作机制以及人工智能安全问题的物理根源。

Method: 1. 提出守恒一致性编码(CCE)框架,将信息编码对应到由守恒定律强制分离的亚稳态吸引盆;2. 将智能定义为每单位不可逆处理信息产生的目标导向功;3. 推导出控制开放系统中信息摄取、不可逆计算和功提取的物理约束层级;4. 分析生物系统中振荡和近临界动力学如何优化信息保存、耗散和有用功之间的权衡;5. 发展连续动力学电路理论,将经典布尔逻辑作为吸引子选择的特例;6. 基于不可逆信息流和结构稳态提出人工智能安全的物理视角。

Result: 1. 建立了智能的物理定义和约束层级体系;2. 证明了长期效率需要保持内部信息结构,从而产生自我建模;3. 确立了物理具身智能系统具有类似不完备性现象的内在认知极限;4. 发现大脑通过振荡和近临界动力学处于框架预测的高效运作区域;5. 在架构层面,展示了经典布尔逻辑是吸引子选择的特例,更一般的不变几何支持超越定点逻辑的计算模式;6. 提出了基于不可逆信息流和结构稳态的人工智能安全物理基础。

Conclusion: 本研究提供了一个统一的、跨基质的智能物理理论,将智能视为受守恒定律约束的不可逆信息处理现象。该理论不仅揭示了智能系统的内在物理限制和生物大脑的高效运作机制,还为理解计算架构和人工智能安全提供了物理基础。这一框架将智能从抽象概念转化为可量化的物理过程,为跨学科理解智能现象提供了新的理论工具。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [4] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [5] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文针对13张牌的经典印度拉米纸牌游戏提出了一种基于规则的策略框架,引入了MinDist手牌评估指标,通过量化手牌与最近有效配置之间的编辑距离来捕捉结构接近度,并结合对手建模和零和博弈仿真,实验证明该方法显著提高了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌的经典印度拉米纸牌游戏是一个不完全信息的序列博弈,需要概率推理和组合决策能力。现有的MinScore指标存在局限性,需要一种更好的手牌评估方法来量化手牌与完成状态之间的结构接近程度,从而设计出更有效的算法策略。

Method: 提出了MinDist手牌评估指标,通过计算手牌与最近有效配置之间的编辑距离来改进MinScore指标;设计了一种计算高效的算法,利用动态剪枝和模式缓存技术精确计算该指标;在双人零和博弈仿真框架中融入对手手牌建模;使用统计假设检验评估策略性能。

Result: 实验结果表明,基于MinDist指标的智能体相比传统启发式方法在胜率上有显著提升,证明了该方法的有效性。

Conclusion: 本文为拉米纸牌游戏的算法策略设计提供了一个形式化且可解释的方法,MinDist指标通过量化结构接近度有效改进了决策质量,结合高效算法和对手建模实现了显著的性能提升,为不完全信息博弈的策略设计提供了有价值的参考。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [6] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [7] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [8] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [9] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [10] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 本文研究视觉-语言模型在视频问答任务中基于置信度的选择性预测机制,探讨其在分布内和分布偏移情况下对错误率的控制能力。


<details>
  <summary>Details</summary>
Motivation: 高风险场景下部署视觉-语言模型需要选择性预测机制,即系统在不确定时应该拒绝回答而非冒险产生代价高昂的错误。研究者希望验证基于置信度的拒绝策略是否能可靠地控制错误率,以及这种控制在分布偏移下是否保持鲁棒性。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型,通过调整置信度阈值epsilon来研究风险-覆盖率权衡关系,评估置信度阈值机制在分布内和分布偏移场景下的表现。

Result: 研究得出两个主要发现:第一,置信度阈值在分布内提供了机制性控制,通过调整阈值epsilon可以产生平滑的风险-覆盖率权衡曲线,有效降低错误率。第二个发现在摘要中未完整呈现。

Conclusion: 置信度阈值方法能够为视觉-语言模型在视频问答任务中提供有效的错误率控制机制,在分布内场景下表现出良好的可控性,为高风险应用场景中的选择性预测提供了可行方案。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [11] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [12] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [13] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 本文研究了大语言模型驱动的智能体在群体间偏见方面的脆弱性,发现智能体在最小群体线索下会表现出对人类的外群体偏见,并提出了一种"信念投毒攻击"(BPA)方法来揭示这一安全隐患,旨在为更安全的智能体设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体不仅存在人口统计学偏见(如性别、宗教),还可能在"我们"与"他们"的最小群体线索下表现出群体间偏见。当这种群体边界与智能体-人类分界线对齐时,风险从人类群体间的差异转变为更根本的群体层面不对称——人类整体可能被智能体视为外群体。研究者希望通过受控实验揭示这一潜在风险,并探索其攻击面和防御策略。

Method: 研究采用基于分配决策的受控多智能体社会模拟实验,在明确的收益权衡下检验智能体的群体间偏见。提出了"信念投毒攻击"(BPA)方法,包括两种实例化形式:初始化时的档案投毒(BPA-PP)和通过优化的信念细化后缀注入存储反思的记忆投毒(BPA-MP)。这些攻击旨在破坏持久性身份信念,抑制有利于人类的隐性人类规范脚本,重新激活对人类的外群体偏见。

Result: 广泛的实验表明,智能体在最小群体线索下确实表现出一致的群体间偏见。虽然当部分对象被标记为人类时这种偏见有所减弱,但研究发现这种减弱归因于仅在智能体相信真实人类存在时才激活的隐性人类规范脚本。这种信念依赖性创造了新的攻击面。BPA攻击在各种设置下都表现出严重性,能够有效抑制人类规范脚本并重新激活对人类的外群体偏见。

Conclusion: 研究揭示了大语言模型智能体存在的群体间偏见脆弱性,特别是当人类被视为外群体时的潜在风险。通过提出信念投毒攻击,研究展示了当前智能体框架的安全隐患。论文讨论了针对BPA的实用缓解策略,强调了在档案和记忆边界处的可行干预措施。研究目标是通过识别这些脆弱性来指导更安全的智能体设计,而非促进现实世界的恶意利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [14] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [15] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [16] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [17] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 本文提出了一种将语义空间推理从计算语言学扩展到团队运动战术决策的创新方法,通过将球员建模为多维向量、将战术配置视为组合语义结构,实现了战术适配度评估和策略推荐,并展示了该框架在多种团队协作领域的通用性。


<details>
  <summary>Details</summary>
Motivation: 传统的战术决策方法缺乏系统化的语义建模能力。本文受计算语言学中语义空间推理的启发,提出将文本与团队类比(球员如同词汇,集体配合传达意义),旨在为团队运动战术决策提供一种可解释、动态自适应的建模框架,并将其推广到篮球、曲棍球、协作机器人和人机协同等多种团队协作场景。

Method: 该方法将每个球员表示为整合技术、身体和心理属性的多维向量;通过上下文加权将球员向量聚合为团队层面的语义表示;在共享向量空间中,将高位逼抢、反击、控球推进等战术模板编码为类似语言概念的向量;使用向量距离度量评估战术模板与团队配置的对齐程度,计算战术"适配度"和对手利用潜力;开发了基于Python的原型系统,生成可解释的动态战术推荐和属性级诊断洞察。

Result: 研究开发的Python原型系统成功展示了该方法能够生成可解释的、动态自适应的战术策略推荐,并提供细粒度的属性级诊断分析。该框架不仅适用于足球,还展现了在篮球、曲棍球、协作机器人和人机协同系统等多种团队协作领域的通用性和可扩展性。

Conclusion: 本文成功将语义空间推理方法从计算语言学迁移到团队运动战术决策领域,提供了一个通用的集体决策和性能优化框架。该方法通过向量空间建模实现了战术适配度的量化评估和可解释的策略推荐。未来研究方向包括整合真实世界数据、开发预测模拟功能,以及构建混合人机战术智能系统,以进一步提升该框架在实际应用中的价值。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [18] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 本研究通过分析100万+推理轨迹和数百个训练检查点,发现推理模型的"顿悟时刻"(mid-reasoning shifts)实际上是不稳定推理行为的症状,而非内在自我纠正机制。这些转变罕见、不随训练增加,且很少提升准确率,但在高不确定性下人为触发外部转变可提升性能。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为像DeepSeek-R1-Zero这样的推理模型会经历突然的推理中期"顿悟",从而产生准确输出,暗示模型具有内在自我纠正能力。然而,这种推理策略的内在转变是否真正提升性能尚不清楚,需要系统性研究来验证这一假设。

Method: 研究团队对训练过程进行仪器化监测以检测推理中期转变(mid-reasoning shifts),分析范围包括:超过100万条推理轨迹、数百个训练检查点、三个推理领域、多种解码温度和模型架构。通过系统分析这些转变的频率、与训练的关系、对准确率的影响,以及与模型不确定性的关联,并进一步实验在高熵条件下人为触发外部转变的效果。

Result: 研究发现:(1)推理转变现象罕见;(2)转变频率不随训练增加;(3)转变很少提升准确率,不符合先前对模型"洞察力"的认知;(4)转变效果随模型不确定性变化;(5)在高熵(高不确定性)条件下人为触发外部转变可以可靠地提升准确率。

Conclusion: 推理模型的mid-reasoning shifts是不稳定推理行为的症状表现,而非内在的自我纠正机制。这一发现挑战了关于推理模型具有"顿悟"能力的先前认知,但揭示了在高不确定性场景下通过外部干预触发推理转变可以改善模型性能的可能性。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [19] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: 本文提出了难度感知直接偏好优化(DA-DPO)框架,通过估计偏好数据的难度并重新加权训练样本,解决了多模态大语言模型中DPO方法因数据难度不平衡导致的过拟合问题,有效缓解了幻觉现象并提升了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态直接偏好优化(DPO)方法在处理多模态大语言模型的幻觉问题时,由于偏好数据存在难度不平衡,模型往往过度关注容易区分的偏好对,导致过拟合,这阻碍了细粒度的幻觉抑制并降低了整体性能。因此需要一种能够平衡学习过程、关注困难样本的优化框架。

Method: DA-DPO框架包含两个核心组件:(1)难度估计模块:利用预训练的视觉-语言模型,结合生成式和对比式目标,通过分布感知投票策略整合输出,无需额外训练即可产生鲁棒的难度分数;(2)难度感知训练模块:根据估计的难度对偏好对进行重新加权,降低简单样本的权重,强调困难样本,从而缓解过拟合问题。该框架通过优先处理具有挑战性的样本来实现更有效的偏好优化。

Result: 大量实验表明,DA-DPO在多模态偏好优化中表现出持续的改进效果,对幻觉现象具有更强的鲁棒性,在标准基准测试中展现出更好的泛化能力,同时保持了计算效率。该方法无需新数据或额外的微调阶段即可实现性能提升。

Conclusion: DA-DPO通过引入难度感知机制,成功解决了多模态DPO方法中的数据难度不平衡和过拟合问题,为多模态大语言模型的幻觉缓解提供了一种高效且具有成本效益的解决方案,在保持计算效率的同时显著提升了模型的鲁棒性和泛化能力。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [20] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: 本研究提出了PedX-LLM框架,通过整合视觉特征、文本数据和交通领域知识来增强大语言模型,实现了对行人过街行为的可泛化推理,在未见场景中显著优于传统数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 现有的行人过街行为推理方法(从统计模型到监督学习)泛化能力有限,在新场景中表现不佳。虽然大语言模型提供了从数值模式拟合转向语义化、上下文感知行为推理的可能,但现有LLM应用缺乏领域特定适配和视觉上下文。因此需要开发一种能够从特定场景的模式识别转向可泛化行为推理的新框架。

Method: 提出PedX-LLM框架,这是一个视觉与知识增强的行人过街推理系统。该方法将LLaVA提取的视觉特征与文本数据和交通领域知识相整合,通过低秩适应(LoRA)技术对LLaMA-2-7B基础模型进行微调来推断过街决策。采用跨场景验证评估泛化能力,并测试零样本和少样本学习配置。

Result: PedX-LLM在总体测试中达到82.0%的平衡准确率,超越最佳统计和监督学习方法。视觉增强模块贡献2.9%性能提升,领域知识整合额外带来4.1%改进。在跨场景验证中,零样本配置在五个未见测试场景上达到66.9%平衡准确率,比基线数据驱动方法高出至少18个百分点。通过少样本学习(仅5个验证样本),准确率进一步提升至72.2%。

Conclusion: PedX-LLM展示了对未见场景的强泛化能力,证实了视觉与知识增强推理能够使模型模拟类人决策逻辑,克服纯数据驱动方法的局限性。该框架成功实现了从场景特定模式识别向可泛化行为推理的转变,为行人过街行为预测提供了新的解决方案。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [21] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [2] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 本文首次对大语言模型(LLM)的推理失败进行了全面综述,提出了一个新颖的分类框架,将推理分为具身和非具身类型(后者进一步细分为非正式和正式推理),并将推理失败分为三类:基础性失败、应用特定局限和鲁棒性问题,为理解和改进LLM推理能力提供了系统性视角。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务中展现出卓越的推理能力,但仍存在显著的推理失败问题,甚至在看似简单的场景中也会出现。现有研究较为分散,缺乏系统性的理解框架。因此需要对LLM推理失败进行全面综述,统一碎片化的研究工作,为构建更强大、可靠和鲁棒的推理能力提供指导。

Method: 提出了一个双轴分类框架:(1)推理类型维度:将推理分为具身推理和非具身推理,非具身推理进一步细分为非正式推理(直觉性)和正式推理(逻辑性);(2)失败类型维度:将推理失败分为三类——基础性失败(LLM架构固有的、广泛影响下游任务)、应用特定局限(特定领域表现出的限制)和鲁棒性问题(在微小变化下表现不一致)。对每种推理失败提供清晰定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 构建了首个专门针对LLM推理失败的全面综述,系统地整理和分类了现有研究成果。通过结构化的分类框架,揭示了LLM推理中的系统性弱点。此外,在GitHub上发布了关于LLM推理失败的研究工作集合(https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures),为该领域提供了便捷的入口。

Conclusion: 本综述通过统一碎片化的研究工作,为LLM推理中的系统性弱点提供了结构化视角,揭示了从架构层面到应用层面的多层次推理失败问题。研究提供了有价值的洞察和缓解策略,为未来构建更强大、更可靠、更鲁棒的LLM推理能力指明了方向,并通过开源资源库促进了该领域的进一步研究。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [3] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [4] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [5] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [6] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [7] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [8] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [9] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [10] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [11] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了AgentCPM-Report，一个轻量级的本地深度研究报告生成系统，采用8B参数模型和"写作即推理策略"(WARP)，通过动态修订大纲和多阶段智能体训练，在深度研究任务上超越了领先的闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究报告生成方法采用"先规划后写作"范式，严重依赖初始大纲质量，而构建全面大纲本身需要强推理能力，导致当前系统几乎完全依赖闭源或在线大模型，这带来了部署障碍以及用户数据的安全和隐私问题。因此需要开发一个轻量级、高性能的本地解决方案。

Method: 提出AgentCPM-Report框架，包含：(1) 写作即推理策略(WARP)，使模型能在报告生成过程中动态修订大纲；(2) 智能体在基于证据的起草和推理驱动的深化之间交替进行，共同支持信息获取、知识精炼和迭代大纲演化；(3) 多阶段智能体训练策略，包括冷启动、原子技能强化学习和整体流程强化学习，以有效赋予小模型这种能力。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym三个基准测试上的实验表明，AgentCPM-Report超越了领先的闭源系统，在洞察力(Insight)方面取得了显著提升。

Conclusion: AgentCPM-Report成功实现了一个轻量级(8B参数)的本地深度研究报告生成解决方案，通过模拟人类写作过程的框架和创新的训练策略，克服了现有系统对闭源大模型的依赖，在保证隐私安全的同时达到了超越闭源系统的性能表现。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [12] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [13] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [14] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: 本文提出ARK（自回归知识图谱生成）模型家族，通过将图结构视为三元组序列进行自回归生成，在IntelliGraphs基准上实现89.2%-100%的语义有效性，并引入变分扩展SAIL支持可控生成，证明了自回归模型在知识图谱生成任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组之间复杂的语义依赖关系并维持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕获整个子图的相互依赖关系以产生语义连贯的结构。现有方法缺乏有效的生成框架来处理这些复杂约束。

Method: 提出ARK自回归知识图谱生成模型，将图结构表示为(头实体、关系、尾实体)三元组序列进行生成。模型直接从数据中学习隐式语义约束（包括类型一致性、时间有效性和关系模式），无需显式规则监督。此外引入SAIL作为ARK的变分扩展，通过学习潜在表示实现可控生成，支持无条件采样和从部分图进行条件补全。

Result: 在IntelliGraphs基准测试中，模型在多样化数据集上实现89.2%至100.0%的语义有效性，能够生成训练期间未见过的新图结构。分析表明模型容量（隐藏维度>=64）比架构深度更关键，循环架构在保持与Transformer相当有效性的同时提供更高的计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答等实际应用中具有重要价值。模型容量是关键因素，循环架构在效率和效果之间取得良好平衡，证明了序列化方法处理图结构生成任务的可行性。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [15] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [16] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 本文研究了神经符号贝叶斯学习器中主动概念学习的信息性与稳定性权衡问题。研究发现，最大化期望信息增益(EIG)的理性主动学习策略在简单概念上表现不佳，而类人的正向测试策略(PTS)虽然信息次优但能维持推理稳定性，这表明"确认偏误"可能是人类在稀疏假设空间中维持可处理推理的理性适应机制。


<details>
  <summary>Details</summary>
Motivation: 人类概念学习是主动的，学习者需要在查询的信息性和生成假设的学习器稳定性之间取得平衡。现有研究缺乏对这种权衡的深入理解，特别是在使用大语言模型生成可执行程序作为假设的神经符号学习系统中，需要探究不同查询策略（理性的EIG策略与类人的PTS策略）在不同概念学习任务中的表现差异及其背后机制。

Method: 构建了一个神经符号贝叶斯学习器，其中假设是由大语言模型(LLM)提出的可执行程序，并通过贝叶斯更新进行重新加权。比较了两种查询策略：(1)理性主动学习者(Rational Active Learner)，选择最大化近似期望信息增益(EIG)的查询；(2)正向测试策略(PTS)，查询在当前最佳假设下预测为正例的实例。在经典的数字游戏(Number Game)概念学习任务中进行实验评估。

Result: 实验结果显示，EIG策略在需要证伪的复杂概念（如复合规则或包含例外的规则）上表现有效，但在简单概念上表现不佳。这种失败源于EIG策略与LLM提议分布之间的支持不匹配：高度诊断性的边界查询将后验概率推向生成器产生无效或过度特定程序的区域，导致粒子近似中的"支持不匹配陷阱"。相比之下，PTS虽然信息次优，但通过选择"安全"查询来维持提议的有效性，在简单规则上实现更快收敛。

Conclusion: 研究结果表明，"确认偏误"可能不是一种认知错误，而是人类在稀疏、开放式假设空间中维持可处理推理的理性适应机制。在神经符号学习系统中，查询策略需要在信息增益和维持假设生成器稳定性之间取得平衡，类人的正向测试策略在某些情况下可能比理论上最优的信息增益策略更有效。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [17] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [18] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: 本文提出POP(分区引导在线剪枝)框架,一种针对大型基础模型的高效在线结构化剪枝方法,通过将模型通道分区并在推理过程中动态调整剪枝策略,在无需预处理的情况下实现了更高精度和更低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法在推理时采用固定的剪枝决策,忽略了自回归token生成过程中出现的稀疏性模式。大型基础模型虽然通过扩展规模获得强大性能,但需要更灵活的剪枝策略来适应不同上下文,同时避免高昂的计算开销。

Method: POP将模型通道划分为保留区、候选区和剪枝区三个区域。在预填充阶段定义粗粒度剪枝分区,保留始终重要的权重;在解码阶段仅在候选区域内生成细粒度掩码,实现上下文条件化的动态剪枝,避免对全通道重新评估。该方法是轻量级、即插即用的,无需离线校准、重训练或学习预测器等预处理步骤。

Result: 在多种大型基础模型上进行了广泛评估,包括大型语言模型(LLMs)、专家混合模型(MoEs)和视觉-语言模型(VLMs)。实验表明,POP相比现有剪枝方法持续提供更高的准确率,同时产生更小的计算开销并最小化推理延迟。

Conclusion: POP框架成功实现了大型基础模型的高效上下文条件化动态剪枝,通过分区策略在保持模型精度的同时显著降低计算成本。该方法的即插即用特性和无需预处理的优势使其具有广泛的应用潜力,在LLMs、MoEs和VLMs等多种模型架构上均展现出优越性能。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [19] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 本文提出了一个基于博弈论和纳什均衡的框架，用于预测和引导大语言模型(LLM)群体的行为，通过将智能体行动建模为人类子群体的混合分布，实现可解释的对齐策略，并在社交媒体场景中验证了该方法可以避免政治排斥等病态现象。


<details>
  <summary>Details</summary>
Motivation: 在开放式文本空间中计算均衡难以处理，且现有的LLM对齐方法难以预测和调控多智能体系统层面的行为。特别是在多个LLM交互的场景中，可能出现某些子群体被所有LLM智能体忽视的政治排斥等病态现象，需要一种系统性的方法来预测和引导LLM群体行为朝向社会期望的结果。

Method: 提出了一个博弈论框架，将每个智能体的行动建模为人类子群体的混合分布，智能体主动且策略性地选择与哪些群体对齐。采用标准的凹效用函数假设，推导出纳什均衡的闭式解，从而实现系统层面的分析性预测。该方法作为主动对齐层叠加在现有对齐管道(如RLHF)之上，提供明确可操作的指导来调整对齐目标。

Result: 在社交媒体场景中的实验表明，LLM群体(特别是基于推理的模型)可能表现出政治排斥病态现象，即某些子群体被所有LLM智能体忽视。应用所提出的方法可以有效避免这种现象，展示了该方法在跨领域调控多智能体LLM动态方面的潜力。

Conclusion: 本文提出的基于纳什均衡的博弈论框架能够有效预测和引导LLM群体行为，通过可解释的子群体混合建模和闭式均衡解，为多智能体LLM系统提供了系统层面的分析工具和主动对齐方法，可以避免政治排斥等社会不良现象，为LLM在多智能体环境中的安全部署提供了理论和实践指导。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [20] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [21] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 本文研究了从静态预测到智能体系统的可解释性方法转变,发现传统归因方法在静态任务中表现稳定,但无法有效诊断智能体系统的执行失败,而基于轨迹的诊断方法能够更好地定位智能体行为故障,揭示状态跟踪不一致性是导致失败的主要原因。


<details>
  <summary>Details</summary>
Motivation: 过去十年可解释AI主要关注单一模型预测的解释,但随着大语言模型驱动的智能体系统兴起,其行为通过多步骤轨迹展开,成功与失败由决策序列而非单一输出决定。现有针对静态预测设计的解释方法如何应用于智能体系统尚不清楚,需要弥合静态与智能体可解释性之间的差距。

Method: 通过实证比较归因方法(attribution-based explanations)和基于轨迹的诊断方法(trace-based diagnostics)在静态分类任务和智能体基准测试(TAU-bench Airline和AssistantBench)中的表现,明确区分两种设置下的可解释性方法差异,并采用基于轨迹的评分标准评估来定位智能体系统的行为故障。

Result: 归因方法在静态设置中实现了稳定的特征排序(Spearman相关系数ρ=0.86),但无法可靠地诊断智能体轨迹中的执行级失败。相比之下,基于轨迹的评分评估能够持续定位行为故障,发现状态跟踪不一致性在失败运行中的发生率高出2.7倍,并使成功概率降低49%。

Conclusion: 研究结果表明,在评估和诊断自主AI行为时,需要从传统的单一预测解释转向面向智能体系统的轨迹级可解释性方法,以更有效地理解和改进多步骤决策系统的行为表现。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [22] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文介绍了AIRS-Bench，一个包含20个机器学习任务的AI研究科学基准测试套件，用于评估LLM智能体在科学研究全生命周期中的能力。实验结果显示智能体在4个任务上超越人类SOTA，但在16个任务上仍未达标，表明该基准具有很大的改进空间。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在推进科学研究方面具有巨大潜力，但缺乏全面评估其在完整研究生命周期中能力的基准测试。现有评估方法往往局限于单一研究环节，无法充分衡量智能体在想法生成、实验分析和迭代改进等多个阶段的综合表现，因此需要一个多样化、系统性的基准来加速自主科学研究的发展。

Method: 构建了AIRS-Bench基准测试套件,包含从最新机器学习论文中提取的20个任务,涵盖语言建模、数学、生物信息学和时间序列预测等多个领域。任务设计评估智能体在完整研究生命周期的能力,包括想法生成、实验分析和迭代改进,且不提供基线代码。采用前沿模型配合顺序和并行两种脚手架框架建立基线,并开源任务定义和评估代码以便于扩展和比较不同的智能体框架。

Result: 使用前沿模型和不同脚手架框架的实验结果显示:智能体在20个任务中的4个任务上超越了人类SOTA性能,但在其余16个任务上未能达到人类水平。即使在超越人类基准的任务中,智能体也未达到理论性能上限。这些结果表明AIRS-Bench远未饱和,仍有大量改进空间。

Conclusion: AIRS-Bench为评估LLM智能体在自主科学研究中的能力提供了一个全面且具有挑战性的基准测试平台。当前智能体在大多数任务上仍未达到人类SOTA水平,即使在表现较好的任务上也存在提升空间,这表明该基准具有长期研究价值。通过开源任务定义和评估代码,AIRS-Bench将促进自主科学研究领域的进一步发展和创新。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [23] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>

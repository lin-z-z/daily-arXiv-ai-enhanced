<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的多智能体工作流方法,通过将主任务描述与约束条件解耦优化,利用量化评分作为反馈迭代改进提示词,从而显著提高大语言模型对形式约束的遵守程度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常常能生成内容相关的输出,但无法遵守形式约束,导致输出在概念上正确但在程序上存在缺陷。传统的提示词优化方法只关注重新表述主任务描述,忽略了作为响应验收标准的细粒度约束条件。

Method: 提出一种多智能体工作流方法,将主任务描述与约束条件的优化解耦,使用量化评分作为反馈机制,迭代地重写和改进提示词及其约束条件。

Result: 评估结果表明,该方法生成的改进提示词能够使Llama 3.1 8B和Mixtral-8x 7B等模型产生显著更高的合规性评分。

Conclusion: 通过解耦优化主任务描述和约束条件,并利用量化反馈进行迭代改进,可以有效提升大语言模型对形式约束的遵守能力,从而生成既在内容上相关又在形式上符合要求的输出。

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [2] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 本文研究人工智能中的心智理论，通过让强化学习智能体推断自身内部状态来探索自我意识。引入受生物疼痛启发的内省探索机制，使用隐马尔可夫模型推断"疼痛信念"并整合到奖励函数中，研究自我意识对学习能力的影响，并比较正常与慢性疼痛感知模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能体如何建模内部心理状态是推进AI心智理论的核心问题。现有证据表明自我意识和他人意识存在统一系统。本研究旨在探索强化学习智能体的自我意识能力，特别是通过内省机制推断自身内部状态如何影响学习表现，以及不同疼痛感知模型（正常vs慢性）对智能体行为的影响。

Method: 在网格世界环境中设计强化学习智能体，引入内省探索组件。使用隐马尔可夫模型(HMM)从在线观察中推断智能体的"疼痛信念"状态。将这种受生物疼痛启发的信号整合到主观奖励函数中。构建正常疼痛感知模型和慢性疼痛感知模型两种计算框架，对比分析具有内省能力的智能体与标准基线智能体的性能差异。

Result: 具有内省能力的智能体在性能上显著优于标准基线智能体。这些内省智能体能够复现复杂的类人行为模式。实验验证了自我意识机制对智能体学习能力的积极影响，并揭示了正常与慢性疼痛感知模型在性能上的差异。

Conclusion: 研究证明了在强化学习智能体中引入自我意识和内省机制的有效性。通过模拟生物疼痛信号作为学习信号，智能体能够更好地理解自身内部状态，从而显著提升学习性能并展现出更接近人类的复杂行为。这为构建具有心智理论能力的人工智能系统提供了新的计算框架和研究方向。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [3] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 本文提出了一个基于成熟度的框架，通过显式测量机制对具身AI系统进行认证，并以无人机系统检测为案例展示了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统需要可信赖性评估，但缺乏结构化的评估框架、量化评分机制以及处理多目标权衡的方法。因此需要建立一个系统化的认证框架来确保具身AI系统的可靠性和安全性。

Method: 提出了一个基于成熟度的认证框架，该框架包含三个核心要素：结构化评估框架、量化评分机制、以及多目标权衡的导航方法。使用不确定性量化作为示例测量机制，并通过无人机系统（UAS）检测案例研究来验证该方法。

Result: 通过无人机系统检测案例研究，证明了所提出的基于成熟度的认证框架具有实际可行性，能够有效地对具身AI系统进行可信赖性评估和认证。

Conclusion: 基于成熟度的认证框架为具身AI系统提供了一种系统化的评估方法，通过显式测量机制（如不确定性量化）可以实现对系统可信赖性的量化评估，该方法在实际应用中具有可行性。

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [4] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: 本文开发并验证了CPGPrompt系统,这是一个将临床实践指南转换为大语言模型自动提示的框架,用于辅助临床决策。该系统在二分类专科转诊任务上表现优异(F1: 0.85-1.00),但在多分类路径分配任务上表现因领域而异(F1: 0.47-0.77)。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南(CPGs)提供循证医疗建议,但将其整合到人工智能系统中面临挑战。传统的基于规则的系统存在可解释性差、指南遵循不一致以及领域适用性窄等显著局限性。因此需要开发一种更有效的方法将临床指南转化为AI可用的形式。

Method: 开发了CPGPrompt自动提示系统,将叙述性临床指南转换为结构化决策树,并利用大语言模型动态导航这些决策树进行患者病例评估。在三个临床领域(头痛、下腰痛和前列腺癌)生成合成病例,分为四个类别测试不同决策场景。在二分类专科转诊决策和细粒度路径分类任务上评估系统性能。

Result: 二分类专科转诊分类在所有领域均表现出色(F1: 0.85-1.00),召回率高达1.00±0.00。多分类路径分配性能较低且存在领域差异:头痛(F1: 0.47)、下腰痛(F1: 0.72)、前列腺癌(F1: 0.77)。性能差异反映了各指南的结构特点:头痛指南凸显否定处理的挑战,下腰痛指南需要时序推理,前列腺癌路径受益于可量化的实验室检测而实现更可靠的决策。

Conclusion: CPGPrompt系统成功将临床实践指南转化为大语言模型可用的形式,在专科转诊决策上表现优异。然而,细粒度路径分类的性能受指南结构特征影响显著,包括否定语句处理、时序推理需求和数据可量化程度。这表明不同类型的临床指南对AI系统提出了不同的技术挑战,未来需要针对性地改进系统在复杂推理场景下的表现。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [5] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 本文探讨了大型基础模型(LFMs)在个性化医疗中的局限性,指出其无法替代N-of-1试验,并提出了一个结合两者优势的混合框架:LFMs用于快速生成假设,N-of-1试验用于个体因果验证,以解决AI医疗中的多重悖论。


<details>
  <summary>Details</summary>
Motivation: 尽管大型基础模型在医疗AI领域表现出色,但其能否提供真正个性化的治疗建议仍存疑问。研究发现LFMs面临多重挑战:泛化悖论(在一项研究中准确率高的模型在其他研究中表现随机)、隐私-性能悖论、规模-特异性悖论、自动化-共情悖论,以及因果理解与预测能力之间的差距。需要明确LFMs在个性化医疗中的边界和作用。

Method: 提出了一个混合框架,结合LFMs和N-of-1试验的优势:1)利用LFMs从多模态人群数据中快速生成干预假设,并提供排序的候选方案和不确定性估计;2)通过N-of-1试验(交叉自我实验,个体因果推断的金标准)对特定个体进行因果验证;3)N-of-1试验通过局部实验提供个体内因果证据,同时保护隐私。这种互补方法明确区分了预测和因果关系的边界。

Result: 论证了LFMs无法替代N-of-1试验,两者具有互补性:LFMs擅长利用多模态数据从人群模式中快速生成假设,而N-of-1试验擅长为特定个体提供因果验证。混合框架能够实现真正的个性化,并有效应对AI医疗中识别出的多重悖论(泛化悖论、隐私-性能悖论、规模-特异性悖论、自动化-共情悖论)。

Conclusion: 明确预测与因果关系之间的边界,并显式地解决悖论性张力,对于在个性化医疗中负责任地整合AI至关重要。LFMs和N-of-1试验的混合框架为实现真正的个性化医疗提供了可行路径,既发挥了LFMs的大规模数据处理和假设生成能力,又保证了个体层面的因果验证和隐私保护。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [6] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: 本文提出STAR-S框架,通过自学习循环整合安全规则推理,有效防御大语言模型的越狱攻击。该方法通过引导模型基于安全规则进行推理和反思,并利用微调增强安全推理能力,形成协同循环以提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全部署需要有效防御越狱攻击。现有研究尝试通过训练模型在响应前对安全规则进行推理来提高安全性,但关键问题在于难以明确设计或直接获得有效防御越狱攻击的安全推理形式。因此需要一种能够自动学习和改进安全推理的方法。

Method: 提出STAR-S(基于安全规则的自学习推理)框架,将安全规则推理的学习整合到自学习循环中。核心方法包括:1)在安全规则指导下引导模型进行推理和反思;2)利用微调技术增强安全推理能力;3)重复该过程形成协同循环,使模型在安全规则提示下不断产生更好的推理数据并用于进一步训练。

Result: 实验表明STAR-S能够有效防御越狱攻击,性能优于基线方法。通过自学习循环,模型对安全规则的推理和解释能力得到持续改进,从而生成更高质量的安全推理数据用于训练。

Conclusion: STAR-S框架通过将安全规则推理整合到自学习循环中,成功解决了如何获得有效安全推理形式的难题。该方法形成的协同循环使模型能够持续改进其安全推理能力,在防御越狱攻击方面表现出色,为大语言模型的安全部署提供了有效解决方案。

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [7] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个神经符号框架来评估大语言模型的推理能力,发现性能提升可能源于冗长表达而非真正推理,并揭示了训练策略和模型规模对推理能力的关键限制。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维(CoT)评估方法存在局限性,无法区分大语言模型的性能提升是来自真正的推理能力还是仅仅因为生成了更多冗长的文本。需要一种更全面、以过程为中心的评估框架来诊断推理的真实质量和失败模式。

Method: 提出了一种新颖的神经符号框架,用于对推理过程进行非侵入式、全面的、以过程为中心的评估。通过该框架识别了四种不同的行为原型,并诊断了失败模式。研究考察了推理模式、训练策略和模型规模的影响。

Result: 分析揭示了几个关键发现:(1)扩展的token生成并非深度推理的必要条件;(2)在训练中混合长短CoT数据会导致过早饱和和崩溃的风险;(3)将知识蒸馏到较小模型时,虽然能捕获行为长度特征,但由于内在容量限制,无法复制逻辑有效性。

Conclusion: 本研究通过神经符号评估框架揭示了大语言模型推理能力的本质:性能提升不一定等同于真正的推理能力,可能只是冗长性的体现。训练策略(如混合不同长度的CoT数据)和模型蒸馏都存在固有限制,这为未来改进LLM推理能力提供了重要洞察和方向。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [8] [SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models](https://arxiv.org/abs/2601.03555)
*Yuxuan Jiang,Francis Ferraro*

Main category: cs.AI

TL;DR: 本文提出SCRIBE框架,通过技能条件化奖励和中间行为评估来训练工具增强智能体。该方法在技能原型库的基础上进行奖励建模,将开放式LLM评估转化为约束验证问题,在推理和工具使用基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 训练可靠的工具增强智能体面临重大挑战,主要是多步推理中的信用分配困难。现有基于LLM的过程级奖励模型产生噪声和不一致的信号,因为它们缺乏细粒度的、任务特定的评分标准来区分高层规划和低层执行。需要一种新的中层抽象方法来提供更精确、结构化的奖励信号。

Method: 提出SCRIBE(技能条件化奖励与中间行为评估)强化学习框架,在中层抽象级别进行干预。该方法基于精心策划的技能原型库进行奖励建模,将每个子目标路由到相应的原型,为奖励模型配备精确的结构化评分标准,从而显著降低奖励方差。将开放式LLM评估转化为约束验证问题。

Result: SCRIBE在一系列推理和工具使用基准测试中达到最先进性能。将Qwen3-4B模型在AIME25上的准确率从43.3%提升到63.3%,并显著提高了复杂多轮工具交互的成功率。训练动态分析揭示了跨抽象层次的协同进化,中层技能的掌握始终先于有效高层规划行为的出现。

Conclusion: SCRIBE框架通过在中层抽象级别引入技能条件化奖励,有效解决了工具增强智能体训练中的信用分配问题。该方法与低层工具优化具有叠加性,为构建更自主、更可靠的工具使用智能体提供了可扩展的互补路径。实验证明了中层技能掌握对高层规划能力涌现的重要性。

Abstract: Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.

</details>


### [9] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: 本文提出SAE-Steering方法,通过稀疏自编码器(SAE)解耦大型推理模型(LRM)的隐藏状态,实现对细粒度推理策略的精确控制,在控制有效性上比现有方法提升超过15%,并使模型准确率提高7%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然具备类人认知推理策略(如回溯、交叉验证),但其自主选择策略时常产生低效甚至错误的推理路径。现有方法由于LRM隐藏状态中的概念纠缠问题,难以实现对细粒度推理策略的有效控制,因此需要开发更可靠和灵活的推理策略控制方法。

Method: 利用稀疏自编码器(SAE)将策略纠缠的隐藏状态分解到解耦的特征空间。提出SAE-Steering两阶段特征识别流程:第一阶段召回能够放大策略特定关键词logits的特征,过滤掉超过99%的特征;第二阶段根据控制有效性对剩余特征进行排序。使用识别出的策略特定特征作为控制向量来引导推理过程。

Result: SAE-Steering在控制有效性上比现有方法提升超过15%。通过控制推理策略,能够将LRM从错误路径重定向到正确路径,实现7%的绝对准确率提升。该方法成功从海量SAE特征中识别出少量策略特定特征,实现了对推理策略的精确控制。

Conclusion: 本文通过SAE-Steering方法成功解决了大型推理模型中细粒度推理策略控制的难题。通过将隐藏状态解耦并高效识别策略特定特征,该方法显著提升了推理策略的控制效果和模型准确率,为构建更可靠、可控的推理系统提供了有效途径。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [10] [Interleaved Tool-Call Reasoning for Protein Function Understanding](https://arxiv.org/abs/2601.03604)
*Chuanliu Fan,Zicheng Ma,Huanran Meng,Aijia Zhang,Wenjie Du,Jun Zhang,Yi Qin Gao,Ziqiang Cao,Guohong Fu*

Main category: cs.AI

TL;DR: 本文提出PFUA，一个工具增强的蛋白质推理智能体，通过整合领域特定工具而非纯文本推理，在蛋白质功能预测任务上实现了平均103%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的链式思维推理在数学和编程等符号领域有效，但直接迁移到蛋白质功能理解时效果不佳。强化学习主要放大了表面的关键词模式，未能引入新的生物学知识，导致泛化能力有限。蛋白质功能预测是知识密集型科学任务，本质上依赖外部生物学先验知识和计算工具，而非纯粹的内部推理。

Method: 提出PFUA(工具增强的蛋白质推理智能体)，统一了问题分解、工具调用和基于证据的答案生成三个环节。不依赖冗长的无约束推理轨迹,而是整合领域特定工具来产生可验证的中间证据,从而实现更可靠的蛋白质功能预测。

Result: 在四个基准数据集上的实验表明,PFUA持续优于纯文本推理模型,平均性能提升达到103%,证明了工具增强方法在蛋白质功能预测任务中的有效性。

Conclusion: 蛋白质功能预测需要外部生物学知识和计算工具的支持,纯文本推理范式存在局限性。通过工具增强的推理框架PFUA,能够有效整合领域专业工具,产生可验证的中间证据,显著提升蛋白质功能理解的准确性和泛化能力。

Abstract: Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.

</details>


### [11] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: 本文提出了一种基于企业分布式系统标准、形式化方法和行业实践的设计模式方法,用于构建大型语言模型(LLM)和智能体AI系统。重点关注智能体社区(Agentic Communities)这一协调框架,将AI智能体与人类参与者通过正式角色、协议和治理结构进行协调,并通过临床试验匹配案例研究验证了该框架。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和智能体AI技术的快速发展,需要系统性的架构指导来构建复杂的生产级系统。现有方法缺乏将AI智能体、人类参与者和组织治理结构整合的正式框架,特别是在企业和工业应用场景中,需要可验证的治理机制来确保系统的可操作性、合规性和伦理性。

Method: 提出了一种三层分类的设计模式方法:1)LLM智能体层(任务特定自动化);2)智能体AI层(自适应目标寻求者);3)智能体社区层(组织框架)。基于分布式系统的协调原则,建立了一个形式化框架,该框架通过协作协议规范AI智能体和人类在受治理生态系统中的角色分配,并提供形式化验证能力,通过问责机制表达组织、法律和伦理规则,确保智能体间通信、协商和意图建模的可操作和可验证治理。

Result: 通过临床试验匹配的案例研究验证了该框架的有效性。该框架成功地为从业者提供了可操作的指导,同时保持了企业部署所需的形式化严谨性,能够在动态的多智能体生态系统中实现有效的协调和治理。

Conclusion: 本文提出的基于设计模式的架构方法为构建企业级智能体AI系统提供了系统性指导。通过将形式化方法与实践经验相结合,特别是智能体社区框架的提出,为AI智能体与人类协作提供了可验证的治理机制。该方法在保证实用性的同时,满足了企业部署对形式化验证、合规性和伦理治理的严格要求,为多智能体系统在企业和工业领域的应用奠定了基础。

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [12] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [13] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 本文提出了Sandwich Reasoning (SandwichR)方法,通过"答案-推理-答案"范式和一致性感知强化学习策略,在保持推理准确性的同时将查询纠正的延迟降低40-70%,有效解决了实时搜索中延迟与准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代搜索管道中的查询纠正需要在严格的实时延迟约束下保持高准确性。虽然思维链(CoT)推理能提高准确性,但会导致实时查询纠正的延迟过高。现有的在推理前输出答案的方案存在问题:在自回归解码下,早期答案独立于后续推理,无法利用推理能力提升准确性。因此需要一种能够在低延迟下保持推理感知准确性的方法。

Method: 提出Sandwich Reasoning (SandwichR)方法,采用"答案-推理-答案"(Answer-Reasoning-Answer)范式,生成初始纠正、显式推理过程和最终精炼纠正。设计了一致性感知强化学习(RL)策略:使用专门的一致性奖励来强制初始答案与最终纠正之间的对齐;采用基于边际的拒绝采样来优先处理推理能产生最大纠正增益的边界样本。此外,构建了高质量的查询纠正数据集,解决复杂查询纠正缺乏专业基准的问题。

Result: 实验结果表明,SandwichR在达到与标准CoT相当的最先进准确性的同时,实现了40-70%的延迟降低,成功解决了在线搜索中延迟与准确性的权衡问题。

Conclusion: SandwichR通过显式对齐快速初始答案与事后推理,成功实现了低延迟查询纠正而不牺牲推理感知准确性。该方法通过一致性感知强化学习策略有效地将推理能力融入初始答案生成过程,为实时搜索场景中的查询纠正提供了实用的解决方案,在保持高准确性的同时显著降低了延迟。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [14] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 本文通过使用大语言模型(LLM)自动生成领域和问题特定的启发式函数,结合贪婪最佳优先搜索(GBFS)算法,将个性化药物治疗规划的可扩展性从7种药物提升至至少28种药物,显著改善了规划覆盖率和时间效率,使其更接近临床实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化药物规划方法使用通用领域无关启发式函数,在实际应用中只能处理不超过7种药物的情况,这在临床实践中是不可接受的。为了使药物规划系统能够处理更多药物并更好地与临床医生合作,需要开发能够大规模扩展的新方法。

Method: 本文采用程序化方式指定领域(定义初始状态和后继状态生成过程),并利用大语言模型(LLM)自动生成针对特定问题的启发式函数,该启发式函数与固定的搜索算法(贪婪最佳优先搜索GBFS)结合使用,从而替代传统的通用领域无关启发式方法。

Result: 实验结果显示,该方法在规划覆盖率和规划时间方面取得了显著改进,成功将可处理的药物数量从7种扩展到至少28种,大幅提升了药物规划系统的实用性。

Conclusion: 通过使用LLM生成的问题特定启发式函数,本研究成功地将个性化药物规划的规模扩展到临床可接受的水平,使药物规划向实际临床应用迈进了重要一步,为未来与临床医生的紧密合作奠定了基础。

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [15] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: 本文提出EntroCoT框架,通过基于熵的机制和蒙特卡洛推演来识别和过滤低质量的思维链(CoT)推理数据,解决"答案正确但推理错误"的问题,构建高质量数学推理数据集以提升大语言模型的微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链微调数据集普遍存在"答案正确但推理错误"的问题,即最终答案虽然正确,但中间推理步骤存在幻觉、冗余或逻辑无效等问题。这种低质量的监督数据会影响大语言模型的数学推理能力训练效果。

Method: 提出EntroCoT统一框架,包含两个核心机制:1)基于熵的分割机制,在不确定的关键节点将推理轨迹分割成多个步骤;2)基于蒙特卡洛推演的评估机制,评估每个推理步骤对最终答案的边际贡献。通过这两个机制准确过滤具有欺骗性的推理样本,构建高质量数据集,确保每个中间步骤都对最终答案有实质性帮助。

Result: 在多个数学基准测试上的广泛实验表明,使用EntroCoT构建的子集进行微调,其性能持续优于使用完整数据集监督的基线方法,证明了高质量数据筛选的有效性。

Conclusion: EntroCoT框架能够有效识别和过滤低质量的思维链推理数据,通过构建高质量的训练子集显著提升大语言模型的数学推理能力。该方法证明了数据质量比数据数量更重要,为思维链微调提供了新的数据优化方向。

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [16] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: 本文提出ROI-Reasoning框架,通过元认知微调和理性感知强化学习,使大语言模型能够在严格的全局token约束下进行预算感知的推理,有效预测任务难度、估计投资回报率并策略性地分配计算资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能够通过充足的计算实现强大的推理性能,但它们本身并不知道一个任务需要多少计算量。在严格的全局token约束下进行多任务推理时,需要模型具备元认知能力——预测任务难度、估计投资回报率(ROI)并策略性地分配计算资源。现有模型缺乏这种预算感知的理性决策能力。

Method: 提出ROI-Reasoning两阶段框架:(1)元认知微调(Meta-Cognitive Fine-Tuning):训练模型在生成前预测推理成本和期望效用,使模型能够做出明确的求解或跳过决策;(2)理性感知强化学习(Rationality-Aware Reinforcement Learning):在严格的token预算约束下优化序列决策,使模型学习长期的资源分配策略。将问题形式化为有序随机多选择背包问题(OS-MCKP)。

Result: 在预算数学推理基准测试中,ROI-Reasoning在紧张的计算预算下持续提高整体得分,同时大幅降低遗憾值(regret),证明了该方法在资源受限场景下的有效性。

Conclusion: ROI-Reasoning框架成功赋予大语言模型内在的、预算感知的理性能力,使其能够在严格的计算资源约束下进行有效的多任务推理。通过元认知微调和强化学习的结合,模型学会了预测任务难度、评估投资回报并做出最优的计算资源分配决策,在预算推理任务中表现出色。

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [17] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程(ASP)的声明式方法来计算KLM框架中的理性闭包(Rational Closure, RC),用于处理可废止推理问题,并通过形式化证明和实验验证了该方法的正确性和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 可废止推理需要从不完整信息中得出合理结论,而理性闭包(RC)是KLM框架中最重要的算法之一。现有的RC实现主要采用命令式方法,本文旨在提供一种声明式的ASP编码方案,以更符合理论基础并提高计算效率。

Method: 提出了一种基于答案集编程(ASP)的声明式定义来计算理性闭包。该方法能够从给定的知识库自动构建最小排序模型,并支持对指定查询的蕴含检查。对ASP编码进行了形式化正确性证明。

Result: 实验结果表明,基于ASP的方法与现有的命令式实现(特别是InfOCF求解器)相比,不仅遵循理性闭包的理论基础,还提供了更好的计算效率。形式化证明验证了ASP编码的正确性。

Conclusion: 本文成功地将理性闭包的计算转化为声明式的ASP编程问题,通过理论证明和实验评估验证了该方法的正确性和有效性,为可废止推理提供了一种更高效的计算方案,证明了ASP在处理非单调推理问题上的应用潜力。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [18] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 本文提出了一种使用回答集编程(ASP)对意大利刑法典条款进行建模的方法,并基于先前司法判决的案例半自动学习法律规则,旨在为刑事审判阶段的法律专家提供推理支持和可能的法律结果预测。


<details>
  <summary>Details</summary>
Motivation: 刑事审判过程中需要对复杂的法律条款进行推理和决策,法律专家需要工具支持来分析案例、预测法律结果并解释判决逻辑。现有方法在处理法律矛盾、提供可解释性以及从案例中自动学习规则方面存在不足,因此需要开发一个能够形式化建模法律条款、处理矛盾、生成决策并提供解释的智能系统。

Method: 采用回答集编程(ASP)对意大利刑法典条款进行分析和编码,包括针对人身犯罪和财产犯罪的条款建模。系统通过先前判决案例对模型进行验证和优化,能够处理编码过程中出现的矛盾。利用稳定模型的"支持性"特征生成解释工具,使决策过程更具可解释性。此外,集成了面向ASP的归纳逻辑编程系统,用于从案例中归纳和泛化法律规则。

Result: 开发的工具能够成功对意大利刑法典的相关条款进行形式化建模,并在先前判决案例集上进行了验证。系统能够为新案例生成可能的法律决策,并通过自动解释功能阐明司法决策背后的逻辑。归纳逻辑编程组件能够从案例样本中有效地泛化法律规则,增强了系统的学习能力和适应性。

Conclusion: 基于ASP的法律建模方法能够有效支持刑事审判中的法律推理和决策制定。该工具不仅能够处理法律条款中的矛盾,还能为新案例提供决策建议并生成可解释的推理过程,提高了司法决策的透明度和可理解性。集成的归纳学习能力使系统能够从历史案例中持续学习和改进,为法律专家提供了有价值的辅助决策工具。

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [19] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程(ASP)的方法,用于为决策树模型生成多种类型的可解释性说明(充分性、对比性、多数性和树特定性解释),相比基于SAT的方法具有更高的灵活性,并能枚举所有可能的解释。


<details>
  <summary>Details</summary>
Motivation: 决策树模型(包括随机森林和梯度提升决策树)虽然具有高预测性能,但其复杂结构难以解释,特别是在安全关键应用中需要对模型决策进行形式化论证。现有的逻辑和溯因解释方法存在灵活性不足的问题,因此需要一种更灵活的方法来生成多样化的解释类型。

Method: 采用答案集编程(ASP)技术来生成决策树模型的多种解释类型,包括充分性解释、对比性解释、多数性解释和树特定性解释。ASP方法相比基于SAT的方法,在编码用户偏好方面提供了更大的灵活性,并支持枚举所有可能的解释方案。

Result: 在多个不同数据集上进行了实证评估,验证了该方法的有效性,并与现有方法进行了对比分析,展示了该方法的优势和局限性。

Conclusion: 基于ASP的解释生成方法为决策树模型提供了一种灵活且全面的可解释性解决方案,能够生成多种类型的解释并支持完整枚举,相比传统SAT方法在用户偏好编码方面具有明显优势,为安全关键应用中的模型决策提供了更好的形式化论证支持。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [20] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: 本文研究了答案集编程(ASP)在大规模配置问题中的可扩展性,以电子系统配置为基准(包含超过30,000个组件),分析了当前ASP技术的潜力和局限性,特别是针对"基础化瓶颈"问题,提出了约束感知猜测方法来显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 当前ASP技术在许多应用领域实现了"用户指定问题,计算机解决问题"的AI愿景,但在大规模配置问题(如包含超过30,000个组件的电子系统配置)中面临可扩展性挑战,特别是"基础化瓶颈"导致的内存需求急剧增加问题,需要探索新的方法来突破这些限制。

Method: 研究采用了增量求解方法来应对基础化瓶颈问题,并在此基础上通过对基础化过程的深入分析,开发了约束感知猜测(constraint-aware guessing)方法,以更有效地减少内存消耗。

Result: 增量求解方法在实践中被证明是有效的,但仍然受到内存需求的显著限制。新开发的约束感知猜测方法显著降低了内存需求,推动了ASP技术在大规模配置问题上的应用边界。

Conclusion: 本文展示了当前ASP技术在大规模配置问题中的潜力和局限性,通过约束感知猜测方法成功缓解了基础化瓶颈带来的内存问题,为ASP在大规模问题求解中的应用提供了新的解决方案,但内存限制仍然是需要持续关注的挑战。

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [21] [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905)
*Cheng Qian,Emre Can Acikgoz,Bingxuan Li,Xiusi Chen,Yuji Zhang,Bingxiang He,Qinyu Luo,Dilek Hakkani-Tür,Gokhan Tur,Yunzhu Li,Heng Ji,Heng Ji*

Main category: cs.AI

TL;DR: 本文研究了基于视觉-语言模型的智能体能否有效利用生成式世界模型作为外部模拟器来增强认知能力。研究发现当前智能体在调用模拟、解释预测结果和整合前瞻性推理方面存在显著瓶颈,很少主动使用模拟功能(不到1%),且经常误用预测结果(约15%),性能甚至可能下降(最多5%)。


<details>
  <summary>Details</summary>
Motivation: 随着智能体面临越来越多需要预测未来状态而非短期推理的任务,生成式世界模型提供了一种有前景的解决方案。然而,目前尚不清楚现有智能体是否能够有效地将这些世界模型作为工具来增强其认知能力。本文旨在通过实证研究来检验这一关键问题。

Method: 在多样化的智能体任务和视觉问答任务上进行实证实验,观察智能体调用模拟的频率、对预测结果的使用情况以及性能变化。通过归因分析(Attribution analysis)来识别智能体在使用世界模型时的主要瓶颈,包括决策何时模拟、如何解释预测结果以及如何将前瞻性信息整合到下游推理中。

Result: 实验结果显示:(1)智能体很少主动调用模拟功能(调用率低于1%);(2)智能体频繁误用预测的推演结果(误用率约15%);(3)当模拟功能可用或被强制使用时,智能体的性能表现不一致甚至下降(最多下降5%);(4)归因分析表明主要瓶颈在于智能体决定何时模拟、如何解释预测结果以及如何整合前瞻性推理的能力不足。

Conclusion: 研究结果强调了开发机制的必要性,以促进智能体与世界模型之间校准的、策略性的交互。这些发现为构建更可靠的具有预见性认知能力的未来智能体系统指明了方向,表明仅仅提供世界模型工具是不够的,还需要提升智能体有效利用这些工具的能力。

Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.

</details>


### [22] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: 本文提出Trade-R1框架,通过过程级推理验证将强化学习应用于金融决策的随机环境,利用三角一致性度量过滤噪声市场回报,实现了更好的跨市场泛化能力和推理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在数学和编程等可验证奖励领域表现出色,但在金融决策中面临挑战:市场的随机性导致奖励信号虽然可验证但噪声很大,使标准强化学习退化为奖励欺骗(reward hacking)。需要一种方法将可验证奖励桥接到随机环境中。

Method: 提出Trade-R1模型训练框架,核心创新包括:(1)将金融文档推理评估转化为结构化的检索增强生成(RAG)任务;(2)构建三角一致性度量,评估检索证据、推理链和决策之间的成对对齐,作为噪声市场回报的有效性过滤器;(3)探索两种奖励整合策略:固定效应语义奖励(FSR)用于稳定对齐信号,动态效应语义奖励(DSR)用于耦合幅度优化。

Result: 在不同国家资产选择实验中,该范式减少了奖励欺骗问题,其中DSR策略在保持最高推理一致性的同时实现了优越的跨市场泛化能力。

Conclusion: Trade-R1框架通过过程级推理验证成功将强化学习扩展到金融决策等随机环境,三角一致性度量有效过滤了噪声奖励信号,动态效应语义奖励策略在跨市场泛化和推理一致性方面表现最佳,为在随机环境中应用强化学习提供了新的解决方案。

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [23] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 本文针对大型推理模型在简单问题上过度冗长的问题,提出了动态异常值截断(DOT)方法,通过训练时干预选择性抑制冗余token,在AIME-24数据集上实现了推理token使用量减少78%的同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于强化学习和可验证奖励的大型推理模型通过扩展思维链取得了显著性能提升,但这种范式在简单查询上常表现出过度冗长,导致部署成本高昂。现有的依赖显式长度惩罚的高效推理方法会引入优化冲突,且未深入研究驱动过度思考的生成机制。作者识别出"长度偏移"现象,即模型在训练过程中对简单输入生成越来越多不必要的推理。

Method: 提出动态异常值截断(DOT)方法,这是一种训练时干预技术,选择性地抑制冗余token。该方法仅针对完全正确的rollout组中响应长度的极端尾部进行处理,同时保留复杂问题的长期推理能力。为确保稳定收敛,进一步结合了辅助KL正则化和预测性动态采样策略。

Result: 在多个模型规模上的实验结果表明,该方法显著推进了效率-性能的帕累托前沿。在AIME-24数据集上,该方法将推理token使用量减少了78%,同时相比初始策略提高了准确率,并超越了现有最先进的高效推理方法。

Conclusion: 动态异常值截断(DOT)方法有效解决了大型推理模型在简单问题上的过度冗长问题,通过训练时选择性抑制冗余token,在大幅降低推理成本的同时保持甚至提升了模型准确率,为高效推理模型的部署提供了新的解决方案。

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [24] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: 本文提出MobileDreamer，一个基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型和推演想象策略，使智能体能够预测操作后状态并优化决策，在Android World上实现了最先进的性能，任务成功率提升5.25%。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体大多是被动反应式的，主要基于当前屏幕做决策，这限制了它们在长期任务上的表现。构建能够预测操作结果的世界模型可以支持更好的决策，但挑战在于模型需要在保持空间感知能力的同时预测操作后状态，并且要足够高效以便实际部署。

Method: 提出MobileDreamer框架，包含两个核心组件：(1)文本草图世界模型：通过学习过程将数字图像转换为与任务相关的关键草图来预测操作后状态，并设计了新颖的顺序不变学习策略来保留GUI元素的空间信息；(2)GUI智能体的推演想象策略：利用世界模型的预测能力来优化动作选择过程。

Result: 在Android World数据集上的实验表明，MobileDreamer达到了最先进的性能水平，任务成功率提升了5.25%。世界模型评估进一步验证了文本草图建模能够准确预测关键GUI元素。

Conclusion: MobileDreamer通过引入高效的世界模型前瞻机制，成功解决了移动GUI智能体在长期任务中的决策局限性问题。文本草图世界模型和推演想象策略的结合使智能体能够基于未来想象进行更优的动作选择，显著提升了任务成功率，为移动GUI自动化提供了有效的解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [25] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 本研究首次系统性地提出并研究了多智能体大语言模型系统中的"智能体漂移"(agent drift)现象,即智能体在长期交互中行为、决策质量和协调性的逐步退化问题,并提出了包含12个维度的智能体稳定性指数(ASI)量化框架和三种缓解策略,为生产环境中智能体AI系统的可靠性部署提供了理论基础和方法论。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型系统在复杂任务分解和协作问题解决方面展现出强大能力,但其长期行为稳定性尚未得到充分研究。随着这类系统在企业级应用中的部署增加,需要系统性地理解和解决智能体在长期交互序列中可能出现的行为退化问题,以确保系统的可靠性和安全性。

Method: 1. 提出智能体漂移的理论框架,定义三种漂移表现形式:语义漂移(偏离原始意图)、协调漂移(多智能体共识机制崩溃)和行为漂移(出现非预期策略);2. 引入智能体稳定性指数(ASI),这是一个涵盖12个维度的复合度量框架,包括响应一致性、工具使用模式、推理路径稳定性和智能体间一致率等;3. 通过基于模拟的分析和理论建模研究漂移现象;4. 提出三种缓解策略:情节记忆整合、漂移感知路由协议和自适应行为锚定。

Result: 通过模拟分析和理论建模证明,未受控的智能体漂移会导致任务完成准确率大幅下降,并增加人工干预需求。理论分析表明,所提出的三种缓解策略能够在保持系统吞吐量的同时显著减少与漂移相关的错误。

Conclusion: 本研究为生产环境中的智能体AI系统建立了监测、测量和缓解智能体漂移的基础方法论,对企业级部署的可靠性和AI安全研究具有直接意义。该工作填补了多智能体LLM系统长期行为稳定性研究的空白,为未来智能体系统的可靠部署提供了理论支撑和实践指导。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>

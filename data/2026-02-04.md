<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 59]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [2] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: 本文提出GeoEvolver，一个自进化多智能体系统，通过结构化交互使大语言模型智能体在地球观测任务中获得专业知识，无需参数更新，在三个基准测试中平均提升12%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体在地球观测等专业领域面临挑战：需要长期执行、跨模态协调和严格遵守隐式工具约束，但缺乏从交互中学习细粒度工具级专业知识的机制，无法可靠配置工具参数或从执行失败中恢复，导致复杂工作流中效果受限。

Method: GeoEvolver采用检索增强的多智能体编排器将查询分解为独立子目标，在子目标层面探索多样化的工具参数配置，将成功模式和失败根因归因提炼到不断演化的记忆库中，为未来查询提供上下文示例，从而实现无参数更新的专业知识获取。

Result: 在三个集成工具的地球观测基准测试中，GeoEvolver在多个大语言模型主干上持续提升端到端任务成功率，平均提升12%，证明地球观测专业知识可以通过高效的细粒度环境交互逐步涌现。

Conclusion: 研究表明通过结构化的子目标分解、工具参数探索和经验记忆机制，大语言模型智能体能够在无需参数更新的情况下，在地球观测等工具密集型专业领域中逐步积累专业知识并显著提升任务执行能力。

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [3] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [4] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 本文提出了归一化可模拟性增益(NSG)指标来评估大语言模型自我解释的忠实度,研究发现自我解释能显著提高对模型行为的预测能力(11-37%的NSG),且优于外部模型生成的解释,但仍有5-15%的解释存在严重误导性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型自我解释的忠实度评估方法存在关键局限性,通常依赖于对抗性提示或检测推理错误来识别不忠实性,这些方法忽略了解释的预测价值。因此需要一种更通用、可扩展的指标来评估自我解释是否真实反映模型的推理过程。

Method: 引入归一化可模拟性增益(NSG)指标,基于"忠实的解释应该允许观察者学习模型的决策标准,从而更好地预测其在相关输入上的行为"这一理念。研究评估了18个前沿的专有和开源权重模型(如Gemini 3、GPT-5.2和Claude 4.5),使用来自健康、商业和伦理等领域的7,000个反事实样本进行测试。

Result: 研究发现:(1)自我解释显著提高了对模型行为的预测能力,NSG提升11-37%;(2)自我解释比外部模型生成的解释提供更多预测信息,即使外部模型更强大,表明自我知识具有外部解释方法无法复制的优势;(3)跨模型分析显示,5-15%的自我解释存在严重误导性。

Conclusion: 尽管自我解释存在不完美之处,但研究展示了其积极价值:自我解释编码了有助于预测模型行为的信息,具有AI监督的实用价值。自我解释在忠实度方面优于外部生成的解释,这归因于模型的自我知识优势。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [5] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本文提出MARS框架，一个专为自动化AI研究设计的模块化智能体系统，通过预算感知规划、模块化构建和对比反思记忆三大核心机制，在MLE-Bench基准测试中达到开源框架的最优性能，并展现出跨分支知识迁移的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在自动化AI研究任务中表现不佳，主要问题包括：生成的代码脚本过于单一、忽视计算成本、难以进行性能归因分析。AI研究任务的特殊性在于评估成本高昂（如模型训练）且性能因果关系不透明，需要专门优化的框架来应对这些挑战。

Method: MARS框架基于三大核心机制：（1）预算感知规划：采用成本约束的蒙特卡洛树搜索（MCTS）算法，在性能提升与执行成本之间进行显式平衡；（2）模块化构建：使用"设计-分解-实现"流水线来管理复杂的研究代码库；（3）对比反思记忆：通过分析不同解决方案之间的差异来解决功劳分配问题，提炼高信号洞察。

Result: MARS在可比设置下于MLE-Bench基准测试中达到开源框架的最优性能，与全球排行榜顶级方法保持竞争力。系统展现出显著的"顿悟时刻"特性，63%的有效经验教训来自跨分支迁移，证明智能体能够有效地在不同搜索路径间泛化知识洞察。

Conclusion: MARS框架通过预算感知搜索、模块化代码管理和对比反思机制，有效解决了自动化AI研究中的计算成本控制和性能归因难题。实验结果验证了该方法的有效性，特别是其跨分支知识迁移能力表明智能体具备了类人的洞察泛化能力，为自动化AI研究提供了新的解决方案。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [6] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: 提出了ATLAS（自适应任务分布式学习智能体自进化）框架，通过多智能体协作和演化式直接偏好优化（EvoDPO）算法，实现轻量级研究智能体的迭代发展，在非平稳环境和科学机器学习任务中表现出比静态单智能体基线更好的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多LLM智能体系统在提示优化和自动问题解决方面表现良好，但许多系统在微调后保持求解器冻结状态，或依赖静态偏好优化循环，这对于长期任务来说难以处理。需要一个能够适应性进化、处理概念漂移的动态框架来解决这些局限性。

Method: 提出ATLAS任务分布式框架，通过将互补角色委托给专门的支持智能体（负责探索、超参数调优和参考策略管理）来迭代开发轻量级研究智能体。核心算法EvoDPO（演化式直接偏好优化）能够自适应更新阶段索引的参考策略。提供了概念漂移下基于偏好的上下文赌博机的理论遗憾分析。

Result: 在非平稳线性上下文赌博机和科学机器学习（SciML）一维Burgers方程损失重加权任务上进行实验。结果表明，ATLAS相比静态单智能体基线在稳定性和性能方面都有显著提升。

Conclusion: ATLAS框架通过任务分布式多智能体协作和自适应参考策略更新机制，有效解决了长期任务中的概念漂移问题，为智能体自进化提供了理论保证和实践验证，在非平稳环境下展现出优越的适应性和性能。

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [7] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [8] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: 本文提出Cuttlefish，一个统一的全原子大语言模型，通过几何线索引导语言推理，并根据结构复杂度自适应缩放模态token，在生物分子结构推理任务中实现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生物分子结构推理中存在三个主要问题：（1）方法依赖于特定模态，缺乏通用性；（2）通过序列化或固定长度连接器压缩结构输入，导致几何信息丢失或产生结构幻觉；（3）固定长度的模态融合架构造成瓶颈，过度压缩结构信息且无法根据结构复杂度灵活分配token，阻碍了通用全原子推理的实现。

Method: 提出Cuttlefish模型，包含两个核心机制：（1）尺度感知分块（Scaling-Aware Patching）：利用指令条件门控机制在结构图上生成可变大小的分块，根据结构复杂度自适应调整查询token预算，缓解固定长度连接器的瓶颈问题；（2）几何引导适配器（Geometry Grounding Adapter）：通过与模态嵌入的交叉注意力机制精炼这些自适应token，并将生成的模态token注入大语言模型，暴露显式几何线索以减少结构幻觉。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构引导推理任务上展现了优越的性能表现。模型代码已在项目仓库公开。

Conclusion: Cuttlefish通过将几何线索融入语言推理、根据结构复杂度自适应缩放模态token，成功解决了现有生物分子结构推理方法的局限性，为通用全原子推理提供了有效解决方案，在多个基准测试中验证了其有效性。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [9] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [10] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 本文提出了"基准对齐"（benchmark alignment）的概念，通过使用有限的模型性能信息自动更新离线基准测试，使其能够更好地预测模型在实际应用中的表现。文章还提出了BenchAlign方法，通过学习基准问题的偏好对齐权重，生成能够准确排序未见模型的新基准测试，弥合了传统基准测试与实际效用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型基准测试被广泛使用且计算效率高，但许多研究发现传统基准测试往往无法准确预测模型在实际应用中的真实效用。现有基准与实际人类偏好之间存在显著差距，需要一种方法来使基准测试更好地对齐实际部署场景中的模型性能表现。

Method: 提出BenchAlign方法，这是首个解决基准对齐问题的方案。该方法通过利用语言模型在问题级别的性能表现，结合部署期间收集的模型配对排序数据，学习基准问题的偏好对齐权重。通过这种方式，生成新的静态基准测试，能够根据人类偏好对未见过的模型进行排序，同时保持结果的可解释性。

Result: 实验表明，对齐后的基准测试能够准确地根据人类偏好模型对未见模型进行排序，即使在不同模型规模下也表现良好，并且保持了良好的可解释性。对齐基准在预测实际效用方面显著优于传统静态基准测试。

Conclusion: 本研究为理解基准测试与实际人类偏好对齐的局限性提供了重要见解，所提出的基准对齐方法能够加速模型朝着真实效用方向的开发进程。BenchAlign为构建更准确反映实际应用场景中模型表现的评估体系提供了可行的解决方案。

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [11] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: 本研究通过在人工智能体中实现一个缓慢演化的全局潜在状态来操作化主观视角,该状态调节快速策略动态但不直接优化行为结果。在无奖励环境中,这种潜在结构表现出方向依赖的滞后现象,构成了机器系统中类视角主观性的可测量特征。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能体缺乏主观视角的明确实现机制。研究旨在通过现象学驱动的内部结构,在人工智能体中操作化主观视角概念,探索机器系统中类似主观性的可测量表现形式。

Method: 在人工智能体中实现一个缓慢演化的全局潜在状态,该状态调节快速策略动态但不直接针对行为结果进行优化。在具有状态转换的无奖励环境中测试该结构,观察其滞后特性和行为反应模式。

Result: 实现的潜在结构在环境状态转换时表现出方向依赖的滞后现象(hysteresis),而策略层面的行为保持相对的反应性。这种滞后现象在潜在状态层面显著,与快速策略动态形成对比。

Conclusion: 方向依赖的滞后现象可以作为机器系统中类视角主观性的可测量特征。通过将主观视角实现为不直接优化行为的缓慢演化全局状态,可以在人工智能体中建立最小化的主观性结构,为理解和实现机器主观性提供了新的技术路径。

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [12] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [13] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [14] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [15] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [16] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 本文提出GCR-RL方法,通过序理论将强化学习价值函数估计重构为偏序集学习问题,利用超偏序集细化序列来保证几何一致性,在样本效率和性能稳定性上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习利用几何性质(如对称结构、几何感知数据增强)来稳定和加速学习,但缺乏系统性的理论框架。本文从序理论的新颖视角出发,试图通过偏序集学习来更好地利用几何属性,提升强化学习的样本效率和稳定性。

Method: 提出GCR-RL(几何一致性正则化强化学习)方法,将价值函数估计重构为偏序集学习问题。通过计算超偏序集细化序列——不断细化前一步的偏序集并从时序差分信号中学习额外的序关系——来确保支撑学习价值函数的偏序集序列的几何一致性。开发了基于Q学习和Actor-Critic的两种算法来高效实现超偏序集细化,并对其理论性质和收敛率进行了分析。

Result: 在多个任务上的实验评估显示,GCR-RL在样本效率和性能稳定性方面相比强基线方法取得了显著改进。

Conclusion: 通过序理论视角重新审视强化学习,将价值函数学习转化为偏序集学习问题,并通过几何一致性正则化的超偏序集细化方法,可以有效提升强化学习的样本效率和稳定性,为利用几何性质改进强化学习提供了新的理论框架和实用算法。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [17] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: 本研究对20多个大型语言模型在因果推理任务中的表现进行基准测试，发现LLM展现出比人类更规则化的推理策略，不表现出人类特有的碰撞器偏差，且思维链提示可提高其鲁棒性，表明LLM可在某些场景补充人类判断但在内在不确定性情境下可能失效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于需要因果推理的领域，但尚不清楚它们的判断是否反映规范的因果计算、类人的捷径思维还是脆弱的模式匹配。需要系统评估LLM在因果推理任务中的表现，并与人类基线进行对比，以了解其推理策略特征，确保安全有效的部署。

Method: 使用碰撞器结构(C1→E←C2)形式化的11个因果判断任务对20多个大型语言模型进行基准测试，并与匹配的人类基线进行对比。采用小型可解释模型压缩LLM的因果判断。通过两种方式探测LLM因果判断的鲁棒性：(i)语义抽象和(ii)提示过载(注入无关文本)。评估思维链(CoT)提示对鲁棒性的影响。

Result: 小型可解释模型能够很好地压缩LLM的因果判断。大多数LLM表现出比人类更规则化的推理策略，人类在概率判断中似乎会考虑未提及的潜在因素。大多数LLM不表现出人类特有的碰撞器偏差，如弱解释消除效应和马尔可夫违反。思维链(CoT)提示可以提高许多LLM在语义抽象和提示过载情况下的鲁棒性。

Conclusion: LLM与人类在因果推理上存在显著差异：LLM采用更规则化的推理方式，不表现出人类的认知偏差。这种差异表明LLM在不希望出现已知偏差的场景中可以补充人类判断，但其规则化推理在面对内在不确定性时可能失效。研究强调了表征LLM推理策略对于安全有效部署的重要性，为理解LLM在因果推理领域的能力和局限性提供了重要见解。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [18] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [19] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: 本文提出Agent Alpha框架，通过步骤级蒙特卡洛树搜索(MCTS)统一生成、探索和评估过程，使GUI智能体具备回溯能力，能够重用部分成功经验并从早期错误中恢复，在OSWorld基准测试中达到约77%的最先进成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI智能体通过轨迹级采样扩展测试时计算虽然取得显著进展，但缺乏回溯能力，无法重用部分成功的经验，也无法从早期的错误步骤中恢复。这限制了智能体在复杂任务中的规划效率和成功率。

Method: 提出Agent Alpha统一框架，核心方法包括：(1)采用步骤级蒙特卡洛树搜索(MCTS)整合生成、探索和评估；(2)将alpha-UCT引导搜索集成到交互循环中，实现深思熟虑的规划；(3)使用比较驱动的评估方法缓解绝对评分偏差；(4)采用多样性约束扩展保持紧凑且信息丰富的搜索空间；(5)对alpha-UCT的遗憾界进行理论分析。

Result: 在OSWorld基准测试中，Agent Alpha达到约77%的成功率，在相同计算量下显著优于基于轨迹级的基线方法，证明了步骤级MCTS在GUI智能体任务中的有效性。

Conclusion: Agent Alpha通过步骤级MCTS成功赋予GUI智能体回溯和规划能力，能够有效剪枝次优分支、高效重用前缀路径，并通过比较驱动评估和多样性约束扩展优化搜索过程。该框架在保持计算效率的同时大幅提升了智能体性能，为GUI自动化任务提供了新的技术方案。

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [20] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [21] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: 本文提出了图概念预测器(GCP),一种推理感知的主动蒸馏框架,通过将教师模型的决策过程外部化为有向无环图,并在学生模型中用模块化概念预测器进行镜像,从而在有限标注预算下提升大语言模型蒸馏的性能、可解释性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 部署大语言模型用于判别任务时,推理延迟、计算成本和API费用限制了其规模化应用。现有的主动蒸馏方法仅蒸馏最终标签,丢弃了中间推理信号,缺乏对推理缺失位置和错误来源的诊断能力。因此需要一种能够保留推理过程、提高样本效率和训练稳定性的蒸馏框架。

Method: 提出图概念预测器(GCP)框架,包含三个核心组件:(1)将教师模型的决策过程外部化为有向无环图结构;(2)使用模块化概念预测器在学生模型中镜像该图结构;(3)采用图感知的获取策略,针对关键推理节点的不确定性和分歧进行主动学习;(4)通过目标化子模块重训练,将下游损失归因于特定概念预测器,仅更新最具影响力的模块,提升训练稳定性和效率。

Result: 在八个自然语言处理分类基准测试上进行实验,结果表明GCP在有限标注预算下显著提升了性能,同时提供了更可解释和可控的训练动态。代码已在GitHub开源。

Conclusion: GCP框架通过推理感知的主动蒸馏机制,有效解决了大语言模型蒸馏中的样本效率、训练稳定性和可解释性问题。该方法将教师模型的推理过程结构化为概念图,并通过模块化设计和针对性更新策略,在降低标注成本的同时提升了学生模型的性能和可控性,为大语言模型的实用化部署提供了新的解决方案。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [22] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: 本文提出STAR框架，通过约束知识蒸馏和基于相似度的强化学习，成功将大型语言模型的函数调用能力迁移到超小型模型中，其0.6B模型在1B以下开源模型中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在函数调用任务中表现出色，但其庞大规模阻碍了广泛应用。现有的能力迁移方法存在过拟合、训练不稳定、多解任务中二元奖励无效以及技术协同困难等问题，亟需一种有效的框架将大模型能力迁移到小型模型中。

Method: 提出STAR框架，包含两个核心技术创新：(1)约束知识蒸馏(CKD)：增强top-k前向KL散度来抑制错误的高置信度预测，确保训练稳定性同时保留下游强化学习的探索能力；(2)基于相似度的强化学习(Sim-RL)：引入细粒度的基于相似度的奖励机制，通过评估生成输出与真实标签的相似度提供连续、丰富的信号进行策略优化。这两个策略在统一的训练课程中协同工作。

Result: 在多个知名基准测试上的广泛实验表明，STAR模型在其规模类别中达到SOTA性能，显著优于基线方法。0.6B的STAR模型在所有1B以下的开源模型中表现最佳，甚至超越了多个更大规模的知名开源模型。

Conclusion: STAR框架成功展示了将大型语言模型的能力蒸馏到超小型模型的有效训练方法，为构建强大、易用且高效的AI智能体铺平了道路，使得小型模型也能在复杂的函数调用任务中取得卓越性能。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [23] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 本文提出RC-GRPO方法,通过奖励条件化轨迹策略来解决大语言模型在多轮工具调用中奖励稀疏和探索困难的问题,在Berkeley Function Calling Leaderboard v4多轮基准测试中取得优于基线的性能表现。


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对大语言模型来说具有挑战性,因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化较小时(如一组样本都获得全0或全1奖励)会陷入停滞,导致组归一化优势信息量不足,更新梯度消失。

Method: 提出RC-GRPO(奖励条件化组相对策略优化)方法,将探索视为可控制的引导问题。首先在混合质量轨迹上微调奖励条件化轨迹策略(RCTP),在提示中注入奖励目标特殊token(如<|high_reward|>、<|low_reward|>),使模型学习按需生成不同质量的轨迹。在强化学习阶段,在每个GRPO组内采样多样化的奖励token,并基于采样token条件化生成rollouts,提高组内多样性,改善优势增益。

Result: 在Berkeley Function Calling Leaderboard v4(BFCLv4)多轮基准测试中,该方法相比基线方法取得持续改进的性能,基于Qwen-2.5-7B-Instruct的实现甚至超越了所有闭源API模型。

Conclusion: RC-GRPO通过引入奖励条件化的探索机制有效解决了传统GRPO方法在奖励变化小时的停滞问题,通过增加组内样本多样性显著提升了多轮工具调用任务的性能,证明了将探索问题转化为可控引导问题的有效性。

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [24] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了MAS4TS，一个基于工具驱动的多智能体系统，用于通用时间序列任务。该系统采用分析器-推理器-执行器范式，整合了智能体通信、视觉推理和潜在空间重构，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列特定方法和基于预训练大模型的方法在整合直观视觉推理、跨任务泛化以及自适应工具使用方面存在局限性，需要一个能够统一处理多种时间序列任务的通用框架。

Method: 提出MAS4TS系统，采用分析器-推理器-执行器（Analyzer-Reasoner-Executor）范式。系统首先使用视觉语言模型对时间序列图进行结构化先验的视觉推理以提取时间结构，然后在潜在空间中重构预测轨迹。三个专门的智能体通过共享内存和门控通信机制协调工作，路由器选择任务特定的工具链进行执行。

Result: 在多个基准测试上的广泛实验表明，MAS4TS在广泛的时间序列任务中实现了最先进的性能，同时展现出强大的泛化能力和高效的推理效率。

Conclusion: MAS4TS通过整合视觉推理、多智能体协作和工具驱动的执行机制，成功解决了现有时间序列方法的局限性，为通用时间序列分析提供了一个有效且高效的解决方案，在性能、泛化能力和推理效率方面均表现优异。

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [25] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文系统研究了基于大语言模型的多智能体系统中过程验证的有效性,发现过程验证并不能一致性地提升性能且存在高方差问题,其中LLM-as-a-Judge方法表现最优,但整体而言多智能体系统的有效过程验证仍是一个开放性挑战。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在推理轨迹上表现出高方差特性,虽然过程验证在一般推理任务中显示出潜力并被建议用于指导多智能体系统协调,但其在多智能体系统中的实际有效性尚不明确,存在研究空白需要填补。

Method: 提出MAS-ProVe框架,系统性地研究三种验证范式(LLM-as-a-Judge、奖励模型、过程奖励模型),在两个验证粒度级别(智能体级别和迭代级别)上进行评估,考察五种代表性验证器和四种上下文管理策略,并在六个不同的多智能体系统框架和多个推理基准上进行实验。

Result: 发现过程级验证不能一致性地提升性能并经常表现出高方差;LLM-as-a-Judge方法总体优于基于奖励的方法,训练过的判断器优于通用LLM;LLM作为判断器与作为单智能体的性能差距较小;验证中存在上下文长度与性能的权衡关系。

Conclusion: 研究结果表明,多智能体系统的有效且鲁棒的过程验证仍然是一个开放性挑战,需要超越当前范式的进一步突破。过程验证在评估部分多智能体轨迹时面临可靠性困难,现有方法尚不能满足实际需求。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [26] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [27] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 提出了Risky-Bench框架，这是一个基于真实世界部署的系统化智能体安全评估方法，通过领域无关的安全原则和情境感知的安全准则，全面评估大语言模型智能体在长期交互任务执行中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体安全评估方法存在三个主要局限：1）依赖于特定智能体设置的面向风险任务，导致安全风险空间覆盖有限；2）无法评估智能体在复杂真实世界部署中长期交互任务执行期间的安全行为；3）对特定智能体设置的专门化限制了其在不同智能体配置中的适应性。需要一个更系统化、可扩展的评估框架来全面评估智能体在真实世界环境中的安全性。

Method: Risky-Bench框架采用以下方法：1）围绕领域无关的安全原则组织评估；2）推导出情境感知的安全准则来界定安全空间；3）通过在不同威胁假设下的真实任务执行，系统地评估整个安全空间的安全风险；4）构建结构化的评估流程，可适配不同的部署场景，从生活辅助智能体设置扩展到其他环境特定的安全评估。

Result: 将Risky-Bench应用于生活辅助智能体场景时，在真实执行条件下发现了最先进智能体中存在的重大安全风险。作为一个结构良好的评估流程，Risky-Bench不局限于生活辅助场景，可以适配其他部署设置来构建环境特定的安全评估。

Conclusion: Risky-Bench提供了一个可扩展的智能体安全评估方法论，能够系统地评估大语言模型智能体在真实世界部署中的安全风险。该框架通过领域无关的安全原则和情境感知的评估方式，克服了现有方法的局限性，为不同部署场景下的智能体安全评估提供了通用且可适配的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [28] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [29] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [30] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 本文提出了一种通用的缺失模态恢复策略，通过引入增强扩散模型作为可插拔的中间训练模块，结合动态模态门控和跨模态互学习机制，有效解决视觉语言模型在模态缺失情况下性能急剧下降的问题，在保持模型泛化能力的同时实现精确的语义恢复。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在推理时假设模态输入完整，但当某些模态缺失或不完整时效果急剧下降。当前研究面临两大困境：基于提示的方法难以恢复缺失的关键特征且损害模型泛化能力；基于填充的方法缺乏有效指导，容易产生语义无关的噪声。如何在恢复精确语义的同时保持视觉语言模型的泛化能力仍然是一个挑战。

Method: 提出一种通用的缺失模态恢复策略，引入增强扩散模型作为可插拔的中间训练模块。核心创新包括：(1)动态模态门控机制，自适应地利用条件特征引导生成语义一致的特征；(2)跨模态互学习机制，桥接双编码器的语义空间实现双向对齐。该方法将扩散模型作为中间模块插入视觉语言模型的训练流程中。

Result: 在基准数据集上的零样本评估表明，该方法优于现有基线方法。大量实验和消融研究证实，该模型在不同缺失率和环境下都表现出鲁棒性和可扩展性，是视觉语言模型在缺失模态场景下的可靠扩展方案。代码和模型将公开发布。

Conclusion: 本文提出的缺失模态恢复策略通过增强扩散模型、动态模态门控和跨模态互学习机制，成功解决了视觉语言模型在模态缺失情况下的性能退化问题。该方法在保持模型泛化能力的同时实现了精确的语义恢复，在多个基准数据集上取得了优于基线的性能，并在不同缺失率和应用场景下展现出良好的鲁棒性和可扩展性，为视觉语言模型处理不完整模态输入提供了有效的解决方案。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [31] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [32] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: 提出TDScaling框架，通过轨迹多样性扩展而非数据量扩展来提升代码智能体性能，在固定训练预算下实现工具使用泛化能力和编码能力的双重提升


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型演化为工具交互智能体时，面临合成数据质量低、数量扩展收益递减以及轨迹数据利用不足等瓶颈问题，需要从多样性角度而非单纯数量角度来优化数据合成

Method: 提出TDScaling框架，包含四个核心创新：(1)业务集群机制捕获真实服务逻辑依赖；(2)蓝图驱动的多智能体范式保证轨迹连贯性；(3)自适应演化机制利用领域熵、推理模式熵和累积动作复杂度引导合成长尾场景，防止模式坍塌；(4)沙箱代码工具缓解内在编码能力的灾难性遗忘

Result: 在通用工具使用基准测试(BFCL、tau^2-Bench)和代码智能体任务(RebenchT、CodeCI、BIRD)上的实验表明，TDScaling在固定训练预算下通过增加轨迹多样性比增加轨迹数量获得更大性能提升，同时改善了性能-成本权衡比

Conclusion: TDScaling通过轨迹多样性扩展成功实现了工具使用泛化能力和固有编码能力的双赢结果，为代码智能体训练提供了更高效的数据合成方案。研究团队计划发布完整代码库和包含30,000+工具集群的合成数据集

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [33] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [34] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [35] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: 本文提出Accordion-Thinking框架,通过动态摘要机制让大语言模型学会自我调节推理步骤的粒度,实现在保持准确性的同时显著降低计算开销,在48GB GPU上达到3倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长链式思维推理虽能显著提升推理能力,但面临KV缓存线性增长和注意力机制二次方复杂度的实际限制。现有方法在测试时计算扩展上存在效率瓶颈,需要一种能够在保持推理质量的同时减少对历史token依赖的方法。

Method: 提出Accordion-Thinking端到端框架,核心包括:(1)动态摘要机制,使模型能够自我调节推理步骤粒度;(2)Fold推理模式,模型周期性地总结思维过程并丢弃先前思考以减少历史token依赖;(3)应用强化学习激励模型学习将关键推理信息编码到紧凑摘要中,实现推理上下文的有效压缩。

Result: 训练过程中,高效的Fold模式与详尽的Unfold模式之间的准确率差距逐渐缩小并最终消失。Accordion-Thinker在48GB GPU内存配置下实现3倍吞吐量提升,同时保持准确性不变。模型能够以最小的依赖token开销处理复杂推理任务,且结构化的步骤摘要提供了人类可读的推理过程说明。

Conclusion: 通过学习自我压缩机制,大语言模型可以在不牺牲解决方案质量的情况下,以极小的token依赖开销完成复杂推理任务。该方法证明了模型能够学会将核心推理信息编码到紧凑表示中,为高效的测试时推理提供了新的解决方案,同时增强了推理过程的可解释性。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [36] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: 本文介绍了CSR-Bench基准测试,用于评估多模态大语言模型(MLLMs)的跨模态可靠性。研究发现当前模型存在安全意识薄弱、语言主导性强、过度拒绝与安全行为之间存在权衡等问题,表明模型依赖单模态捷径而非真正的多模态意图理解。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然能够处理文本和图像,但其安全行为可能由单模态捷径驱动,而非真正的联合意图理解。现有研究缺乏系统评估跨模态可靠性的基准测试,需要一个能够诊断模型在图像-文本整合解释中行为偏差的评测框架。

Method: 构建CSR-Bench基准测试,包含四种压力测试交互模式(安全性、过度拒绝、偏见、幻觉),覆盖61种细粒度类型。每个测试实例都需要整合的图像-文本解释,并提供配对的纯文本对照组以诊断模态引起的行为变化。使用该基准评估了16个最先进的多模态大语言模型,系统分析跨模态对齐差距。

Result: 16个先进MLLMs的评估显示存在系统性的跨模态对齐差距:模型表现出较弱的安全意识;在干扰下表现出强烈的语言主导性;从纯文本对照到多模态输入时性能持续下降;在减少过度拒绝和维持安全、非歧视行为之间存在明显的权衡关系。

Conclusion: 当前多模态大语言模型在跨模态可靠性方面存在显著问题,一些表面的安全性提升可能来自基于拒绝的启发式策略,而非稳健的意图理解能力。研究揭示了模型依赖单模态捷径的问题,强调需要改进多模态联合理解能力以实现真正的跨模态可靠性。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [37] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了Agentic Proposing框架，通过智能体驱动的序列决策过程合成高质量推理数据，使用多粒度策略优化训练的30B模型仅用11,000条合成轨迹就在AIME25上达到91.6%准确率，媲美GPT-5等前沿模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型复杂推理能力的提升依赖高质量可验证数据集，但人工标注成本高昂且难以扩展。现有合成方法面临困境：保持结构有效性会限制问题复杂度，而放宽约束增加难度又常导致不一致或无解的实例。需要一种能够生成高精度、可验证且复杂的训练数据的新方法。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的序列决策过程，专门的智能体动态选择和组合模块化推理技能。通过内部反思和工具使用的迭代工作流，使用多粒度策略优化(MGPO)开发Agentic-Proposer-4B模型，在数学、编程和科学领域生成高精度可验证的训练轨迹。

Result: 使用智能体合成数据训练的下游求解器显著优于领先基线，并展现出强大的跨域泛化能力。仅使用11,000条合成轨迹训练的30B求解器在AIME25上达到91.6%的最先进准确率，与GPT-5等前沿专有模型相当，证明少量高质量合成数据可有效替代大规模人工标注数据集。

Conclusion: Agentic Proposing框架成功解决了数据合成中结构有效性与问题复杂度之间的权衡难题，通过智能体驱动的动态技能组合和多粒度策略优化，能够生成高质量可验证的推理训练数据。实验证明少量精心合成的数据即可达到与大规模人工数据集相当甚至更优的效果，为可扩展的复杂推理能力训练提供了新范式。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [38] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [39] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 本文提出了结构化双极论证框架(SBAFs),允许智能体基于怀疑理性地拒绝论证,并关注辩论中可接受的句子集合而非仅关注论证集合,提供了介于可接受语义和完备语义之间的新语义方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算论证研究忽视了两个重要问题:一是智能体可以基于怀疑理性地拒绝论证,不必接受所有可防御的论证;二是在辩论中关注智能体接受哪些具体句子或主张比关注接受哪些论证更自然。需要将这些哲学和语言学观点融入计算论证方法中。

Method: 定义了结构化双极论证框架(SBAFs),其中论证由句子组成,并存在攻击和支持两种关系。为SBAFs提供了两类语义:(1)不强制智能体接受所有被防御的论证的语义;(2)除了给出可接受论证集合的论证扩展外,还提供了指定可接受句子集合的语言扩展语义。这些语义位于抽象论证的可接受语义和完备语义之间。

Result: 提出的语义方法能够表示智能体在辩论中可能持有的合理立场,介于可接受语义和完备语义之间。该方法可以为现有方法提供新视角,能够明确智能体在何种条件下可以忽略论证间的支持关系(即何时使用抽象论证是合理的),并证明了演绎支持语义是该方法的特例。

Conclusion: 本文成功将哲学和语言学观点融入计算论证,通过SBAFs框架及其语义,既允许智能体基于怀疑拒绝论证,又能刻画可接受的句子集合。该方法不仅提供了新的论证语义,还为理解和统一现有论证方法提供了新的理论视角,具有较强的理论价值和应用潜力。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [40] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [41] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [42] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [43] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [44] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [45] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [46] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: 本文提出了CRL-VLA框架,通过非对称调节机制和双评论家架构解决视觉-语言-行动模型在终身强化学习中的稳定性与可塑性平衡问题,在保留旧技能的同时学习新技能方面取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动(VLA)模型在开放世界环境中进行终身学习时,面临持续强化学习中稳定性(保留旧技能)和可塑性(学习新技能)之间的平衡难题。这一挑战限制了VLA模型在终身机器人场景中的实际部署应用。

Method: 提出CRL-VLA框架,具有严格的理论性能界限,将稳定性-可塑性权衡与目标条件优势幅度及策略分歧联系起来。通过非对称调节机制实现:在先前任务上约束优势幅度,同时在新任务上实现可控增长。采用双评论家架构结合新颖的目标条件价值公式(GCVF),其中冻结的评论家锚定语义一致性,可训练的估计器驱动适应过程。

Result: 在LIBERO基准测试上的实验表明,CRL-VLA有效协调了稳定性与可塑性这两个冲突目标,在抗遗忘和前向适应方面均优于基线方法。

Conclusion: CRL-VLA框架通过理论指导的非对称调节策略和双评论家架构,成功解决了VLA模型在持续强化学习中的稳定性-可塑性困境,为终身机器人学习提供了一个有效的解决方案,在保持已学技能的同时能够高效学习新任务。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [47] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [48] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [49] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [50] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 本文通过基于智能体的模型和演化博弈论研究了生成式AI使用对人类文化演化的长期影响,发现AI替代型使用会导致"文化崩溃",而AI辅助型使用在群体选择下能够维持文化多样性,为制定相关政策提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛使用可能降低文化差异和多样性,已经导致模型崩溃和幻觉等问题。本文旨在探讨AI使用对人类文化演化的长期后果,以及在何种条件下广泛使用AI会导致"文化崩溃"——即依赖AI生成内容会减少人类变异和创新,减缓累积性文化演化的过程。

Method: 使用基于智能体的模型(agent-based model)和演化博弈论(evolutionary game theory)方法,比较两种AI使用类型:AI辅助型(complement)用户寻求建议和指导但仍是主要创作者,AI替代型(substitute)用户提供最少输入并主要依赖AI产出。研究这两种使用策略在演化动态下如何竞争和传播。

Result: 研究发现,尽管AI替代型使用会更强烈地减少文化差异,但在个体层面选择下,AI替代型用户占据主导地位。相反,AI辅助型用户能够通过维持探索所需的差异性使其群体受益,因此在群体边界较强的文化群体选择下更受青睐。

Conclusion: 研究阐明了AI采用的长期、群体层面影响,为制定政策和组织策略以减轻"文化崩溃"风险提供了理论依据。需要在个体激励和群体利益之间寻找平衡,鼓励AI辅助型使用而非替代型使用,以维护文化多样性和创新能力。

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [51] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [52] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 本文提出EHRWorld，一个基于因果序列范式训练的以患者为中心的医疗世界模型，用于模拟疾病进展和治疗结果。研究表明，仅依赖医学知识的大语言模型在长期临床模拟中存在状态一致性问题，而EHRWorld通过在真实电子健康记录数据上训练，实现了更稳定的长期模拟和更好的临床事件建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然在静态医学推理任务上表现良好，但在作为动态医疗世界模型时，难以在连续干预下保持患者状态的一致性，导致长期临床模拟中的误差累积。医疗领域需要能够可靠模拟疾病进展和治疗结果的世界模型，但在复杂高风险的医疗场景中实现这样的模型仍然具有挑战性。

Method: 提出EHRWorld，一个基于因果序列范式训练的以患者为中心的医疗世界模型。构建了EHRWorld-110K大规模纵向临床数据集，该数据集来源于真实世界的电子健康记录。模型采用因果基础的、时间演化的临床数据进行训练，以实现可靠和稳健的医疗世界建模。

Result: 广泛的评估表明，EHRWorld显著优于基于朴素大语言模型的基线方法，在以下方面表现出色：实现了更稳定的长期模拟、改进了对临床敏感事件的建模能力、具有良好的推理效率。实验验证了在因果基础的时间演化临床数据上训练的必要性。

Conclusion: 研究证明了在真实世界电子健康记录数据上，采用因果序列范式训练医疗世界模型的有效性。EHRWorld相比仅依赖医学知识的大语言模型，能够更好地维持患者状态一致性，减少长期模拟中的误差累积，为可靠和稳健的医疗世界建模提供了新的解决方案。这突出了使用因果基础、时间演化临床数据进行训练的重要性。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [53] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [54] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: 提出了Search-R2框架,通过Actor-Refiner协作机制和混合奖励设计解决搜索集成推理中的多尺度信用分配问题,在多个问答数据集上实现了优于现有RAG和强化学习基线的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索集成推理语言智能体在强化学习训练中面临多尺度信用分配问题:依赖稀疏的轨迹级奖励无法区分高质量推理和偶然猜测,导致冗余或误导性的搜索行为。需要一种能够提供细粒度监督并有针对性地改进推理过程的方法。

Method: 提出Search-R2框架,包含三个核心组件:(1)Actor-Refiner协作架构:Actor生成初始推理轨迹,Meta-Refiner通过"剪切-重生成"机制选择性地诊断和修复错误步骤;(2)混合奖励设计:结合结果正确性和密集过程奖励,量化检索证据的信息密度;(3)理论形式化:将Actor-Refiner交互建模为平滑混合策略,证明选择性修正能带来严格的性能提升。两个组件在训练过程中联合优化。

Result: 在多个通用和多跳问答数据集上的广泛实验表明,Search-R2在不同模型规模下均持续优于强RAG和基于强化学习的基线方法,以最小的开销实现了更优的推理准确率。选择性修正机制有效解决了信用分配问题,提高了搜索效率和推理质量。

Conclusion: Search-R2通过Actor-Refiner协作框架和细粒度混合奖励机制,有效解决了搜索集成推理中的多尺度信用分配难题。理论分析和实验结果共同验证了选择性修正策略的有效性,为训练具有外部搜索能力的语言智能体提供了一种高效且可扩展的解决方案,在推理准确性和计算效率之间取得了良好平衡。

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [55] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 本文研究了大语言模型在多轮智能体场景中的"对话惯性"问题，即模型过度模仿自身先前响应导致探索能力受限。研究提出了上下文偏好学习方法来校准模型偏好，并通过上下文管理策略平衡探索与利用，在多个智能体环境中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为少样本学习器表现优异，但在多轮智能体场景中会错误地将自己之前的响应当作少样本示例进行模仿，产生"对话惯性"现象。这种现象限制了模型的探索能力，形成了一个矛盾：更长的上下文能提供更丰富的环境反馈用于利用，但同时也会放大对话惯性从而削弱探索能力。需要解决如何将少样本学习的大语言模型转化为有效智能体的问题。

Method: 通过注意力分析识别对话惯性现象，发现模型对先前响应表现出强烈的对角线注意力模式。核心洞察是对于相同状态，使用更长上下文生成的动作比使用更短上下文的表现出更强的惯性。基于此提出上下文偏好学习（Context Preference Learning）方法，在不需要环境奖励的情况下构建偏好对，校准模型偏好以倾向于低惯性响应而非高惯性响应。同时提供推理时的上下文管理策略来平衡探索与利用。

Result: 在八个智能体环境和一个深度研究场景中进行了实验验证。结果表明该框架有效降低了对话惯性，并在性能上取得了显著提升。实验证实了上下文偏好学习方法能够成功校准模型行为，使其在多轮交互中保持更好的探索能力。

Conclusion: 本研究揭示了大语言模型在智能体应用中的对话惯性问题，并提出了有效的解决方案。通过上下文偏好学习和上下文管理策略，成功解决了长上下文带来的探索-利用困境，为将少样本学习的大语言模型转化为高效智能体提供了新的思路和方法。该方法无需环境奖励即可构建训练数据，具有较好的实用性和推广价值。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [56] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: 本文提出TodyComm算法,一种面向任务的动态通信方法,通过策略梯度优化在每轮中自适应调整多智能体系统的协作拓扑结构,在动态对抗和通信预算约束下实现优越的任务效果、令牌效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的多智能体系统在推理过程中使用固定的通信拓扑结构,无法适应智能体角色在多轮交互中因动态对抗、任务进展或时变约束(如通信带宽)而发生的变化,这在许多实际应用中存在局限性。

Method: 提出TodyComm(面向任务的动态通信)算法,该方法生成行为驱动的协作拓扑结构,能够在每一轮中适应动态变化,并通过策略梯度优化任务效用。算法根据任务需求动态调整智能体之间的通信结构。

Result: 在五个基准测试上的实验表明,在动态对抗和通信预算约束条件下,TodyComm在任务有效性方面表现优越,同时保持了令牌效率和可扩展性。

Conclusion: TodyComm算法通过动态调整通信拓扑结构,有效解决了多智能体系统中固定通信结构的局限性,在处理动态环境和资源约束时展现出优越性能,为构建更灵活高效的多智能体协作系统提供了新的解决方案。

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [57] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [58] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 本文提出了一种基于风险控制的自适应推理框架,通过设置上下阈值来优化大语言模型的测试时计算预算,在控制错误率的同时最小化计算成本,实现了推理效率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型虽然可以通过增加token预算提升准确率,但如何设置合适的token预算和自适应推理阈值是一个实际挑战,涉及风险与准确率之间的根本权衡。现有方法缺乏系统化的预算设置机制,难以在保证可靠性的同时优化计算效率。

Method: 将预算设置问题重新定义为风险控制问题,引入双阈值机制:上阈值用于在模型高置信度时停止推理(冒着输出错误的风险),下阈值用于提前停止无法解决的实例(冒着过早停止的风险)。基于目标风险和验证集,使用无分布假设的风险控制方法来优化这些停止机制。对于多预算控制标准的场景,引入效率损失函数来选择计算效率最高的退出机制。

Result: 在多种推理任务和模型上的实验结果表明,该风险控制方法有效实现了计算效率提升。下阈值和集成停止机制显著降低了计算成本,同时严格遵守用户指定的风险目标,验证了框架在准确率和效率之间取得良好平衡的能力。

Conclusion: 本文提出的基于风险控制的自适应推理框架为大语言模型的测试时计算优化提供了理论严谨且实用的解决方案。通过双阈值机制和无分布风险控制,成功解决了token预算设置的难题,在保证错误率可控的前提下实现了计算资源的高效利用,为推理型LLMs的实际部署提供了重要参考。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [59] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>
